{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DDPG, TD3, SAC, PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from beads_gym.environment.beads_cart_pole_environment import BeadsCartPoleEnvironment\n",
    "from beads_gym.environment.beads_quad_copter_environment import BeadsQuadCopterEnvironment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings ; warnings.filterwarnings('ignore')\n",
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.multiprocessing as mp\n",
    "from torch.distributions import Normal\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "from itertools import count\n",
    "import moviepy.editor as mpy\n",
    "\n",
    "import os.path\n",
    "import tempfile\n",
    "import random\n",
    "import base64\n",
    "import glob\n",
    "import time\n",
    "import json\n",
    "import gym\n",
    "import io\n",
    "import os\n",
    "import gc\n",
    "\n",
    "from gym import wrappers\n",
    "from IPython.display import HTML\n",
    "\n",
    "LEAVE_PRINT_EVERY_N_SECS = 300\n",
    "ERASE_LINE = '\\x1b[2K'\n",
    "EPS = 1e-6\n",
    "BEEP = lambda: os.system(\"printf '\\a'\")\n",
    "RESULTS_DIR = os.path.join('..', 'results')\n",
    "SEEDS = (12, 34, 56) #, 78, 90)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = BeadsQuadCopterEnvironment()\n",
    "env.reset()\n",
    "plt.imshow(env.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')\n",
    "params = {\n",
    "    'figure.figsize': (15, 8),\n",
    "    'font.size': 24,\n",
    "    'legend.fontsize': 20,\n",
    "    'axes.titlesize': 28,\n",
    "    'axes.labelsize': 24,\n",
    "    'xtick.labelsize': 20,\n",
    "    'ytick.labelsize': 20\n",
    "}\n",
    "pylab.rcParams.update(params)\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_make_env_fn(**kargs):\n",
    "    def make_env_fn(env_name, seed=None, render=None, record=False,\n",
    "                    unwrapped=False, monitor_mode=None, \n",
    "                    inner_wrappers=None, outer_wrappers=None):\n",
    "        mdir = tempfile.mkdtemp()\n",
    "        env = None\n",
    "        if render:\n",
    "            try:\n",
    "                env = gym.make(env_name, render=render)\n",
    "            except:\n",
    "                pass\n",
    "        if env_name == \"BeadsCartPoleEnvironment\":\n",
    "            env = BeadsCartPoleEnvironment()\n",
    "            env.do_render = render\n",
    "            env.do_record = record\n",
    "            env.monitor_mode = monitor_mode\n",
    "            return env\n",
    "        if env_name == \"BeadsQuadCopterEnvironment\":\n",
    "            env = BeadsQuadCopterEnvironment()\n",
    "            env.do_render = render\n",
    "            env.do_record = record\n",
    "            env.monitor_mode = monitor_mode\n",
    "            return env\n",
    "        if env is None:\n",
    "            env = gym.make(env_name)\n",
    "        if seed is not None: env.seed(seed)\n",
    "        env = env.unwrapped if unwrapped else env\n",
    "        if inner_wrappers:\n",
    "            for wrapper in inner_wrappers:\n",
    "                env = wrapper(env)\n",
    "        env = wrappers.Monitor(\n",
    "            env, mdir, force=True, \n",
    "            mode=monitor_mode, \n",
    "            video_callable=lambda e_idx: record) if monitor_mode else env\n",
    "        if outer_wrappers:\n",
    "            for wrapper in outer_wrappers:\n",
    "                env = wrapper(env)\n",
    "        return env\n",
    "    return make_env_fn, kargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_videos_html(env_videos, title, max_n_videos=4):\n",
    "    videos = np.array(env_videos)\n",
    "    if len(videos) == 0:\n",
    "        return\n",
    "    \n",
    "    n_videos = max(1, min(max_n_videos, len(videos)))\n",
    "    idxs = np.linspace(0, len(videos) - 1, n_videos).astype(int) if n_videos > 1 else [-1,]\n",
    "    videos = videos[idxs,...]\n",
    "\n",
    "    strm = '<h2>{}<h2>'.format(title)\n",
    "    for video_path, meta_path in videos:\n",
    "        video = io.open(video_path, 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "\n",
    "        with open(meta_path) as data_file:    \n",
    "            meta = json.load(data_file)\n",
    "\n",
    "        html_tag = \"\"\"\n",
    "        <h3>{0}<h3/>\n",
    "        <video width=\"960\" height=\"540\" controls>\n",
    "            <source src=\"data:video/mp4;base64,{1}\" type=\"video/mp4\" />\n",
    "        </video>\"\"\"\n",
    "        strm += html_tag.format('Episode ' + str(meta['episode_id']), encoded.decode('ascii'))\n",
    "    return strm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gif_html(video, title, video_id):\n",
    "    video = np.array(video)\n",
    "    num_frames = len(video)\n",
    "    fps = 10\n",
    "\n",
    "    # Create a VideoClip\n",
    "    clip = mpy.VideoClip(\n",
    "        # fun,\n",
    "        lambda t: video[int(t * fps)],\n",
    "        duration=num_frames / fps,\n",
    "    )\n",
    "\n",
    "    # Write the VideoClip to a file\n",
    "    clip.write_videofile(f\"{title}_{video_id}.mp4\", fps=fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RenderUint8(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "    def reset(self, **kwargs):\n",
    "        return self.env.reset(**kwargs)\n",
    "    def render(self, mode='rgb_array'):\n",
    "        frame = self.env.render(mode=mode)\n",
    "        return frame.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCQV(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim, \n",
    "                 output_dim, \n",
    "                 hidden_dims=(32,32), \n",
    "                 activation_fc=F.relu):\n",
    "        super(FCQV, self).__init__()\n",
    "        self.activation_fc = activation_fc\n",
    "\n",
    "        self.input_layer = nn.Linear(input_dim, hidden_dims[0])\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for i in range(len(hidden_dims)-1):\n",
    "            in_dim = hidden_dims[i]\n",
    "            if i == 0: \n",
    "                in_dim += output_dim\n",
    "            hidden_layer = nn.Linear(in_dim, hidden_dims[i+1])\n",
    "            self.hidden_layers.append(hidden_layer)\n",
    "        self.output_layer = nn.Linear(hidden_dims[-1], 1)\n",
    "\n",
    "        device = \"cpu\"\n",
    "        if torch.cuda.is_available():\n",
    "            device = \"cuda:0\"\n",
    "        self.device = torch.device(device)\n",
    "        self.to(self.device)\n",
    "    \n",
    "    def _format(self, state, action):\n",
    "        x, u = state, action\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.tensor(x, \n",
    "                             device=self.device, \n",
    "                             dtype=torch.float32)\n",
    "            x = x.unsqueeze(0)\n",
    "        if not isinstance(u, torch.Tensor):\n",
    "            u = torch.tensor(u, \n",
    "                             device=self.device, \n",
    "                             dtype=torch.float32)\n",
    "            u = u.unsqueeze(0)\n",
    "        return x, u\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x, u = self._format(state, action)\n",
    "        x = self.activation_fc(self.input_layer(x))\n",
    "        for i, hidden_layer in enumerate(self.hidden_layers):\n",
    "            if i == 0:\n",
    "                x = torch.cat((x, u), dim=1)\n",
    "            x = self.activation_fc(hidden_layer(x))\n",
    "        return self.output_layer(x)\n",
    "    \n",
    "    def load(self, experiences):\n",
    "        states, actions, new_states, rewards, is_terminals = experiences\n",
    "        states = torch.from_numpy(states).float().to(self.device)\n",
    "        actions = torch.from_numpy(actions).float().to(self.device)\n",
    "        new_states = torch.from_numpy(new_states).float().to(self.device)\n",
    "        rewards = torch.from_numpy(rewards).float().to(self.device)\n",
    "        is_terminals = torch.from_numpy(is_terminals).float().to(self.device)\n",
    "        return states, actions, new_states, rewards, is_terminals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCDP(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim,\n",
    "                 action_bounds,\n",
    "                 hidden_dims=(32,32), \n",
    "                 activation_fc=F.selu,\n",
    "                 out_activation_fc=F.tanh):\n",
    "        super(FCDP, self).__init__()\n",
    "        self.activation_fc = activation_fc\n",
    "        self.out_activation_fc = out_activation_fc\n",
    "        self.act_min, self.act_max = action_bounds\n",
    "\n",
    "        self.input_layer = nn.Linear(input_dim, hidden_dims[0])\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for i in range(len(hidden_dims)-1):\n",
    "            hidden_layer = nn.Linear(hidden_dims[i], hidden_dims[i+1])\n",
    "            self.hidden_layers.append(hidden_layer)\n",
    "        self.output_layer = nn.Linear(hidden_dims[-1], len(self.act_max))\n",
    "\n",
    "        device = \"cpu\"\n",
    "        if torch.cuda.is_available():\n",
    "            device = \"cuda:0\"\n",
    "        self.device = torch.device(device)\n",
    "        self.to(self.device)\n",
    "        \n",
    "        self.act_min = torch.tensor(self.act_min,\n",
    "                                    device=self.device, \n",
    "                                    dtype=torch.float32)\n",
    "\n",
    "        self.act_max = torch.tensor(self.act_max,\n",
    "                                    device=self.device, \n",
    "                                    dtype=torch.float32)\n",
    "        \n",
    "        self.nn_min = self.out_activation_fc(\n",
    "            torch.Tensor([float('-inf')])).to(self.device)\n",
    "        self.nn_max = self.out_activation_fc(\n",
    "            torch.Tensor([float('inf')])).to(self.device)\n",
    "        self.rescale_fn = lambda x: (x - self.nn_min) * (self.act_max - self.act_min) / \\\n",
    "                                    (self.nn_max - self.nn_min) + self.act_min\n",
    "\n",
    "    def _format(self, state):\n",
    "        x = state\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.tensor(x, \n",
    "                             device=self.device, \n",
    "                             dtype=torch.float32)\n",
    "            x = x.unsqueeze(0)\n",
    "        return x\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self._format(state)\n",
    "        x = self.activation_fc(self.input_layer(x))\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            x = self.activation_fc(hidden_layer(x))\n",
    "        x = self.output_layer(x)\n",
    "        x = self.out_activation_fc(x)\n",
    "        return self.rescale_fn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self, \n",
    "                 max_size=10000, \n",
    "                 batch_size=64):\n",
    "        self.ss_mem = np.empty(shape=(max_size), dtype=np.ndarray)\n",
    "        self.as_mem = np.empty(shape=(max_size), dtype=np.ndarray)\n",
    "        self.rs_mem = np.empty(shape=(max_size), dtype=np.ndarray)\n",
    "        self.ps_mem = np.empty(shape=(max_size), dtype=np.ndarray)\n",
    "        self.ds_mem = np.empty(shape=(max_size), dtype=np.ndarray)\n",
    "\n",
    "        self.max_size = max_size\n",
    "        self.batch_size = batch_size\n",
    "        self._idx = 0\n",
    "        self.size = 0\n",
    "    \n",
    "    def store(self, sample):\n",
    "        s, a, r, p, d = sample\n",
    "        self.ss_mem[self._idx] = s\n",
    "        self.as_mem[self._idx] = a\n",
    "        self.rs_mem[self._idx] = r\n",
    "        self.ps_mem[self._idx] = p\n",
    "        self.ds_mem[self._idx] = d\n",
    "        \n",
    "        self._idx += 1\n",
    "        self._idx = self._idx % self.max_size\n",
    "\n",
    "        self.size += 1\n",
    "        self.size = min(self.size, self.max_size)\n",
    "\n",
    "    def sample(self, batch_size=None):\n",
    "        if batch_size == None:\n",
    "            batch_size = self.batch_size\n",
    "\n",
    "        idxs = np.random.choice(\n",
    "            self.size, batch_size, replace=False)\n",
    "        experiences = np.vstack(self.ss_mem[idxs]), \\\n",
    "                      np.vstack(self.as_mem[idxs]), \\\n",
    "                      np.vstack(self.rs_mem[idxs]), \\\n",
    "                      np.vstack(self.ps_mem[idxs]), \\\n",
    "                      np.vstack(self.ds_mem[idxs])\n",
    "        return experiences\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedyStrategy():\n",
    "    def __init__(self, bounds):\n",
    "        self.low, self.high = bounds\n",
    "        self.ratio_noise_injected = 0\n",
    "\n",
    "    def select_action(self, model, state):\n",
    "        with torch.no_grad():\n",
    "            greedy_action = model(state).cpu().detach().data.numpy().squeeze()\n",
    "\n",
    "        action = np.clip(greedy_action, self.low, self.high)\n",
    "        return np.reshape(action, self.high.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalNoiseStrategy():\n",
    "    def __init__(self, bounds, exploration_noise_ratio=0.1, exploration_noise_amplitude=None, ou_process=False):\n",
    "        self.low, self.high = bounds\n",
    "        \n",
    "        if exploration_noise_ratio is None: assert exploration_noise_amplitude is not None\n",
    "        if exploration_noise_amplitude is None: assert exploration_noise_ratio is not None\n",
    "        self.exploration_noise_ratio = exploration_noise_ratio\n",
    "        self.exploration_noise_amplitude = exploration_noise_amplitude\n",
    "        self.ou_process = ou_process\n",
    "        self.prev_noise = np.zeros(len(self.high))\n",
    "        self.ratio_noise_injected = 0\n",
    "        \n",
    "    def select_action(self, model, state, max_exploration=False):\n",
    "        with torch.no_grad():\n",
    "            greedy_action = model(state).cpu().detach().data.numpy().squeeze()\n",
    "            \n",
    "        noise = self.prev_noise if self.ou_process else np.zeros(len(self.high))\n",
    "        if max_exploration:\n",
    "            noise += np.random.normal(loc=0, scale=self.high, size=len(self.high))\n",
    "        else:\n",
    "            if self.exploration_noise_ratio is not None:\n",
    "                noise += np.random.normal(loc=0, scale=1, size=len(self.high)) * np.abs(greedy_action) * self.exploration_noise_ratio\n",
    "            elif self.exploration_noise_amplitude is not None:\n",
    "                noise += np.random.normal(loc=0, scale=self.exploration_noise_amplitude, size=len(self.high))\n",
    "            else:\n",
    "                raise ValueError(\"No exploration noise specified\")\n",
    "\n",
    "        noisy_action = greedy_action + noise\n",
    "        self.prev_noise = noise\n",
    "        action = np.clip(noisy_action, self.low, self.high)\n",
    "        \n",
    "        self.ratio_noise_injected = np.mean(abs((greedy_action - action)/(self.high - self.low)))\n",
    "        return action\n",
    "    \n",
    "    \n",
    "class NormalNoiseDecayStrategy():\n",
    "    def __init__(\n",
    "        self,\n",
    "        bounds,\n",
    "        init_noise_ratio_mult=0.5, min_noise_ratio_mult=0.1,\n",
    "        init_noise_ratio_add=0.5, min_noise_ratio_add=0.1,\n",
    "        decay_steps=10000,\n",
    "    ):\n",
    "        self.t = 0\n",
    "        self.low, self.high = bounds\n",
    "        self.noise_ratio_mult = init_noise_ratio_mult\n",
    "        self.init_noise_ratio_mult = init_noise_ratio_mult\n",
    "        self.min_noise_ratio_mult = min_noise_ratio_mult\n",
    "        self.noise_ratio_add = init_noise_ratio_add\n",
    "        self.init_noise_ratio_add = init_noise_ratio_add\n",
    "        self.min_noise_ratio_add = min_noise_ratio_add\n",
    "        self.decay_steps = decay_steps\n",
    "        self.ratio_noise_injected = 0\n",
    "\n",
    "    def _noise_ratio_update(self):\n",
    "        noise_ratio = 1 - self.t / self.decay_steps\n",
    "        noise_ratio_mult = (self.init_noise_ratio_mult - self.min_noise_ratio_mult) * noise_ratio + self.min_noise_ratio_mult\n",
    "        self.noise_ratio_mult = np.clip(noise_ratio_mult, self.min_noise_ratio_mult, self.init_noise_ratio_mult)\n",
    "        noise_ratio_add = (self.init_noise_ratio_add - self.min_noise_ratio_add) * noise_ratio + self.min_noise_ratio_add\n",
    "        self.noise_ratio_add = np.clip(noise_ratio_add, self.min_noise_ratio_add, self.init_noise_ratio_add)\n",
    "        self.t += 1\n",
    "\n",
    "    def select_action(self, model, state, max_exploration=False):\n",
    "        with torch.no_grad():\n",
    "            greedy_action = model(state).cpu().detach().data.numpy().squeeze()\n",
    "            \n",
    "        noise = np.zeros(len(self.high))\n",
    "        if max_exploration:\n",
    "            noise += np.random.normal(loc=0, scale=self.high, size=len(self.high))\n",
    "        else:\n",
    "            mult_noise_scale = np.abs(greedy_action) * self.noise_ratio_mult\n",
    "            noise += np.random.normal(loc=0, scale=mult_noise_scale, size=len(self.high))\n",
    "            noise += np.random.normal(loc=0, scale=self.noise_ratio_add, size=len(self.high))\n",
    "\n",
    "        noisy_action = greedy_action + noise\n",
    "        action = np.clip(noisy_action, self.low, self.high)\n",
    "        \n",
    "        self._noise_ratio_update()\n",
    "        \n",
    "        self.ratio_noise_injected = np.mean(abs((greedy_action - action)/(self.high - self.low)))\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPG():\n",
    "    def __init__(self, \n",
    "                 replay_buffer_fn,\n",
    "                 policy_model_fn, \n",
    "                 policy_max_grad_norm, \n",
    "                 policy_optimizer_fn, \n",
    "                 policy_optimizer_lr,\n",
    "                 value_model_fn, \n",
    "                 value_max_grad_norm, \n",
    "                 value_optimizer_fn, \n",
    "                 value_optimizer_lr, \n",
    "                 training_strategy_fn,\n",
    "                 evaluation_strategy_fn,\n",
    "                 n_warmup_batches,\n",
    "                 update_target_every_steps,\n",
    "                 tau):\n",
    "        self.replay_buffer_fn = replay_buffer_fn\n",
    "\n",
    "        self.policy_model_fn = policy_model_fn\n",
    "        self.policy_max_grad_norm = policy_max_grad_norm\n",
    "        self.policy_optimizer_fn = policy_optimizer_fn\n",
    "        self.policy_optimizer_lr = policy_optimizer_lr\n",
    "        \n",
    "        self.value_model_fn = value_model_fn\n",
    "        self.value_max_grad_norm = value_max_grad_norm\n",
    "        self.value_optimizer_fn = value_optimizer_fn\n",
    "        self.value_optimizer_lr = value_optimizer_lr\n",
    "\n",
    "        self.training_strategy_fn = training_strategy_fn\n",
    "        self.evaluation_strategy_fn = evaluation_strategy_fn\n",
    "\n",
    "        self.n_warmup_batches = n_warmup_batches\n",
    "        self.update_target_every_steps = update_target_every_steps\n",
    "        self.tau = tau\n",
    "\n",
    "    def optimize_model(self, experiences):\n",
    "        states, actions, rewards, next_states, is_terminals = experiences\n",
    "\n",
    "        argmax_a_q_sp = self.target_policy_model(next_states)\n",
    "        max_a_q_sp = self.target_value_model(next_states, argmax_a_q_sp)\n",
    "        target_q_sa = rewards + self.gamma * max_a_q_sp * (1 - is_terminals)\n",
    "        q_sa = self.online_value_model(states, actions)\n",
    "        td_error = q_sa - target_q_sa.detach()\n",
    "        value_loss = td_error.pow(2).mul(0.5).mean()\n",
    "        self.value_optimizer.zero_grad()\n",
    "        value_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.online_value_model.parameters(), \n",
    "                                       self.value_max_grad_norm)\n",
    "        self.value_optimizer.step()\n",
    "\n",
    "        argmax_a_q_s = self.online_policy_model(states)\n",
    "        max_a_q_s = self.online_value_model(states, argmax_a_q_s)\n",
    "        policy_loss = -max_a_q_s.mean()\n",
    "        self.policy_optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.online_policy_model.parameters(), \n",
    "                                       self.policy_max_grad_norm)        \n",
    "        self.policy_optimizer.step()\n",
    "\n",
    "    def interaction_step(self, state, env, state_noise=None):\n",
    "        min_samples = self.replay_buffer.batch_size * self.n_warmup_batches\n",
    "        noisy_state = state if state_noise is None else state + state_noise\n",
    "        action = self.training_strategy.select_action(self.online_policy_model, \n",
    "                                                      noisy_state, \n",
    "                                                      len(self.replay_buffer) < min_samples)\n",
    "        new_state, reward, is_terminal, info = env.step(action)\n",
    "        is_truncated = 'TimeLimit.truncated' in info and info['TimeLimit.truncated']\n",
    "        is_failure = is_terminal and not is_truncated\n",
    "        experience = (state, action, reward, new_state, float(is_failure))\n",
    "        self.replay_buffer.store(experience)\n",
    "        self.episode_reward[-1] += reward\n",
    "        self.episode_timestep[-1] += 1\n",
    "        self.episode_exploration[-1] += self.training_strategy.ratio_noise_injected\n",
    "        return new_state, is_terminal\n",
    "    \n",
    "    def update_networks(self, tau=None):\n",
    "        tau = self.tau if tau is None else tau\n",
    "        for target, online in zip(self.target_value_model.parameters(), \n",
    "                                  self.online_value_model.parameters()):\n",
    "            target_ratio = (1.0 - tau) * target.data\n",
    "            online_ratio = tau * online.data\n",
    "            mixed_weights = target_ratio + online_ratio\n",
    "            target.data.copy_(mixed_weights)\n",
    "\n",
    "        for target, online in zip(self.target_policy_model.parameters(), \n",
    "                                  self.online_policy_model.parameters()):\n",
    "            target_ratio = (1.0 - tau) * target.data\n",
    "            online_ratio = tau * online.data\n",
    "            mixed_weights = target_ratio + online_ratio\n",
    "            target.data.copy_(mixed_weights)\n",
    "\n",
    "    def train(self, make_env_fn, make_env_kargs, seed, gamma, \n",
    "              max_minutes, max_episodes, goal_mean_100_reward):\n",
    "        training_start, last_debug_time = time.time(), float('-inf')\n",
    "\n",
    "        self.checkpoint_dir = tempfile.mkdtemp()\n",
    "        self.make_env_fn = make_env_fn\n",
    "        self.make_env_kargs = make_env_kargs\n",
    "        self.seed = seed\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        env = self.make_env_fn(**self.make_env_kargs, seed=self.seed)\n",
    "        torch.manual_seed(self.seed) ; np.random.seed(self.seed) ; random.seed(self.seed)\n",
    "    \n",
    "        nS, nA = env.observation_space.shape[0], env.action_space.shape[0]\n",
    "        action_bounds = env.action_space.low, env.action_space.high\n",
    "        self.episode_timestep = []\n",
    "        self.episode_reward = []\n",
    "        self.episode_seconds = []\n",
    "        self.evaluation_scores = []        \n",
    "        self.episode_exploration = []\n",
    "        \n",
    "        self.target_value_model = self.value_model_fn(nS, nA)\n",
    "        self.online_value_model = self.value_model_fn(nS, nA)\n",
    "        self.target_policy_model = self.policy_model_fn(nS, action_bounds)\n",
    "        self.online_policy_model = self.policy_model_fn(nS, action_bounds)\n",
    "        self.update_networks(tau=1.0)\n",
    "        self.value_optimizer = self.value_optimizer_fn(self.online_value_model, \n",
    "                                                       self.value_optimizer_lr)        \n",
    "        self.policy_optimizer = self.policy_optimizer_fn(self.online_policy_model, \n",
    "                                                         self.policy_optimizer_lr)\n",
    "\n",
    "        self.replay_buffer = self.replay_buffer_fn()\n",
    "        self.training_strategy = training_strategy_fn(action_bounds)\n",
    "        self.evaluation_strategy = evaluation_strategy_fn(action_bounds)\n",
    "                    \n",
    "        result = np.empty((max_episodes, 6))\n",
    "        result[:] = np.nan\n",
    "        training_time = 0\n",
    "        state_noise_scale = 0.05\n",
    "        reasonable_bound = 0.1\n",
    "        how_often_within_reasonable_bounds = (np.linalg.norm(np.random.normal(loc=0, scale=state_noise_scale, size=(10000, 3)), axis=1) < reasonable_bound).mean()\n",
    "        print(\n",
    "            f\"With the scale of {state_noise_scale:.2f}, the noise vector will be \"\n",
    "            f\"{100 * how_often_within_reasonable_bounds:.2f}% of the time within a ball \"\n",
    "            f\"of radius: {reasonable_bound}\"\n",
    "        )\n",
    "        for episode in range(1, max_episodes + 1):\n",
    "            episode_start = time.time()\n",
    "            \n",
    "            state, is_terminal = env.reset(), False\n",
    "            self.episode_reward.append(0.0)\n",
    "            self.episode_timestep.append(0.0)\n",
    "            self.episode_exploration.append(0.0)\n",
    "\n",
    "            for step in count():\n",
    "                if np.random.uniform() > 1.0:\n",
    "                    state_noise = np.random.normal(loc=0, scale=state_noise_scale, size=len(state))\n",
    "                else:\n",
    "                    state_noise = None\n",
    "                state, is_terminal = self.interaction_step(state, env, state_noise)\n",
    "\n",
    "                min_samples = self.replay_buffer.batch_size * self.n_warmup_batches\n",
    "                if len(self.replay_buffer) > min_samples:\n",
    "                    experiences = self.replay_buffer.sample()\n",
    "                    experiences = self.online_value_model.load(experiences)\n",
    "                    self.optimize_model(experiences)\n",
    "\n",
    "                if np.sum(self.episode_timestep) % self.update_target_every_steps == 0:\n",
    "                    self.update_networks()\n",
    "\n",
    "                if is_terminal:\n",
    "                    gc.collect()\n",
    "                    break\n",
    "            \n",
    "            # stats\n",
    "            episode_elapsed = time.time() - episode_start\n",
    "            self.episode_seconds.append(episode_elapsed)\n",
    "            training_time += episode_elapsed\n",
    "            evaluation_score, _ = self.evaluate(self.online_policy_model, env)\n",
    "            self.save_checkpoint(episode-1, self.online_policy_model)\n",
    "\n",
    "            total_step = int(np.sum(self.episode_timestep))\n",
    "            self.evaluation_scores.append(evaluation_score)\n",
    "            \n",
    "            mean_10_reward = np.mean(self.episode_reward[-10:])\n",
    "            std_10_reward = np.std(self.episode_reward[-10:])\n",
    "            mean_100_reward = np.mean(self.episode_reward[-100:])\n",
    "            std_100_reward = np.std(self.episode_reward[-100:])\n",
    "            mean_100_eval_score = np.mean(self.evaluation_scores[-100:])\n",
    "            std_100_eval_score = np.std(self.evaluation_scores[-100:])\n",
    "            lst_100_exp_rat = np.array(\n",
    "                self.episode_exploration[-100:])/np.array(self.episode_timestep[-100:])\n",
    "            mean_100_exp_rat = np.mean(lst_100_exp_rat)\n",
    "            std_100_exp_rat = np.std(lst_100_exp_rat)\n",
    "            \n",
    "            wallclock_elapsed = time.time() - training_start\n",
    "            result[episode-1] = total_step, mean_100_reward, \\\n",
    "                mean_100_eval_score, mean_100_exp_rat, training_time, wallclock_elapsed\n",
    "            \n",
    "            reached_debug_time = time.time() - last_debug_time >= LEAVE_PRINT_EVERY_N_SECS\n",
    "            reached_max_minutes = wallclock_elapsed >= max_minutes * 60\n",
    "            reached_max_episodes = episode >= max_episodes\n",
    "            reached_goal_mean_reward = mean_100_eval_score >= goal_mean_100_reward\n",
    "            training_is_over = reached_max_minutes or \\\n",
    "                               reached_max_episodes or \\\n",
    "                               reached_goal_mean_reward\n",
    "            elapsed_str = time.strftime(\"%H:%M:%S\", time.gmtime(time.time() - training_start))\n",
    "            debug_message = (\n",
    "                f'el {elapsed_str}, ep {episode-1:04}, ts {total_step:07}, '\n",
    "                f'mean_10_reward: {mean_10_reward:.2f}\\u00B1{std_10_reward:.2f}, '\n",
    "                f'mean_100_reward: {mean_100_reward:.2f}\\u00B1{std_100_reward:.2f}, '\n",
    "                f'mean_100_exp_rat: {mean_100_exp_rat:.2f}\\u00B1{std_100_exp_rat:.2f}, '\n",
    "                f'mean_100_eval_score: {mean_100_eval_score:.2f}\\u00B1{std_100_eval_score:.2f}'\n",
    "            )\n",
    "            print(debug_message, end='\\r', flush=True)\n",
    "            if reached_debug_time or training_is_over:\n",
    "                print(ERASE_LINE + debug_message, flush=True)\n",
    "                last_debug_time = time.time()\n",
    "            if training_is_over:\n",
    "                if reached_max_minutes: print(u'--> reached_max_minutes \\u2715')\n",
    "                if reached_max_episodes: print(u'--> reached_max_episodes \\u2715')\n",
    "                if reached_goal_mean_reward: print(u'--> reached_goal_mean_reward \\u2713')\n",
    "                break\n",
    "                \n",
    "        final_eval_score, score_std = self.evaluate(self.online_policy_model, env, n_episodes=100)\n",
    "        wallclock_time = time.time() - training_start\n",
    "        print('Training complete.')\n",
    "        print('Final evaluation score {:.2f}\\u00B1{:.2f} in {:.2f}min training time,'\n",
    "              ' {:.2f}min wall-clock time.\\n'.format(\n",
    "                  final_eval_score, score_std, training_time / 60, wallclock_time / 60))\n",
    "        env.close() ; del env\n",
    "        self.get_cleaned_checkpoints()\n",
    "        return result, final_eval_score, training_time, wallclock_time\n",
    "    \n",
    "    def evaluate(self, eval_policy_model, eval_env, n_episodes=1):\n",
    "        rs = []\n",
    "        for _ in range(n_episodes):\n",
    "            s, d = eval_env.reset(), False\n",
    "            rs.append(0)\n",
    "            for _ in count():\n",
    "                a = self.evaluation_strategy.select_action(eval_policy_model, s)\n",
    "                s, r, d, _ = eval_env.step(a)\n",
    "                rs[-1] += r\n",
    "                if eval_env.do_render: eval_env.render()\n",
    "                if d: break\n",
    "        return np.mean(rs), np.std(rs)\n",
    "\n",
    "    def get_cleaned_checkpoints(self, n_checkpoints=4):\n",
    "        try: \n",
    "            return self.checkpoint_paths\n",
    "        except AttributeError:\n",
    "            self.checkpoint_paths = {}\n",
    "\n",
    "        paths = glob.glob(os.path.join(self.checkpoint_dir, '*.tar'))\n",
    "        paths_dic = {int(path.split('.')[-2]):path for path in paths}\n",
    "        last_ep = max(paths_dic.keys())\n",
    "        # checkpoint_idxs = np.geomspace(1, last_ep+1, n_checkpoints, endpoint=True, dtype=np.int)-1\n",
    "        checkpoint_idxs = np.linspace(1, last_ep+1, n_checkpoints, endpoint=True, dtype=int)-1\n",
    "\n",
    "        for idx, path in paths_dic.items():\n",
    "            if idx in checkpoint_idxs:\n",
    "                self.checkpoint_paths[idx] = path\n",
    "            else:\n",
    "                os.unlink(path)\n",
    "\n",
    "        return self.checkpoint_paths\n",
    "\n",
    "    def demo_last(self, title='Fully-trained {} Agent', n_episodes=2, max_n_videos=2):\n",
    "\n",
    "        checkpoint_paths = self.get_cleaned_checkpoints()\n",
    "        last_ep = max(checkpoint_paths.keys())\n",
    "        self.online_policy_model.load_state_dict(torch.load(checkpoint_paths[last_ep]))\n",
    "\n",
    "        for i in range(n_episodes):\n",
    "            env = self.make_env_fn(**self.make_env_kargs, monitor_mode='evaluation', render=True, record=True)\n",
    "            self.evaluate(self.online_policy_model, env, n_episodes=1)\n",
    "            env.close()\n",
    "            data = get_gif_html(env.videos[0], \n",
    "                                title.format(self.__class__.__name__),\n",
    "                                i)\n",
    "        del env\n",
    "        return HTML(data=data)\n",
    "\n",
    "    def demo_progression(self, title='{} Agent progression', max_n_videos=4):\n",
    "        env = self.make_env_fn(**self.make_env_kargs, monitor_mode='evaluation', render=True, record=True)\n",
    "\n",
    "        checkpoint_paths = self.get_cleaned_checkpoints()\n",
    "        for i in sorted(checkpoint_paths.keys()):\n",
    "            self.online_policy_model.load_state_dict(torch.load(checkpoint_paths[i]))\n",
    "            self.evaluate(self.online_policy_model, env, n_episodes=1)\n",
    "\n",
    "        env.close()\n",
    "        data = get_gif_html(env_videos=env.videos, \n",
    "                            title=title.format(self.__class__.__name__),\n",
    "                            subtitle_eps=sorted(checkpoint_paths.keys()),\n",
    "                            max_n_videos=max_n_videos)\n",
    "        del env\n",
    "        return HTML(data=data)\n",
    "\n",
    "    def save_checkpoint(self, episode_idx, model):\n",
    "        torch.save(model.state_dict(), \n",
    "                   os.path.join(self.checkpoint_dir, 'model.{}.tar'.format(episode_idx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ddpg_results = []\n",
    "best_agent, best_eval_score = None, float('-inf')\n",
    "for seed in SEEDS:\n",
    "    environment_settings = {\n",
    "        # 'env_name': 'BeadsCartPole',\n",
    "        'env_name': 'BeadsQuadCopterEnvironment',\n",
    "        'gamma': 0.99, # 0.99,\n",
    "        'max_minutes': 180,\n",
    "        'max_episodes': 1000,\n",
    "        'goal_mean_100_reward': 140000.0,\n",
    "    }\n",
    "\n",
    "    policy_model_fn = lambda nS, bounds: FCDP(nS, bounds, hidden_dims=(256,256))\n",
    "    policy_max_grad_norm = float('inf')\n",
    "    policy_optimizer_fn = lambda net, lr: optim.Adam(net.parameters(), lr=lr)\n",
    "    policy_optimizer_lr = 0.0003\n",
    "\n",
    "    value_model_fn = lambda nS, nA: FCQV(nS, nA, hidden_dims=(256,256))\n",
    "    value_max_grad_norm = float('inf')\n",
    "    value_optimizer_fn = lambda net, lr: optim.Adam(net.parameters(), lr=lr)\n",
    "    value_optimizer_lr = 0.0003\n",
    "\n",
    "    # training_strategy_fn = lambda bounds: NormalNoiseStrategy(bounds, exploration_noise_ratio=0.1, exploration_noise_amplitude=1.0)\n",
    "    training_strategy_fn = lambda bounds: NormalNoiseDecayStrategy(bounds,\n",
    "                                                                   init_noise_ratio_mult=0.1,\n",
    "                                                                   min_noise_ratio_mult=0.01,\n",
    "                                                                   init_noise_ratio_add=1.5,\n",
    "                                                                   min_noise_ratio_add=0.01,\n",
    "                                                                   decay_steps=800_000)\n",
    "    # training_strategy_fn = lambda bounds: NormalNoiseStrategy(\n",
    "    #     bounds,\n",
    "    #     exploration_noise_ratio=0.1,\n",
    "    #     exploration_noise_amplitude=0.2,\n",
    "    #     ou_process=True,    \n",
    "    # )\n",
    "    evaluation_strategy_fn = lambda bounds: GreedyStrategy(bounds)\n",
    "\n",
    "    replay_buffer_fn = lambda: ReplayBuffer(max_size=1_000_000, batch_size=256)\n",
    "    n_warmup_batches = 5\n",
    "    update_target_every_steps = 2\n",
    "    tau = 0.005\n",
    "    \n",
    "    env_name, gamma, max_minutes, \\\n",
    "    max_episodes, goal_mean_100_reward = environment_settings.values()\n",
    "\n",
    "    agent = DDPG(replay_buffer_fn,\n",
    "                 policy_model_fn, \n",
    "                 policy_max_grad_norm, \n",
    "                 policy_optimizer_fn, \n",
    "                 policy_optimizer_lr,\n",
    "                 value_model_fn, \n",
    "                 value_max_grad_norm, \n",
    "                 value_optimizer_fn, \n",
    "                 value_optimizer_lr, \n",
    "                 training_strategy_fn,\n",
    "                 evaluation_strategy_fn,\n",
    "                 n_warmup_batches,\n",
    "                 update_target_every_steps,\n",
    "                 tau)\n",
    "\n",
    "    make_env_fn, make_env_kargs = get_make_env_fn(env_name=env_name)\n",
    "    result, final_eval_score, training_time, wallclock_time = agent.train(\n",
    "        make_env_fn, make_env_kargs, seed, gamma, max_minutes, max_episodes, goal_mean_100_reward)\n",
    "    ddpg_results.append(result)\n",
    "    if final_eval_score > best_eval_score:\n",
    "        best_eval_score = final_eval_score\n",
    "        print('best_agent assigned!\\n')\n",
    "        best_agent = agent\n",
    "        \n",
    "ddpg_results = np.array(ddpg_results)\n",
    "_ = BEEP()\n",
    "\n",
    "del best_agent.replay_buffer\n",
    "print(gc.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_max_t, ddpg_max_r, ddpg_max_s, ddpg_max_exp, \\\n",
    "ddpg_max_sec, ddpg_max_rt = np.max(ddpg_results, axis=0).T\n",
    "ddpg_min_t, ddpg_min_r, ddpg_min_s, ddpg_min_exp, \\\n",
    "ddpg_min_sec, ddpg_min_rt = np.min(ddpg_results, axis=0).T\n",
    "ddpg_mean_t, ddpg_mean_r, ddpg_mean_s, ddpg_mean_exp, \\\n",
    "ddpg_mean_sec, ddpg_mean_rt = np.mean(ddpg_results, axis=0).T\n",
    "ddpg_x = np.arange(len(ddpg_mean_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 1, figsize=(15,10), sharey=False, sharex=True)\n",
    "\n",
    "# DDPG\n",
    "axs[0].plot(ddpg_max_r, 'r', linewidth=1)\n",
    "axs[0].plot(ddpg_min_r, 'r', linewidth=1)\n",
    "axs[0].plot(ddpg_mean_r, 'r:', label='DDPG', linewidth=2)\n",
    "axs[0].fill_between(\n",
    "    ddpg_x, ddpg_min_r, ddpg_max_r, facecolor='r', alpha=0.3)\n",
    "\n",
    "axs[1].plot(ddpg_max_s, 'r', linewidth=1)\n",
    "axs[1].plot(ddpg_min_s, 'r', linewidth=1)\n",
    "axs[1].plot(ddpg_mean_s, 'r:', label='DDPG', linewidth=2)\n",
    "axs[1].fill_between(\n",
    "    ddpg_x, ddpg_min_s, ddpg_max_s, facecolor='r', alpha=0.3)\n",
    "\n",
    "axs[2].plot(ddpg_max_exp, 'r', linewidth=1)\n",
    "axs[2].plot(ddpg_min_exp, 'r', linewidth=1)\n",
    "axs[2].plot(ddpg_mean_exp, 'r:', label='DDPG', linewidth=2)\n",
    "axs[2].fill_between(\n",
    "    ddpg_x, ddpg_min_exp, ddpg_max_exp, facecolor='r', alpha=0.3)\n",
    "\n",
    "# ALL\n",
    "axs[0].set_title('Moving Avg Reward (Training)')\n",
    "axs[1].set_title('Moving Avg Reward (Evaluation)')\n",
    "axs[2].set_title('Moving Noise')\n",
    "plt.xlabel('Episodes')\n",
    "axs[0].legend(loc='upper left')\n",
    "plt.savefig(\"progress.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del ddpg_results\n",
    "print(gc.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_agent.demo_last(title=\"last\")\n",
    "print('done')\n",
    "# best_agent.demo_progression(title=\"progress\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 1, figsize=(15,15), sharey=False, sharex=True)\n",
    "\n",
    "# DDPG\n",
    "axs[0].plot(ddpg_max_t, 'r', linewidth=1)\n",
    "axs[0].plot(ddpg_min_t, 'r', linewidth=1)\n",
    "axs[0].plot(ddpg_mean_t, 'r:', label='DDPG', linewidth=2)\n",
    "axs[0].fill_between(\n",
    "    ddpg_x, ddpg_min_t, ddpg_max_t, facecolor='r', alpha=0.3)\n",
    "\n",
    "axs[1].plot(ddpg_max_sec, 'r', linewidth=1)\n",
    "axs[1].plot(ddpg_min_sec, 'r', linewidth=1)\n",
    "axs[1].plot(ddpg_mean_sec, 'r:', label='DDPG', linewidth=2)\n",
    "axs[1].fill_between(\n",
    "    ddpg_x, ddpg_min_sec, ddpg_max_sec, facecolor='r', alpha=0.3)\n",
    "\n",
    "axs[2].plot(ddpg_max_rt, 'r', linewidth=1)\n",
    "axs[2].plot(ddpg_min_rt, 'r', linewidth=1)\n",
    "axs[2].plot(ddpg_mean_rt, 'r:', label='DDPG', linewidth=2)\n",
    "axs[2].fill_between(\n",
    "    ddpg_x, ddpg_min_rt, ddpg_max_rt, facecolor='r', alpha=0.3)\n",
    "\n",
    "# ALL\n",
    "axs[0].set_title('Total Steps')\n",
    "axs[1].set_title('Training Time')\n",
    "axs[2].set_title('Wall-clock Time')\n",
    "plt.xlabel('Episodes')\n",
    "axs[0].legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_root_dir = os.path.join(RESULTS_DIR, 'ddpg')\n",
    "not os.path.exists(ddpg_root_dir) and os.makedirs(ddpg_root_dir)\n",
    "\n",
    "np.save(os.path.join(ddpg_root_dir, 'x'), ddpg_x)\n",
    "\n",
    "np.save(os.path.join(ddpg_root_dir, 'max_r'), ddpg_max_r)\n",
    "np.save(os.path.join(ddpg_root_dir, 'min_r'), ddpg_min_r)\n",
    "np.save(os.path.join(ddpg_root_dir, 'mean_r'), ddpg_mean_r)\n",
    "\n",
    "np.save(os.path.join(ddpg_root_dir, 'max_s'), ddpg_max_s)\n",
    "np.save(os.path.join(ddpg_root_dir, 'min_s'), ddpg_min_s )\n",
    "np.save(os.path.join(ddpg_root_dir, 'mean_s'), ddpg_mean_s)\n",
    "\n",
    "np.save(os.path.join(ddpg_root_dir, 'max_t'), ddpg_max_t)\n",
    "np.save(os.path.join(ddpg_root_dir, 'min_t'), ddpg_min_t)\n",
    "np.save(os.path.join(ddpg_root_dir, 'mean_t'), ddpg_mean_t)\n",
    "\n",
    "np.save(os.path.join(ddpg_root_dir, 'max_sec'), ddpg_max_sec)\n",
    "np.save(os.path.join(ddpg_root_dir, 'min_sec'), ddpg_min_sec)\n",
    "np.save(os.path.join(ddpg_root_dir, 'mean_sec'), ddpg_mean_sec)\n",
    "\n",
    "np.save(os.path.join(ddpg_root_dir, 'max_rt'), ddpg_max_rt)\n",
    "np.save(os.path.join(ddpg_root_dir, 'min_rt'), ddpg_min_rt)\n",
    "np.save(os.path.join(ddpg_root_dir, 'mean_rt'), ddpg_mean_rt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TD3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalNoiseDecayStrategy():\n",
    "    def __init__(self, bounds, init_noise_ratio=0.5, min_noise_ratio=0.1, decay_steps=10000):\n",
    "        self.t = 0\n",
    "        self.low, self.high = bounds\n",
    "        self.noise_ratio = init_noise_ratio\n",
    "        self.init_noise_ratio = init_noise_ratio\n",
    "        self.min_noise_ratio = min_noise_ratio\n",
    "        self.decay_steps = decay_steps\n",
    "        self.ratio_noise_injected = 0\n",
    "\n",
    "    def _noise_ratio_update(self):\n",
    "        noise_ratio = 1 - self.t / self.decay_steps\n",
    "        noise_ratio = (self.init_noise_ratio - self.min_noise_ratio) * noise_ratio + self.min_noise_ratio\n",
    "        noise_ratio = np.clip(noise_ratio, self.min_noise_ratio, self.init_noise_ratio)\n",
    "        self.t += 1\n",
    "        return noise_ratio\n",
    "\n",
    "    def select_action(self, model, state, max_exploration=False):\n",
    "        if max_exploration:\n",
    "            noise_scale = self.high\n",
    "        else:\n",
    "            noise_scale = self.noise_ratio * self.high\n",
    "\n",
    "        with torch.no_grad():\n",
    "            greedy_action = model(state).cpu().detach().data.numpy().squeeze()\n",
    "\n",
    "        noise = np.random.normal(loc=0, scale=noise_scale, size=len(self.high))\n",
    "        noisy_action = greedy_action + noise\n",
    "        action = np.clip(noisy_action, self.low, self.high)\n",
    "\n",
    "        self.noise_ratio = self._noise_ratio_update()\n",
    "        self.ratio_noise_injected = np.mean(abs((greedy_action - action)/(self.high - self.low)))\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = NormalNoiseDecayStrategy(([-2],[2]))\n",
    "plt.plot([s._noise_ratio_update() for _ in range(50000)])\n",
    "plt.title('Normal Noise Linear ratio')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCTQV(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim, \n",
    "                 output_dim, \n",
    "                 hidden_dims=(32,32), \n",
    "                 activation_fc=F.relu):\n",
    "        super(FCTQV, self).__init__()\n",
    "        self.activation_fc = activation_fc\n",
    "\n",
    "        self.input_layer_a = nn.Linear(input_dim + output_dim, hidden_dims[0])\n",
    "        self.input_layer_b = nn.Linear(input_dim + output_dim, hidden_dims[0])\n",
    "\n",
    "        self.hidden_layers_a = nn.ModuleList()\n",
    "        self.hidden_layers_b = nn.ModuleList()\n",
    "        for i in range(len(hidden_dims)-1):\n",
    "            hidden_layer_a = nn.Linear(hidden_dims[i], hidden_dims[i+1])\n",
    "            self.hidden_layers_a.append(hidden_layer_a)\n",
    "\n",
    "            hidden_layer_b = nn.Linear(hidden_dims[i], hidden_dims[i+1])\n",
    "            self.hidden_layers_b.append(hidden_layer_b)\n",
    "\n",
    "        self.output_layer_a = nn.Linear(hidden_dims[-1], 1)\n",
    "        self.output_layer_b = nn.Linear(hidden_dims[-1], 1)\n",
    "\n",
    "        device = \"cpu\"\n",
    "        if torch.cuda.is_available():\n",
    "            device = \"cuda:0\"\n",
    "        self.device = torch.device(device)\n",
    "        self.to(self.device)\n",
    "\n",
    "    def _format(self, state, action):\n",
    "        x, u = state, action\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.tensor(x, \n",
    "                             device=self.device, \n",
    "                             dtype=torch.float32)\n",
    "            x = x.unsqueeze(0)\n",
    "        if not isinstance(u, torch.Tensor):\n",
    "            u = torch.tensor(u, \n",
    "                             device=self.device, \n",
    "                             dtype=torch.float32)\n",
    "            u = u.unsqueeze(0)\n",
    "        return x, u\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x, u = self._format(state, action)\n",
    "        x = torch.cat((x, u), dim=1)\n",
    "        xa = self.activation_fc(self.input_layer_a(x))\n",
    "        xb = self.activation_fc(self.input_layer_b(x))\n",
    "        for hidden_layer_a, hidden_layer_b in zip(self.hidden_layers_a, self.hidden_layers_b):\n",
    "            xa = self.activation_fc(hidden_layer_a(xa))\n",
    "            xb = self.activation_fc(hidden_layer_b(xb))\n",
    "        xa = self.output_layer_a(xa)\n",
    "        xb = self.output_layer_b(xb)\n",
    "        return xa, xb\n",
    "    \n",
    "    def Qa(self, state, action):\n",
    "        x, u = self._format(state, action)\n",
    "        x = torch.cat((x, u), dim=1)\n",
    "        xa = self.activation_fc(self.input_layer_a(x))\n",
    "        for hidden_layer_a in self.hidden_layers_a:\n",
    "            xa = self.activation_fc(hidden_layer_a(xa))\n",
    "        return self.output_layer_a(xa)\n",
    "    \n",
    "    def load(self, experiences):\n",
    "        states, actions, new_states, rewards, is_terminals = experiences\n",
    "        states = torch.from_numpy(states).float().to(self.device)\n",
    "        actions = torch.from_numpy(actions).float().to(self.device)\n",
    "        new_states = torch.from_numpy(new_states).float().to(self.device)\n",
    "        rewards = torch.from_numpy(rewards).float().to(self.device)\n",
    "        is_terminals = torch.from_numpy(is_terminals).float().to(self.device)\n",
    "        return states, actions, new_states, rewards, is_terminals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TD3():\n",
    "    def __init__(self, \n",
    "                 replay_buffer_fn,\n",
    "                 policy_model_fn, \n",
    "                 policy_max_grad_norm, \n",
    "                 policy_optimizer_fn, \n",
    "                 policy_optimizer_lr,\n",
    "                 value_model_fn, \n",
    "                 value_max_grad_norm, \n",
    "                 value_optimizer_fn, \n",
    "                 value_optimizer_lr, \n",
    "                 training_strategy_fn,\n",
    "                 evaluation_strategy_fn,\n",
    "                 n_warmup_batches,\n",
    "                 update_value_target_every_steps,\n",
    "                 update_policy_target_every_steps,\n",
    "                 train_policy_every_steps,\n",
    "                 tau,\n",
    "                 policy_noise_ratio,\n",
    "                 policy_noise_clip_ratio):\n",
    "        self.replay_buffer_fn = replay_buffer_fn\n",
    "\n",
    "        self.policy_model_fn = policy_model_fn\n",
    "        self.policy_max_grad_norm = policy_max_grad_norm\n",
    "        self.policy_optimizer_fn = policy_optimizer_fn\n",
    "        self.policy_optimizer_lr = policy_optimizer_lr\n",
    "        \n",
    "        self.value_model_fn = value_model_fn\n",
    "        self.value_max_grad_norm = value_max_grad_norm\n",
    "        self.value_optimizer_fn = value_optimizer_fn\n",
    "        self.value_optimizer_lr = value_optimizer_lr\n",
    "\n",
    "        self.training_strategy_fn = training_strategy_fn\n",
    "        self.evaluation_strategy_fn = evaluation_strategy_fn\n",
    "\n",
    "        self.n_warmup_batches = n_warmup_batches\n",
    "        self.update_value_target_every_steps = update_value_target_every_steps\n",
    "        self.update_policy_target_every_steps = update_policy_target_every_steps\n",
    "        self.train_policy_every_steps = train_policy_every_steps\n",
    "        \n",
    "        self.tau = tau\n",
    "        self.policy_noise_ratio = policy_noise_ratio\n",
    "        self.policy_noise_clip_ratio = policy_noise_clip_ratio\n",
    "\n",
    "    def optimize_model(self, experiences):\n",
    "        states, actions, rewards, next_states, is_terminals = experiences\n",
    "        batch_size = len(is_terminals)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            a_ran = self.target_policy_model.env_max - self.target_policy_model.env_min\n",
    "            a_noise = torch.randn_like(actions) * self.policy_noise_ratio * a_ran\n",
    "            n_min = self.target_policy_model.env_min * self.policy_noise_clip_ratio\n",
    "            n_max = self.target_policy_model.env_max * self.policy_noise_clip_ratio            \n",
    "            a_noise = torch.max(torch.min(a_noise, n_max), n_min)\n",
    "\n",
    "            argmax_a_q_sp = self.target_policy_model(next_states)\n",
    "            noisy_argmax_a_q_sp = argmax_a_q_sp + a_noise\n",
    "            noisy_argmax_a_q_sp = torch.max(torch.min(noisy_argmax_a_q_sp, \n",
    "                                                      self.target_policy_model.env_max),\n",
    "                                            self.target_policy_model.env_min)\n",
    "\n",
    "            max_a_q_sp_a, max_a_q_sp_b = self.target_value_model(next_states, noisy_argmax_a_q_sp)\n",
    "            max_a_q_sp = torch.min(max_a_q_sp_a, max_a_q_sp_b)\n",
    "\n",
    "            target_q_sa = rewards + self.gamma * max_a_q_sp * (1 - is_terminals)\n",
    "\n",
    "        q_sa_a, q_sa_b = self.online_value_model(states, actions)\n",
    "        td_error_a = q_sa_a - target_q_sa\n",
    "        td_error_b = q_sa_b - target_q_sa\n",
    "\n",
    "        value_loss = td_error_a.pow(2).mul(0.5).mean() + td_error_b.pow(2).mul(0.5).mean()\n",
    "        self.value_optimizer.zero_grad()\n",
    "        value_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.online_value_model.parameters(), \n",
    "                                       self.value_max_grad_norm)\n",
    "        self.value_optimizer.step()\n",
    "\n",
    "        if np.sum(self.episode_timestep) % self.train_policy_every_steps == 0:\n",
    "            argmax_a_q_s = self.online_policy_model(states)\n",
    "            max_a_q_s = self.online_value_model.Qa(states, argmax_a_q_s)\n",
    "\n",
    "            policy_loss = -max_a_q_s.mean()\n",
    "            self.policy_optimizer.zero_grad()\n",
    "            policy_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.online_policy_model.parameters(), \n",
    "                                           self.policy_max_grad_norm)        \n",
    "            self.policy_optimizer.step()\n",
    "\n",
    "    def interaction_step(self, state, env):\n",
    "        min_samples = self.replay_buffer.batch_size * self.n_warmup_batches\n",
    "        action = self.training_strategy.select_action(self.online_policy_model, \n",
    "                                                      state, \n",
    "                                                      len(self.replay_buffer) < min_samples)\n",
    "        new_state, reward, is_terminal, info = env.step(action)\n",
    "        is_truncated = 'TimeLimit.truncated' in info and info['TimeLimit.truncated']\n",
    "        is_failure = is_terminal and not is_truncated\n",
    "        experience = (state, action, reward, new_state, float(is_failure))\n",
    "\n",
    "        self.replay_buffer.store(experience)\n",
    "        self.episode_reward[-1] += reward\n",
    "        self.episode_timestep[-1] += 1\n",
    "        self.episode_exploration[-1] += self.training_strategy.ratio_noise_injected\n",
    "        return new_state, is_terminal\n",
    "\n",
    "    def update_value_network(self, tau=None):\n",
    "        tau = self.tau if tau is None else tau\n",
    "        for target, online in zip(self.target_value_model.parameters(), \n",
    "                                  self.online_value_model.parameters()):\n",
    "            target_ratio = (1.0 - tau) * target.data\n",
    "            online_ratio = tau * online.data\n",
    "            mixed_weights = target_ratio + online_ratio\n",
    "            target.data.copy_(mixed_weights)\n",
    "\n",
    "    def update_policy_network(self, tau=None):\n",
    "        tau = self.tau if tau is None else tau\n",
    "        for target, online in zip(self.target_policy_model.parameters(), \n",
    "                                  self.online_policy_model.parameters()):\n",
    "            target_ratio = (1.0 - tau) * target.data\n",
    "            online_ratio = tau * online.data\n",
    "            mixed_weights = target_ratio + online_ratio\n",
    "            target.data.copy_(mixed_weights)\n",
    "\n",
    "    def train(self, make_env_fn, make_env_kargs, seed, gamma, \n",
    "              max_minutes, max_episodes, goal_mean_100_reward):\n",
    "        training_start, last_debug_time = time.time(), float('-inf')\n",
    "\n",
    "        self.checkpoint_dir = tempfile.mkdtemp()\n",
    "        self.make_env_fn = make_env_fn\n",
    "        self.make_env_kargs = make_env_kargs\n",
    "        self.seed = seed\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        env = self.make_env_fn(**self.make_env_kargs, seed=self.seed)\n",
    "        torch.manual_seed(self.seed) ; np.random.seed(self.seed) ; random.seed(self.seed)\n",
    "    \n",
    "        nS, nA = env.observation_space.shape[0], env.action_space.shape[0]\n",
    "        action_bounds = env.action_space.low, env.action_space.high\n",
    "        self.episode_timestep = []\n",
    "        self.episode_reward = []\n",
    "        self.episode_seconds = []\n",
    "        self.evaluation_scores = []        \n",
    "        self.episode_exploration = []\n",
    "        \n",
    "        self.target_value_model = self.value_model_fn(nS, nA)\n",
    "        self.online_value_model = self.value_model_fn(nS, nA)\n",
    "        self.update_value_network(tau=1.0)\n",
    "\n",
    "        self.target_policy_model = self.policy_model_fn(nS, action_bounds)\n",
    "        self.online_policy_model = self.policy_model_fn(nS, action_bounds)\n",
    "        self.update_policy_network(tau=1.0)\n",
    "\n",
    "        self.value_optimizer = self.value_optimizer_fn(self.online_value_model, \n",
    "                                                       self.value_optimizer_lr)        \n",
    "        self.policy_optimizer = self.policy_optimizer_fn(self.online_policy_model, \n",
    "                                                         self.policy_optimizer_lr)\n",
    "\n",
    "        self.replay_buffer = self.replay_buffer_fn()\n",
    "        self.training_strategy = training_strategy_fn(action_bounds)\n",
    "        self.evaluation_strategy = evaluation_strategy_fn(action_bounds)\n",
    "                    \n",
    "        result = np.empty((max_episodes, 5))\n",
    "        result[:] = np.nan\n",
    "        training_time = 0\n",
    "        for episode in range(1, max_episodes + 1):\n",
    "            episode_start = time.time()\n",
    "            \n",
    "            state, is_terminal = env.reset(), False\n",
    "            self.episode_reward.append(0.0)\n",
    "            self.episode_timestep.append(0.0)\n",
    "            self.episode_exploration.append(0.0)\n",
    "\n",
    "            for step in count():\n",
    "                state, is_terminal = self.interaction_step(state, env)\n",
    "\n",
    "                min_samples = self.replay_buffer.batch_size * self.n_warmup_batches\n",
    "                if len(self.replay_buffer) > min_samples:\n",
    "                    experiences = self.replay_buffer.sample()\n",
    "                    experiences = self.online_value_model.load(experiences)\n",
    "                    self.optimize_model(experiences)\n",
    "\n",
    "                if np.sum(self.episode_timestep) % self.update_value_target_every_steps == 0:\n",
    "                    self.update_value_network()\n",
    "\n",
    "                if np.sum(self.episode_timestep) % self.update_policy_target_every_steps == 0:\n",
    "                    self.update_policy_network()\n",
    "\n",
    "                if is_terminal:\n",
    "                    gc.collect()\n",
    "                    break\n",
    "            \n",
    "            # stats\n",
    "            episode_elapsed = time.time() - episode_start\n",
    "            self.episode_seconds.append(episode_elapsed)\n",
    "            training_time += episode_elapsed\n",
    "            evaluation_score, _ = self.evaluate(self.online_policy_model, env)\n",
    "            self.save_checkpoint(episode-1, self.online_policy_model)\n",
    "\n",
    "            total_step = int(np.sum(self.episode_timestep))\n",
    "            self.evaluation_scores.append(evaluation_score)\n",
    "            \n",
    "            mean_10_reward = np.mean(self.episode_reward[-10:])\n",
    "            std_10_reward = np.std(self.episode_reward[-10:])\n",
    "            mean_100_reward = np.mean(self.episode_reward[-100:])\n",
    "            std_100_reward = np.std(self.episode_reward[-100:])\n",
    "            mean_100_eval_score = np.mean(self.evaluation_scores[-100:])\n",
    "            std_100_eval_score = np.std(self.evaluation_scores[-100:])\n",
    "            lst_100_exp_rat = np.array(\n",
    "                self.episode_exploration[-100:])/np.array(self.episode_timestep[-100:])\n",
    "            mean_100_exp_rat = np.mean(lst_100_exp_rat)\n",
    "            std_100_exp_rat = np.std(lst_100_exp_rat)\n",
    "            \n",
    "            wallclock_elapsed = time.time() - training_start\n",
    "            result[episode-1] = total_step, mean_100_reward, \\\n",
    "                mean_100_eval_score, training_time, wallclock_elapsed\n",
    "            \n",
    "            reached_debug_time = time.time() - last_debug_time >= LEAVE_PRINT_EVERY_N_SECS\n",
    "            reached_max_minutes = wallclock_elapsed >= max_minutes * 60\n",
    "            reached_max_episodes = episode >= max_episodes\n",
    "            reached_goal_mean_reward = mean_100_eval_score >= goal_mean_100_reward\n",
    "            training_is_over = reached_max_minutes or \\\n",
    "                               reached_max_episodes or \\\n",
    "                               reached_goal_mean_reward\n",
    "            elapsed_str = time.strftime(\"%H:%M:%S\", time.gmtime(time.time() - training_start))\n",
    "            debug_message = 'el {}, ep {:04}, ts {:07}, '\n",
    "            debug_message += 'ar 10 {:05.1f}\\u00B1{:05.1f}, '\n",
    "            debug_message += '100 {:05.1f}\\u00B1{:05.1f}, '\n",
    "            debug_message += 'ex 100 {:02.1f}\\u00B1{:02.1f}, '\n",
    "            debug_message += 'ev {:05.1f}\\u00B1{:05.1f}'\n",
    "            debug_message = debug_message.format(\n",
    "                elapsed_str, episode-1, total_step, mean_10_reward, std_10_reward, \n",
    "                mean_100_reward, std_100_reward, mean_100_exp_rat, std_100_exp_rat,\n",
    "                mean_100_eval_score, std_100_eval_score)\n",
    "            print(debug_message, end='\\r', flush=True)\n",
    "            if reached_debug_time or training_is_over:\n",
    "                print(ERASE_LINE + debug_message, flush=True)\n",
    "                last_debug_time = time.time()\n",
    "            if training_is_over:\n",
    "                if reached_max_minutes: print(u'--> reached_max_minutes \\u2715')\n",
    "                if reached_max_episodes: print(u'--> reached_max_episodes \\u2715')\n",
    "                if reached_goal_mean_reward: print(u'--> reached_goal_mean_reward \\u2713')\n",
    "                break\n",
    "                \n",
    "        final_eval_score, score_std = self.evaluate(self.online_policy_model, env, n_episodes=100)\n",
    "        wallclock_time = time.time() - training_start\n",
    "        print('Training complete.')\n",
    "        print('Final evaluation score {:.2f}\\u00B1{:.2f} in {:.2f}s training time,'\n",
    "              ' {:.2f}s wall-clock time.\\n'.format(\n",
    "                  final_eval_score, score_std, training_time, wallclock_time))\n",
    "        env.close() ; del env\n",
    "        self.get_cleaned_checkpoints()\n",
    "        return result, final_eval_score, training_time, wallclock_time\n",
    "    \n",
    "    def evaluate(self, eval_policy_model, eval_env, n_episodes=1):\n",
    "        rs = []\n",
    "        for _ in range(n_episodes):\n",
    "            s, d = eval_env.reset(), False\n",
    "            rs.append(0)\n",
    "            for _ in count():\n",
    "                a = self.evaluation_strategy.select_action(eval_policy_model, s)\n",
    "                s, r, d, _ = eval_env.step(a)\n",
    "                rs[-1] += r\n",
    "                if d: break\n",
    "        return np.mean(rs), np.std(rs)\n",
    "\n",
    "    def get_cleaned_checkpoints(self, n_checkpoints=4):\n",
    "        try: \n",
    "            return self.checkpoint_paths\n",
    "        except AttributeError:\n",
    "            self.checkpoint_paths = {}\n",
    "\n",
    "        paths = glob.glob(os.path.join(self.checkpoint_dir, '*.tar'))\n",
    "        paths_dic = {int(path.split('.')[-2]):path for path in paths}\n",
    "        last_ep = max(paths_dic.keys())\n",
    "        # checkpoint_idxs = np.geomspace(1, last_ep+1, n_checkpoints, endpoint=True, dtype=np.int)-1\n",
    "        checkpoint_idxs = np.linspace(1, last_ep+1, n_checkpoints, endpoint=True, dtype=np.int)-1\n",
    "\n",
    "        for idx, path in paths_dic.items():\n",
    "            if idx in checkpoint_idxs:\n",
    "                self.checkpoint_paths[idx] = path\n",
    "            else:\n",
    "                os.unlink(path)\n",
    "\n",
    "        return self.checkpoint_paths\n",
    "\n",
    "    def demo_last(self, title='Fully-trained {} Agent', n_episodes=2, max_n_videos=2):\n",
    "        env = self.make_env_fn(**self.make_env_kargs, monitor_mode='evaluation', render=True, record=True)\n",
    "\n",
    "        checkpoint_paths = self.get_cleaned_checkpoints()\n",
    "        last_ep = max(checkpoint_paths.keys())\n",
    "        self.online_policy_model.load_state_dict(torch.load(checkpoint_paths[last_ep]))\n",
    "\n",
    "        self.evaluate(self.online_policy_model, env, n_episodes=n_episodes)\n",
    "        env.close()\n",
    "        data = get_gif_html(env_videos=env.videos, \n",
    "                            title=title.format(self.__class__.__name__),\n",
    "                            max_n_videos=max_n_videos)\n",
    "        del env\n",
    "        return HTML(data=data)\n",
    "\n",
    "    def demo_progression(self, title='{} Agent progression', max_n_videos=4):\n",
    "        env = self.make_env_fn(**self.make_env_kargs, monitor_mode='evaluation', render=True, record=True)\n",
    "\n",
    "        checkpoint_paths = self.get_cleaned_checkpoints()\n",
    "        for i in sorted(checkpoint_paths.keys()):\n",
    "            self.online_policy_model.load_state_dict(torch.load(checkpoint_paths[i]))\n",
    "            self.evaluate(self.online_policy_model, env, n_episodes=1)\n",
    "\n",
    "        env.close()\n",
    "        data = get_gif_html(env_videos=env.videos, \n",
    "                            title=title.format(self.__class__.__name__),\n",
    "                            subtitle_eps=sorted(checkpoint_paths.keys()),\n",
    "                            max_n_videos=max_n_videos)\n",
    "        del env\n",
    "        return HTML(data=data)\n",
    "\n",
    "    def save_checkpoint(self, episode_idx, model):\n",
    "        torch.save(model.state_dict(), \n",
    "                   os.path.join(self.checkpoint_dir, 'model.{}.tar'.format(episode_idx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "td3_results = []\n",
    "best_agent, best_eval_score = None, float('-inf')\n",
    "for seed in SEEDS:\n",
    "    environment_settings = {\n",
    "        'env_name': 'HopperBulletEnv-v0',\n",
    "        'gamma': 0.99,\n",
    "        'max_minutes': 300,\n",
    "        'max_episodes': 10000,\n",
    "        'goal_mean_100_reward': 1500\n",
    "    }\n",
    "    \n",
    "    policy_model_fn = lambda nS, bounds: FCDP(nS, bounds, hidden_dims=(256,256))\n",
    "    policy_max_grad_norm = float('inf')\n",
    "    policy_optimizer_fn = lambda net, lr: optim.Adam(net.parameters(), lr=lr)\n",
    "    policy_optimizer_lr = 0.0003\n",
    "\n",
    "    value_model_fn = lambda nS, nA: FCTQV(nS, nA, hidden_dims=(256,256))\n",
    "    value_max_grad_norm = float('inf')\n",
    "    value_optimizer_fn = lambda net, lr: optim.Adam(net.parameters(), lr=lr)\n",
    "    value_optimizer_lr = 0.0003\n",
    "\n",
    "    training_strategy_fn = lambda bounds: NormalNoiseDecayStrategy(bounds,\n",
    "                                                                   init_noise_ratio=0.5,\n",
    "                                                                   min_noise_ratio=0.1,\n",
    "                                                                   decay_steps=200000)\n",
    "    evaluation_strategy_fn = lambda bounds: GreedyStrategy(bounds)\n",
    "\n",
    "    replay_buffer_fn = lambda: ReplayBuffer(max_size=1000000, batch_size=256)\n",
    "    n_warmup_batches = 5\n",
    "    update_value_target_every_steps = 2\n",
    "    update_policy_target_every_steps = 2\n",
    "    train_policy_every_steps = 2\n",
    "    policy_noise_ratio = 0.1\n",
    "    policy_noise_clip_ratio = 0.5\n",
    "    tau = 0.01\n",
    "\n",
    "    env_name, gamma, max_minutes, \\\n",
    "    max_episodes, goal_mean_100_reward = environment_settings.values()\n",
    "\n",
    "    agent = TD3(replay_buffer_fn,\n",
    "                policy_model_fn, \n",
    "                policy_max_grad_norm, \n",
    "                policy_optimizer_fn, \n",
    "                policy_optimizer_lr,\n",
    "                value_model_fn,\n",
    "                value_max_grad_norm, \n",
    "                value_optimizer_fn, \n",
    "                value_optimizer_lr, \n",
    "                training_strategy_fn,\n",
    "                evaluation_strategy_fn,\n",
    "                n_warmup_batches,\n",
    "                update_value_target_every_steps,\n",
    "                update_policy_target_every_steps,\n",
    "                train_policy_every_steps,\n",
    "                tau,\n",
    "                policy_noise_ratio,\n",
    "                policy_noise_clip_ratio)\n",
    "\n",
    "    make_env_fn, make_env_kargs = get_make_env_fn(env_name=env_name, inner_wrappers=[RenderUint8])\n",
    "    result, final_eval_score, training_time, wallclock_time = agent.train(\n",
    "        make_env_fn, make_env_kargs, seed, gamma, max_minutes, max_episodes, goal_mean_100_reward)\n",
    "    td3_results.append(result)\n",
    "    if final_eval_score > best_eval_score:\n",
    "        best_eval_score = final_eval_score\n",
    "        best_agent = agent\n",
    "td3_results = np.array(td3_results)\n",
    "_ = BEEP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "best_agent.demo_progression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_agent.demo_last()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td3_max_t, td3_max_r, td3_max_s, \\\n",
    "td3_max_sec, td3_max_rt = np.max(td3_results, axis=0).T\n",
    "td3_min_t, td3_min_r, td3_min_s, \\\n",
    "td3_min_sec, td3_min_rt = np.min(td3_results, axis=0).T\n",
    "td3_mean_t, td3_mean_r, td3_mean_s, \\\n",
    "td3_mean_sec, td3_mean_rt = np.mean(td3_results, axis=0).T\n",
    "td3_x = np.arange(len(td3_mean_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 1, figsize=(15,10), sharey=False, sharex=True)\n",
    "\n",
    "# TD3\n",
    "axs[0].plot(td3_max_r, 'b', linewidth=1)\n",
    "axs[0].plot(td3_min_r, 'b', linewidth=1)\n",
    "axs[0].plot(td3_mean_r, 'b:', label='TD3', linewidth=2)\n",
    "axs[0].fill_between(\n",
    "    td3_x, td3_min_r, td3_max_r, facecolor='b', alpha=0.3)\n",
    "\n",
    "axs[1].plot(td3_max_s, 'b', linewidth=1)\n",
    "axs[1].plot(td3_min_s, 'b', linewidth=1)\n",
    "axs[1].plot(td3_mean_s, 'b:', label='TD3', linewidth=2)\n",
    "axs[1].fill_between(\n",
    "    td3_x, td3_min_s, td3_max_s, facecolor='b', alpha=0.3)\n",
    "\n",
    "# ALL\n",
    "axs[0].set_title('Moving Avg Reward (Training)')\n",
    "axs[1].set_title('Moving Avg Reward (Evaluation)')\n",
    "plt.xlabel('Episodes')\n",
    "axs[0].legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 1, figsize=(15,15), sharey=False, sharex=True)\n",
    "\n",
    "# TD3\n",
    "axs[0].plot(td3_max_t, 'b', linewidth=1)\n",
    "axs[0].plot(td3_min_t, 'b', linewidth=1)\n",
    "axs[0].plot(td3_mean_t, 'b:', label='TD3', linewidth=2)\n",
    "axs[0].fill_between(\n",
    "    td3_x, td3_min_t, td3_max_t, facecolor='b', alpha=0.3)\n",
    "\n",
    "axs[1].plot(td3_max_sec, 'b', linewidth=1)\n",
    "axs[1].plot(td3_min_sec, 'b', linewidth=1)\n",
    "axs[1].plot(td3_mean_sec, 'b:', label='TD3', linewidth=2)\n",
    "axs[1].fill_between(\n",
    "    td3_x, td3_min_sec, td3_max_sec, facecolor='b', alpha=0.3)\n",
    "\n",
    "axs[2].plot(td3_max_rt, 'b', linewidth=1)\n",
    "axs[2].plot(td3_min_rt, 'b', linewidth=1)\n",
    "axs[2].plot(td3_mean_rt, 'b:', label='TD3', linewidth=2)\n",
    "axs[2].fill_between(\n",
    "    td3_x, td3_min_rt, td3_max_rt, facecolor='b', alpha=0.3)\n",
    "\n",
    "# ALL\n",
    "axs[0].set_title('Total Steps')\n",
    "axs[1].set_title('Training Time')\n",
    "axs[2].set_title('Wall-clock Time')\n",
    "plt.xlabel('Episodes')\n",
    "axs[0].legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td3_root_dir = os.path.join(RESULTS_DIR, 'td3')\n",
    "not os.path.exists(td3_root_dir) and os.makedirs(td3_root_dir)\n",
    "\n",
    "np.save(os.path.join(td3_root_dir, 'x'), td3_x)\n",
    "\n",
    "np.save(os.path.join(td3_root_dir, 'max_r'), td3_max_r)\n",
    "np.save(os.path.join(td3_root_dir, 'min_r'), td3_min_r)\n",
    "np.save(os.path.join(td3_root_dir, 'mean_r'), td3_mean_r)\n",
    "\n",
    "np.save(os.path.join(td3_root_dir, 'max_s'), td3_max_s)\n",
    "np.save(os.path.join(td3_root_dir, 'min_s'), td3_min_s )\n",
    "np.save(os.path.join(td3_root_dir, 'mean_s'), td3_mean_s)\n",
    "\n",
    "np.save(os.path.join(td3_root_dir, 'max_t'), td3_max_t)\n",
    "np.save(os.path.join(td3_root_dir, 'min_t'), td3_min_t)\n",
    "np.save(os.path.join(td3_root_dir, 'mean_t'), td3_mean_t)\n",
    "\n",
    "np.save(os.path.join(td3_root_dir, 'max_sec'), td3_max_sec)\n",
    "np.save(os.path.join(td3_root_dir, 'min_sec'), td3_min_sec)\n",
    "np.save(os.path.join(td3_root_dir, 'mean_sec'), td3_mean_sec)\n",
    "\n",
    "np.save(os.path.join(td3_root_dir, 'max_rt'), td3_max_rt)\n",
    "np.save(os.path.join(td3_root_dir, 'min_rt'), td3_min_rt)\n",
    "np.save(os.path.join(td3_root_dir, 'mean_rt'), td3_mean_rt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCQSA(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim, \n",
    "                 output_dim, \n",
    "                 hidden_dims=(32,32), \n",
    "                 activation_fc=F.relu):\n",
    "        super(FCQSA, self).__init__()\n",
    "        self.activation_fc = activation_fc\n",
    "\n",
    "        self.input_layer = nn.Linear(input_dim + output_dim, hidden_dims[0])\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for i in range(len(hidden_dims)-1):\n",
    "            hidden_layer = nn.Linear(hidden_dims[i], hidden_dims[i+1])\n",
    "            self.hidden_layers.append(hidden_layer)\n",
    "        self.output_layer = nn.Linear(hidden_dims[-1], 1)\n",
    "\n",
    "        device = \"cpu\"\n",
    "        if torch.cuda.is_available():\n",
    "            device = \"cuda:0\"\n",
    "        self.device = torch.device(device)\n",
    "        self.to(self.device)\n",
    "\n",
    "    def _format(self, state, action):\n",
    "        x, u = state, action\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.tensor(x, \n",
    "                             device=self.device, \n",
    "                             dtype=torch.float32)\n",
    "            x = x.unsqueeze(0)\n",
    "        if not isinstance(u, torch.Tensor):\n",
    "            u = torch.tensor(u, \n",
    "                             device=self.device, \n",
    "                             dtype=torch.float32)\n",
    "            u = u.unsqueeze(0)\n",
    "        return x, u\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x, u = self._format(state, action)\n",
    "        x = self.activation_fc(self.input_layer(torch.cat((x, u), dim=1)))\n",
    "        for i, hidden_layer in enumerate(self.hidden_layers):\n",
    "            x = self.activation_fc(hidden_layer(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "    \n",
    "    def load(self, experiences):\n",
    "        states, actions, new_states, rewards, is_terminals = experiences\n",
    "        states = torch.from_numpy(states).float().to(self.device)\n",
    "        actions = torch.from_numpy(actions).float().to(self.device)\n",
    "        new_states = torch.from_numpy(new_states).float().to(self.device)\n",
    "        rewards = torch.from_numpy(rewards).float().to(self.device)\n",
    "        is_terminals = torch.from_numpy(is_terminals).float().to(self.device)\n",
    "        return states, actions, new_states, rewards, is_terminals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCGP(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim, \n",
    "                 action_bounds,\n",
    "                 log_std_min=-20, \n",
    "                 log_std_max=2,\n",
    "                 hidden_dims=(32,32), \n",
    "                 activation_fc=F.relu,\n",
    "                 entropy_lr=0.001):\n",
    "        super(FCGP, self).__init__()\n",
    "        self.activation_fc = activation_fc\n",
    "        self.env_min, self.env_max = action_bounds\n",
    "        \n",
    "        self.log_std_min = log_std_min\n",
    "        self.log_std_max = log_std_max\n",
    "        \n",
    "        self.input_layer = nn.Linear(input_dim, \n",
    "                                     hidden_dims[0])\n",
    "        \n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for i in range(len(hidden_dims)-1):\n",
    "            hidden_layer = nn.Linear(\n",
    "                hidden_dims[i], hidden_dims[i+1])\n",
    "            self.hidden_layers.append(hidden_layer)\n",
    "\n",
    "        self.output_layer_mean = nn.Linear(hidden_dims[-1], len(self.env_max))\n",
    "        self.output_layer_log_std = nn.Linear(hidden_dims[-1], len(self.env_max))\n",
    "\n",
    "        device = \"cpu\"\n",
    "        if torch.cuda.is_available():\n",
    "            device = \"cuda:0\"\n",
    "        self.device = torch.device(device)\n",
    "        self.to(self.device)\n",
    "\n",
    "        self.env_min = torch.tensor(self.env_min,\n",
    "                                    device=self.device, \n",
    "                                    dtype=torch.float32)\n",
    "\n",
    "        self.env_max = torch.tensor(self.env_max,\n",
    "                                    device=self.device, \n",
    "                                    dtype=torch.float32)\n",
    "        \n",
    "        self.nn_min = F.tanh(torch.Tensor([float('-inf')])).to(self.device)\n",
    "        self.nn_max = F.tanh(torch.Tensor([float('inf')])).to(self.device)\n",
    "        self.rescale_fn = lambda x: (x - self.nn_min) * (self.env_max - self.env_min) / \\\n",
    "                                    (self.nn_max - self.nn_min) + self.env_min\n",
    "\n",
    "        self.target_entropy = -np.prod(self.env_max.shape)\n",
    "        self.logalpha = torch.zeros(1, requires_grad=True, device=self.device)\n",
    "        self.alpha_optimizer = optim.Adam([self.logalpha], lr=entropy_lr)\n",
    "\n",
    "    def _format(self, state):\n",
    "        x = state\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.tensor(x, \n",
    "                             device=self.device, \n",
    "                             dtype=torch.float32)\n",
    "            x = x.unsqueeze(0)\n",
    "        return x\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self._format(state)\n",
    "        x = self.activation_fc(self.input_layer(x))\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            x = self.activation_fc(hidden_layer(x))\n",
    "        x_mean = self.output_layer_mean(x)\n",
    "        x_log_std = self.output_layer_log_std(x)\n",
    "        x_log_std = torch.clamp(x_log_std, \n",
    "                                self.log_std_min, \n",
    "                                self.log_std_max)\n",
    "        return x_mean, x_log_std\n",
    "\n",
    "    def full_pass(self, state, epsilon=1e-6):\n",
    "        mean, log_std = self.forward(state)\n",
    "\n",
    "        pi_s = Normal(mean, log_std.exp())\n",
    "        pre_tanh_action = pi_s.rsample()\n",
    "        tanh_action = torch.tanh(pre_tanh_action)\n",
    "        action = self.rescale_fn(tanh_action)\n",
    "\n",
    "        log_prob = pi_s.log_prob(pre_tanh_action) - torch.log(\n",
    "            (1 - tanh_action.pow(2)).clamp(0, 1) + epsilon)\n",
    "        log_prob = log_prob.sum(dim=1, keepdim=True)\n",
    "\n",
    "        return action, log_prob, self.rescale_fn(torch.tanh(mean))\n",
    "\n",
    "    def _update_exploration_ratio(self, greedy_action, action_taken):\n",
    "        env_min, env_max = self.env_min.cpu().numpy(), self.env_max.cpu().numpy()\n",
    "        self.exploration_ratio = np.mean(abs((greedy_action - action_taken)/(env_max - env_min)))\n",
    "\n",
    "    def _get_actions(self, state):\n",
    "        mean, log_std = self.forward(state)\n",
    "\n",
    "        action = self.rescale_fn(torch.tanh(Normal(mean, log_std.exp()).sample()))\n",
    "        greedy_action = self.rescale_fn(torch.tanh(mean))\n",
    "        random_action = np.random.uniform(low=self.env_min.cpu().numpy(),\n",
    "                                          high=self.env_max.cpu().numpy())\n",
    "\n",
    "        action_shape = self.env_max.cpu().numpy().shape\n",
    "        action = action.detach().cpu().numpy().reshape(action_shape)\n",
    "        greedy_action = greedy_action.detach().cpu().numpy().reshape(action_shape)\n",
    "        random_action = random_action.reshape(action_shape)\n",
    "\n",
    "        return action, greedy_action, random_action\n",
    "\n",
    "    def select_random_action(self, state):\n",
    "        action, greedy_action, random_action = self._get_actions(state)\n",
    "        self._update_exploration_ratio(greedy_action, random_action)\n",
    "        return random_action\n",
    "\n",
    "    def select_greedy_action(self, state):\n",
    "        action, greedy_action, random_action = self._get_actions(state)\n",
    "        self._update_exploration_ratio(greedy_action, greedy_action)\n",
    "        return greedy_action\n",
    "\n",
    "    def select_action(self, state):\n",
    "        action, greedy_action, random_action = self._get_actions(state)\n",
    "        self._update_exploration_ratio(greedy_action, action)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAC():\n",
    "    def __init__(self, \n",
    "                 replay_buffer_fn,\n",
    "                 policy_model_fn, \n",
    "                 policy_max_grad_norm, \n",
    "                 policy_optimizer_fn, \n",
    "                 policy_optimizer_lr,\n",
    "                 value_model_fn,\n",
    "                 value_max_grad_norm, \n",
    "                 value_optimizer_fn, \n",
    "                 value_optimizer_lr,\n",
    "                 n_warmup_batches,\n",
    "                 update_target_every_steps,\n",
    "                 tau):\n",
    "        self.replay_buffer_fn = replay_buffer_fn\n",
    "\n",
    "        self.policy_model_fn = policy_model_fn\n",
    "        self.policy_max_grad_norm = policy_max_grad_norm\n",
    "        self.policy_optimizer_fn = policy_optimizer_fn\n",
    "        self.policy_optimizer_lr = policy_optimizer_lr\n",
    "\n",
    "        self.value_model_fn = value_model_fn\n",
    "        self.value_max_grad_norm = value_max_grad_norm\n",
    "        self.value_optimizer_fn = value_optimizer_fn\n",
    "        self.value_optimizer_lr = value_optimizer_lr\n",
    "\n",
    "        self.n_warmup_batches = n_warmup_batches\n",
    "        self.update_target_every_steps = update_target_every_steps\n",
    "\n",
    "        self.tau = tau\n",
    "\n",
    "    def optimize_model(self, experiences):\n",
    "        states, actions, rewards, next_states, is_terminals = experiences\n",
    "        batch_size = len(is_terminals)\n",
    "\n",
    "        # policy loss\n",
    "        current_actions, logpi_s, _ = self.policy_model.full_pass(states)\n",
    "\n",
    "        target_alpha = (logpi_s + self.policy_model.target_entropy).detach()\n",
    "        alpha_loss = -(self.policy_model.logalpha * target_alpha).mean()\n",
    "\n",
    "        self.policy_model.alpha_optimizer.zero_grad()\n",
    "        alpha_loss.backward()\n",
    "        self.policy_model.alpha_optimizer.step()\n",
    "        alpha = self.policy_model.logalpha.exp()\n",
    "\n",
    "        current_q_sa_a = self.online_value_model_a(states, current_actions)\n",
    "        current_q_sa_b = self.online_value_model_b(states, current_actions)\n",
    "        current_q_sa = torch.min(current_q_sa_a, current_q_sa_b)\n",
    "        policy_loss = (alpha * logpi_s - current_q_sa).mean()\n",
    "\n",
    "        # Q loss\n",
    "        ap, logpi_sp, _ = self.policy_model.full_pass(next_states)\n",
    "        q_spap_a = self.target_value_model_a(next_states, ap)\n",
    "        q_spap_b = self.target_value_model_b(next_states, ap)\n",
    "        q_spap = torch.min(q_spap_a, q_spap_b) - alpha * logpi_sp\n",
    "        target_q_sa = (rewards + self.gamma * q_spap * (1 - is_terminals)).detach()\n",
    "\n",
    "        q_sa_a = self.online_value_model_a(states, actions)\n",
    "        q_sa_b = self.online_value_model_b(states, actions)\n",
    "        qa_loss = (q_sa_a - target_q_sa).pow(2).mul(0.5).mean()\n",
    "        qb_loss = (q_sa_b - target_q_sa).pow(2).mul(0.5).mean()\n",
    "\n",
    "        self.value_optimizer_a.zero_grad()\n",
    "        qa_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.online_value_model_a.parameters(), \n",
    "                                       self.value_max_grad_norm)\n",
    "        self.value_optimizer_a.step()\n",
    "\n",
    "        self.value_optimizer_b.zero_grad()\n",
    "        qb_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.online_value_model_b.parameters(),\n",
    "                                       self.value_max_grad_norm)\n",
    "        self.value_optimizer_b.step()\n",
    "\n",
    "        self.policy_optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy_model.parameters(), \n",
    "                                       self.policy_max_grad_norm)        \n",
    "        self.policy_optimizer.step()\n",
    "\n",
    "    def interaction_step(self, state, env):\n",
    "        min_samples = self.replay_buffer.batch_size * self.n_warmup_batches\n",
    "        if len(self.replay_buffer) < min_samples:\n",
    "            action = self.policy_model.select_random_action(state)\n",
    "        else:\n",
    "            action = self.policy_model.select_action(state)\n",
    "\n",
    "        new_state, reward, is_terminal, info = env.step(action)\n",
    "        is_truncated = 'TimeLimit.truncated' in info and info['TimeLimit.truncated']\n",
    "        is_failure = is_terminal and not is_truncated\n",
    "        experience = (state, action, reward, new_state, float(is_failure))\n",
    "\n",
    "        self.replay_buffer.store(experience)\n",
    "        self.episode_reward[-1] += reward\n",
    "        self.episode_timestep[-1] += 1\n",
    "        self.episode_exploration[-1] += self.policy_model.exploration_ratio\n",
    "        return new_state, is_terminal\n",
    "\n",
    "    def update_value_networks(self, tau=None):\n",
    "        tau = self.tau if tau is None else tau\n",
    "        for target, online in zip(self.target_value_model_a.parameters(), \n",
    "                                  self.online_value_model_a.parameters()):\n",
    "            target_ratio = (1.0 - tau) * target.data\n",
    "            online_ratio = tau * online.data\n",
    "            mixed_weights = target_ratio + online_ratio\n",
    "            target.data.copy_(mixed_weights)\n",
    "\n",
    "        for target, online in zip(self.target_value_model_b.parameters(), \n",
    "                                  self.online_value_model_b.parameters()):\n",
    "            target_ratio = (1.0 - tau) * target.data\n",
    "            online_ratio = tau * online.data\n",
    "            mixed_weights = target_ratio + online_ratio\n",
    "            target.data.copy_(mixed_weights)\n",
    "\n",
    "    def train(self, make_env_fn, make_env_kargs, seed, gamma, \n",
    "              max_minutes, max_episodes, goal_mean_100_reward):\n",
    "        training_start, last_debug_time = time.time(), float('-inf')\n",
    "\n",
    "        self.checkpoint_dir = tempfile.mkdtemp()\n",
    "        self.make_env_fn = make_env_fn\n",
    "        self.make_env_kargs = make_env_kargs\n",
    "        self.seed = seed\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        env = self.make_env_fn(**self.make_env_kargs, seed=self.seed)\n",
    "        torch.manual_seed(self.seed) ; np.random.seed(self.seed) ; random.seed(self.seed)\n",
    "    \n",
    "        nS, nA = env.observation_space.shape[0], env.action_space.shape[0]\n",
    "        action_bounds = env.action_space.low, env.action_space.high\n",
    "        self.episode_timestep = []\n",
    "        self.episode_reward = []\n",
    "        self.episode_seconds = []\n",
    "        self.evaluation_scores = []        \n",
    "        self.episode_exploration = []\n",
    "\n",
    "        self.target_value_model_a = self.value_model_fn(nS, nA)\n",
    "        self.online_value_model_a = self.value_model_fn(nS, nA)\n",
    "        self.target_value_model_b = self.value_model_fn(nS, nA)\n",
    "        self.online_value_model_b = self.value_model_fn(nS, nA)\n",
    "        self.update_value_networks(tau=1.0)\n",
    "\n",
    "        self.policy_model = self.policy_model_fn(nS, action_bounds)\n",
    "\n",
    "        self.value_optimizer_a = self.value_optimizer_fn(self.online_value_model_a,\n",
    "                                                         self.value_optimizer_lr)\n",
    "        self.value_optimizer_b = self.value_optimizer_fn(self.online_value_model_b,\n",
    "                                                         self.value_optimizer_lr)\n",
    "        self.policy_optimizer = self.policy_optimizer_fn(self.policy_model,\n",
    "                                                         self.policy_optimizer_lr)\n",
    "\n",
    "        self.replay_buffer = self.replay_buffer_fn()\n",
    "                    \n",
    "        result = np.empty((max_episodes, 5))\n",
    "        result[:] = np.nan\n",
    "        training_time = 0\n",
    "        for episode in range(1, max_episodes + 1):\n",
    "            episode_start = time.time()\n",
    "            \n",
    "            state, is_terminal = env.reset(), False\n",
    "            self.episode_reward.append(0.0)\n",
    "            self.episode_timestep.append(0.0)\n",
    "            self.episode_exploration.append(0.0)\n",
    "\n",
    "            for step in count():\n",
    "                state, is_terminal = self.interaction_step(state, env)\n",
    "\n",
    "                min_samples = self.replay_buffer.batch_size * self.n_warmup_batches\n",
    "                if len(self.replay_buffer) > min_samples:\n",
    "                    experiences = self.replay_buffer.sample()\n",
    "                    experiences = self.online_value_model_a.load(experiences)\n",
    "                    self.optimize_model(experiences)\n",
    "\n",
    "                if np.sum(self.episode_timestep) % self.update_target_every_steps == 0:\n",
    "                    self.update_value_networks()\n",
    "\n",
    "                if is_terminal:\n",
    "                    gc.collect()\n",
    "                    break\n",
    "\n",
    "            # stats\n",
    "            episode_elapsed = time.time() - episode_start\n",
    "            self.episode_seconds.append(episode_elapsed)\n",
    "            training_time += episode_elapsed\n",
    "            evaluation_score, _ = self.evaluate(self.policy_model, env)\n",
    "            self.save_checkpoint(episode-1, self.policy_model)\n",
    "\n",
    "            total_step = int(np.sum(self.episode_timestep))\n",
    "            self.evaluation_scores.append(evaluation_score)\n",
    "            \n",
    "            mean_10_reward = np.mean(self.episode_reward[-10:])\n",
    "            std_10_reward = np.std(self.episode_reward[-10:])\n",
    "            mean_100_reward = np.mean(self.episode_reward[-100:])\n",
    "            std_100_reward = np.std(self.episode_reward[-100:])\n",
    "            mean_100_eval_score = np.mean(self.evaluation_scores[-100:])\n",
    "            std_100_eval_score = np.std(self.evaluation_scores[-100:])\n",
    "            lst_100_exp_rat = np.array(\n",
    "                self.episode_exploration[-100:])/np.array(self.episode_timestep[-100:])\n",
    "            mean_100_exp_rat = np.mean(lst_100_exp_rat)\n",
    "            std_100_exp_rat = np.std(lst_100_exp_rat)\n",
    "            \n",
    "            wallclock_elapsed = time.time() - training_start\n",
    "            result[episode-1] = total_step, mean_100_reward, \\\n",
    "                mean_100_eval_score, training_time, wallclock_elapsed\n",
    "            \n",
    "            reached_debug_time = time.time() - last_debug_time >= LEAVE_PRINT_EVERY_N_SECS\n",
    "            reached_max_minutes = wallclock_elapsed >= max_minutes * 60\n",
    "            reached_max_episodes = episode >= max_episodes\n",
    "            reached_goal_mean_reward = mean_100_eval_score >= goal_mean_100_reward\n",
    "            training_is_over = reached_max_minutes or \\\n",
    "                               reached_max_episodes or \\\n",
    "                               reached_goal_mean_reward\n",
    "            elapsed_str = time.strftime(\"%H:%M:%S\", time.gmtime(time.time() - training_start))\n",
    "            debug_message = 'el {}, ep {:04}, ts {:07}, '\n",
    "            debug_message += 'ar 10 {:05.1f}\\u00B1{:05.1f}, '\n",
    "            debug_message += '100 {:05.1f}\\u00B1{:05.1f}, '\n",
    "            debug_message += 'ex 100 {:02.1f}\\u00B1{:02.1f}, '\n",
    "            debug_message += 'ev {:05.1f}\\u00B1{:05.1f}'\n",
    "            debug_message = debug_message.format(\n",
    "                elapsed_str, episode-1, total_step, mean_10_reward, std_10_reward, \n",
    "                mean_100_reward, std_100_reward, mean_100_exp_rat, std_100_exp_rat,\n",
    "                mean_100_eval_score, std_100_eval_score)\n",
    "            print(debug_message, end='\\r', flush=True)\n",
    "            if reached_debug_time or training_is_over:\n",
    "                print(ERASE_LINE + debug_message, flush=True)\n",
    "                last_debug_time = time.time()\n",
    "            if training_is_over:\n",
    "                if reached_max_minutes: print(u'--> reached_max_minutes \\u2715')\n",
    "                if reached_max_episodes: print(u'--> reached_max_episodes \\u2715')\n",
    "                if reached_goal_mean_reward: print(u'--> reached_goal_mean_reward \\u2713')\n",
    "                break\n",
    "                \n",
    "        final_eval_score, score_std = self.evaluate(self.policy_model, env, n_episodes=100)\n",
    "        wallclock_time = time.time() - training_start\n",
    "        print('Training complete.')\n",
    "        print('Final evaluation score {:.2f}\\u00B1{:.2f} in {:.2f}s training time,'\n",
    "              ' {:.2f}s wall-clock time.\\n'.format(\n",
    "                  final_eval_score, score_std, training_time, wallclock_time))\n",
    "        env.close() ; del env\n",
    "        self.get_cleaned_checkpoints()\n",
    "        return result, final_eval_score, training_time, wallclock_time\n",
    "    \n",
    "    def evaluate(self, eval_policy_model, eval_env, n_episodes=1):\n",
    "        rs = []\n",
    "        for _ in range(n_episodes):\n",
    "            s, d = eval_env.reset(), False\n",
    "            rs.append(0)\n",
    "            for _ in count():\n",
    "                a = eval_policy_model.select_greedy_action(s)\n",
    "                s, r, d, _ = eval_env.step(a)\n",
    "                rs[-1] += r\n",
    "                if d: break\n",
    "        return np.mean(rs), np.std(rs)\n",
    "\n",
    "    def get_cleaned_checkpoints(self, n_checkpoints=4):\n",
    "        try: \n",
    "            return self.checkpoint_paths\n",
    "        except AttributeError:\n",
    "            self.checkpoint_paths = {}\n",
    "\n",
    "        paths = glob.glob(os.path.join(self.checkpoint_dir, '*.tar'))\n",
    "        paths_dic = {int(path.split('.')[-2]):path for path in paths}\n",
    "        last_ep = max(paths_dic.keys())\n",
    "        # checkpoint_idxs = np.geomspace(1, last_ep+1, n_checkpoints, endpoint=True, dtype=np.int)-1\n",
    "        checkpoint_idxs = np.linspace(1, last_ep+1, n_checkpoints, endpoint=True, dtype=np.int)-1\n",
    "\n",
    "        for idx, path in paths_dic.items():\n",
    "            if idx in checkpoint_idxs:\n",
    "                self.checkpoint_paths[idx] = path\n",
    "            else:\n",
    "                os.unlink(path)\n",
    "\n",
    "        return self.checkpoint_paths\n",
    "\n",
    "    def demo_last(self, title='Fully-trained {} Agent', n_episodes=2, max_n_videos=2):\n",
    "        env = self.make_env_fn(**self.make_env_kargs, monitor_mode='evaluation', render=True, record=True)\n",
    "\n",
    "        checkpoint_paths = self.get_cleaned_checkpoints()\n",
    "        last_ep = max(checkpoint_paths.keys())\n",
    "        self.policy_model.load_state_dict(torch.load(checkpoint_paths[last_ep]))\n",
    "\n",
    "        self.evaluate(self.policy_model, env, n_episodes=n_episodes)\n",
    "        env.close()\n",
    "        data = get_gif_html(env_videos=env.videos, \n",
    "                            title=title.format(self.__class__.__name__),\n",
    "                            max_n_videos=max_n_videos)\n",
    "        del env\n",
    "        return HTML(data=data)\n",
    "\n",
    "    def demo_progression(self, title='{} Agent progression', max_n_videos=4):\n",
    "        env = self.make_env_fn(**self.make_env_kargs, monitor_mode='evaluation', render=True, record=True)\n",
    "\n",
    "        checkpoint_paths = self.get_cleaned_checkpoints()\n",
    "        for i in sorted(checkpoint_paths.keys()):\n",
    "            self.policy_model.load_state_dict(torch.load(checkpoint_paths[i]))\n",
    "            self.evaluate(self.policy_model, env, n_episodes=1)\n",
    "\n",
    "        env.close()\n",
    "        data = get_gif_html(env_videos=env.videos, \n",
    "                            title=title.format(self.__class__.__name__),\n",
    "                            subtitle_eps=sorted(checkpoint_paths.keys()),\n",
    "                            max_n_videos=max_n_videos)\n",
    "        del env\n",
    "        return HTML(data=data)\n",
    "\n",
    "    def save_checkpoint(self, episode_idx, model):\n",
    "        torch.save(model.state_dict(), \n",
    "                   os.path.join(self.checkpoint_dir, 'model.{}.tar'.format(episode_idx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sac_results = []\n",
    "best_agent, best_eval_score = None, float('-inf')\n",
    "for seed in SEEDS:\n",
    "    environment_settings = {\n",
    "        'env_name': 'HalfCheetahBulletEnv-v0',\n",
    "        'gamma': 0.99,\n",
    "        'max_minutes': 300,\n",
    "        'max_episodes': 10000,\n",
    "        'goal_mean_100_reward': 2000\n",
    "    }\n",
    "\n",
    "    policy_model_fn = lambda nS, bounds: FCGP(nS, bounds, hidden_dims=(256,256))\n",
    "    policy_max_grad_norm = float('inf')\n",
    "    policy_optimizer_fn = lambda net, lr: optim.Adam(net.parameters(), lr=lr)\n",
    "    policy_optimizer_lr = 0.0003\n",
    "\n",
    "    value_model_fn = lambda nS, nA: FCQSA(nS, nA, hidden_dims=(256,256))\n",
    "    value_max_grad_norm = float('inf')\n",
    "    value_optimizer_fn = lambda net, lr: optim.Adam(net.parameters(), lr=lr)\n",
    "    value_optimizer_lr = 0.0005\n",
    "\n",
    "    replay_buffer_fn = lambda: ReplayBuffer(max_size=100000, batch_size=64)\n",
    "    n_warmup_batches = 10\n",
    "    update_target_every_steps = 1\n",
    "    tau = 0.001\n",
    "\n",
    "    env_name, gamma, max_minutes, \\\n",
    "    max_episodes, goal_mean_100_reward = environment_settings.values()\n",
    "                \n",
    "    agent = SAC(replay_buffer_fn,\n",
    "                policy_model_fn, \n",
    "                policy_max_grad_norm,\n",
    "                policy_optimizer_fn, \n",
    "                policy_optimizer_lr,\n",
    "                value_model_fn,\n",
    "                value_max_grad_norm, \n",
    "                value_optimizer_fn, \n",
    "                value_optimizer_lr, \n",
    "                n_warmup_batches,\n",
    "                update_target_every_steps,\n",
    "                tau)\n",
    "\n",
    "    make_env_fn, make_env_kargs = get_make_env_fn(env_name=env_name, inner_wrappers=[RenderUint8])\n",
    "    result, final_eval_score, training_time, wallclock_time = agent.train(\n",
    "        make_env_fn, make_env_kargs, seed, gamma, max_minutes, max_episodes, goal_mean_100_reward)\n",
    "    sac_results.append(result)\n",
    "    if final_eval_score > best_eval_score:\n",
    "        best_eval_score = final_eval_score\n",
    "        best_agent = agent\n",
    "sac_results = np.array(sac_results)\n",
    "_ = BEEP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "best_agent.demo_progression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_agent.demo_last()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sac_max_t, sac_max_r, sac_max_s, \\\n",
    "sac_max_sec, sac_max_rt = np.max(sac_results, axis=0).T\n",
    "sac_min_t, sac_min_r, sac_min_s, \\\n",
    "sac_min_sec, sac_min_rt = np.min(sac_results, axis=0).T\n",
    "sac_mean_t, sac_mean_r, sac_mean_s, \\\n",
    "sac_mean_sec, sac_mean_rt = np.mean(sac_results, axis=0).T\n",
    "sac_x = np.arange(len(sac_mean_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 1, figsize=(15,10), sharey=False, sharex=True)\n",
    "\n",
    "# SAC\n",
    "axs[0].plot(sac_max_r, 'g', linewidth=1)\n",
    "axs[0].plot(sac_min_r, 'g', linewidth=1)\n",
    "axs[0].plot(sac_mean_r, 'g:', label='SAC', linewidth=2)\n",
    "axs[0].fill_between(\n",
    "    sac_x, sac_min_r, sac_max_r, facecolor='g', alpha=0.3)\n",
    "\n",
    "axs[1].plot(sac_max_s, 'g', linewidth=1)\n",
    "axs[1].plot(sac_min_s, 'g', linewidth=1)\n",
    "axs[1].plot(sac_mean_s, 'g:', label='SAC', linewidth=2)\n",
    "axs[1].fill_between(\n",
    "    sac_x, sac_min_s, sac_max_s, facecolor='g', alpha=0.3)\n",
    "\n",
    "# ALL\n",
    "axs[0].set_title('Moving Avg Reward (Training)')\n",
    "axs[1].set_title('Moving Avg Reward (Evaluation)')\n",
    "plt.xlabel('Episodes')\n",
    "axs[0].legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 1, figsize=(15,15), sharey=False, sharex=True)\n",
    "\n",
    "# SAC\n",
    "axs[0].plot(sac_max_t, 'g', linewidth=1)\n",
    "axs[0].plot(sac_min_t, 'g', linewidth=1)\n",
    "axs[0].plot(sac_mean_t, 'g:', label='SAC', linewidth=2)\n",
    "axs[0].fill_between(\n",
    "    sac_x, sac_min_t, sac_max_t, facecolor='g', alpha=0.3)\n",
    "\n",
    "axs[1].plot(sac_max_sec, 'g', linewidth=1)\n",
    "axs[1].plot(sac_min_sec, 'g', linewidth=1)\n",
    "axs[1].plot(sac_mean_sec, 'g:', label='SAC', linewidth=2)\n",
    "axs[1].fill_between(\n",
    "    sac_x, sac_min_sec, sac_max_sec, facecolor='g', alpha=0.3)\n",
    "\n",
    "axs[2].plot(sac_max_rt, 'g', linewidth=1)\n",
    "axs[2].plot(sac_min_rt, 'g', linewidth=1)\n",
    "axs[2].plot(sac_mean_rt, 'g:', label='SAC', linewidth=2)\n",
    "axs[2].fill_between(\n",
    "    sac_x, sac_min_rt, sac_max_rt, facecolor='g', alpha=0.3)\n",
    "\n",
    "# ALL\n",
    "axs[0].set_title('Total Steps')\n",
    "axs[1].set_title('Training Time')\n",
    "axs[2].set_title('Wall-clock Time')\n",
    "plt.xlabel('Episodes')\n",
    "axs[0].legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sac_root_dir = os.path.join(RESULTS_DIR, 'sac')\n",
    "not os.path.exists(sac_root_dir) and os.makedirs(sac_root_dir)\n",
    "\n",
    "np.save(os.path.join(sac_root_dir, 'x'), sac_x)\n",
    "\n",
    "np.save(os.path.join(sac_root_dir, 'max_r'), sac_max_r)\n",
    "np.save(os.path.join(sac_root_dir, 'min_r'), sac_min_r)\n",
    "np.save(os.path.join(sac_root_dir, 'mean_r'), sac_mean_r)\n",
    "\n",
    "np.save(os.path.join(sac_root_dir, 'max_s'), sac_max_s)\n",
    "np.save(os.path.join(sac_root_dir, 'min_s'), sac_min_s )\n",
    "np.save(os.path.join(sac_root_dir, 'mean_s'), sac_mean_s)\n",
    "\n",
    "np.save(os.path.join(sac_root_dir, 'max_t'), sac_max_t)\n",
    "np.save(os.path.join(sac_root_dir, 'min_t'), sac_min_t)\n",
    "np.save(os.path.join(sac_root_dir, 'mean_t'), sac_mean_t)\n",
    "\n",
    "np.save(os.path.join(sac_root_dir, 'max_sec'), sac_max_sec)\n",
    "np.save(os.path.join(sac_root_dir, 'min_sec'), sac_min_sec)\n",
    "np.save(os.path.join(sac_root_dir, 'mean_sec'), sac_mean_sec)\n",
    "\n",
    "np.save(os.path.join(sac_root_dir, 'max_rt'), sac_max_rt)\n",
    "np.save(os.path.join(sac_root_dir, 'min_rt'), sac_min_rt)\n",
    "np.save(os.path.join(sac_root_dir, 'mean_rt'), sac_mean_rt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiprocessEnv(object):\n",
    "    def __init__(self, make_env_fn, make_env_kargs, seed, n_workers):\n",
    "        self.make_env_fn = make_env_fn\n",
    "        self.make_env_kargs = make_env_kargs\n",
    "        self.seed = seed\n",
    "        self.n_workers = n_workers\n",
    "        self.pipes = [mp.Pipe() for rank in range(self.n_workers)]\n",
    "        self.workers = [\n",
    "            mp.Process(\n",
    "                target=self.work, \n",
    "                args=(rank, self.pipes[rank][1])) for rank in range(self.n_workers)]\n",
    "        [w.start() for w in self.workers]\n",
    "        self.dones = {rank:False for rank in range(self.n_workers)}\n",
    "\n",
    "    def reset(self, ranks=None, **kwargs):\n",
    "        if not (ranks is None):\n",
    "            [self.send_msg(('reset', {}), rank) for rank in ranks]            \n",
    "            return np.stack([parent_end.recv() for rank, (parent_end, _) in enumerate(self.pipes) if rank in ranks])\n",
    "\n",
    "        self.broadcast_msg(('reset', kwargs))\n",
    "        return np.stack([parent_end.recv() for parent_end, _ in self.pipes])\n",
    "\n",
    "    def step(self, actions):\n",
    "        assert len(actions) == self.n_workers\n",
    "        [self.send_msg(\n",
    "            ('step', {'action':actions[rank]}), \n",
    "            rank) for rank in range(self.n_workers)]\n",
    "        results = []\n",
    "        for rank in range(self.n_workers):\n",
    "            parent_end, _ = self.pipes[rank]\n",
    "            o, r, d, i = parent_end.recv()\n",
    "            results.append((o,\n",
    "                            float(r),\n",
    "                            float(d),\n",
    "                            i))\n",
    "        return [np.stack(block).squeeze() for block in np.array(results).T]\n",
    "\n",
    "    def close(self, **kwargs):\n",
    "        self.broadcast_msg(('close', kwargs))\n",
    "        [w.join() for w in self.workers]\n",
    "    \n",
    "    def work(self, rank, worker_end):\n",
    "        env = self.make_env_fn(**self.make_env_kargs, seed=self.seed+rank)\n",
    "        while True:\n",
    "            cmd, kwargs = worker_end.recv()\n",
    "            if cmd == 'reset':\n",
    "                worker_end.send(env.reset(**kwargs))\n",
    "            elif cmd == 'step':\n",
    "                worker_end.send(env.step(**kwargs))\n",
    "            elif cmd == '_past_limit':\n",
    "                worker_end.send(env._elapsed_steps >= env._max_episode_steps)\n",
    "            else:\n",
    "                # including close command \n",
    "                env.close(**kwargs) ; del env ; worker_end.close()\n",
    "                break\n",
    "\n",
    "    def send_msg(self, msg, rank):\n",
    "        parent_end, _ = self.pipes[rank]\n",
    "        parent_end.send(msg)\n",
    "\n",
    "    def broadcast_msg(self, msg):    \n",
    "        [parent_end.send(msg) for parent_end, _ in self.pipes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpisodeBuffer():\n",
    "    def __init__(self,\n",
    "                 state_dim,\n",
    "                 gamma,\n",
    "                 tau,\n",
    "                 n_workers,\n",
    "                 max_episodes,\n",
    "                 max_episode_steps):\n",
    "        \n",
    "        assert max_episodes >= n_workers\n",
    "\n",
    "        self.state_dim = state_dim\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.n_workers = n_workers\n",
    "        self.max_episodes = max_episodes\n",
    "        self.max_episode_steps = max_episode_steps\n",
    "\n",
    "        self._truncated_fn = np.vectorize(lambda x: 'TimeLimit.truncated' in x and x['TimeLimit.truncated'])\n",
    "        self.discounts = np.logspace(\n",
    "            0, max_episode_steps+1, num=max_episode_steps+1, base=gamma, endpoint=False, dtype=np.float128)\n",
    "        self.tau_discounts = np.logspace(\n",
    "            0, max_episode_steps+1, num=max_episode_steps+1, base=gamma*tau, endpoint=False, dtype=np.float128)\n",
    "\n",
    "        device = 'cpu'\n",
    "        if torch.cuda.is_available():\n",
    "            device = 'cuda:0'\n",
    "        self.device = torch.device(device)\n",
    "\n",
    "        self.clear()\n",
    "\n",
    "    def clear(self):\n",
    "        self.states_mem = np.empty(\n",
    "            shape=np.concatenate(((self.max_episodes, self.max_episode_steps), self.state_dim)), dtype=np.float64)\n",
    "        self.states_mem[:] = np.nan\n",
    "\n",
    "        self.actions_mem = np.empty(shape=(self.max_episodes, self.max_episode_steps), dtype=np.uint8)\n",
    "        self.actions_mem[:] = np.nan\n",
    "\n",
    "        self.returns_mem = np.empty(shape=(self.max_episodes,self.max_episode_steps), dtype=np.float32)\n",
    "        self.returns_mem[:] = np.nan\n",
    "\n",
    "        self.gaes_mem = np.empty(shape=(self.max_episodes, self.max_episode_steps), dtype=np.float32)\n",
    "        self.gaes_mem[:] = np.nan\n",
    "\n",
    "        self.logpas_mem = np.empty(shape=(self.max_episodes, self.max_episode_steps), dtype=np.float32)\n",
    "        self.logpas_mem[:] = np.nan\n",
    "\n",
    "        self.episode_steps = np.zeros(shape=(self.max_episodes), dtype=np.uint16)\n",
    "        self.episode_reward = np.zeros(shape=(self.max_episodes), dtype=np.float32)\n",
    "        self.episode_exploration = np.zeros(shape=(self.max_episodes), dtype=np.float32)\n",
    "        self.episode_seconds = np.zeros(shape=(self.max_episodes), dtype=np.float64)\n",
    "\n",
    "        self.current_ep_idxs = np.arange(n_workers, dtype=np.uint16)\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "    def fill(self, envs, policy_model, value_model):\n",
    "        states = envs.reset()\n",
    "\n",
    "        worker_rewards = np.zeros(shape=(n_workers, self.max_episode_steps), dtype=np.float32)\n",
    "        worker_exploratory = np.zeros(shape=(n_workers, self.max_episode_steps), dtype=np.bool)\n",
    "        worker_steps = np.zeros(shape=(n_workers), dtype=np.uint16)\n",
    "        worker_seconds = np.array([time.time(),] * n_workers, dtype=np.float64)\n",
    "\n",
    "        buffer_full = False\n",
    "        while not buffer_full and len(self.episode_steps[self.episode_steps > 0]) < self.max_episodes/2:\n",
    "            with torch.no_grad():\n",
    "                actions, logpas, are_exploratory = policy_model.np_pass(states)\n",
    "                values = value_model(states)\n",
    "\n",
    "            next_states, rewards, terminals, infos = envs.step(actions)\n",
    "            self.states_mem[self.current_ep_idxs, worker_steps] = states\n",
    "            self.actions_mem[self.current_ep_idxs, worker_steps] = actions\n",
    "            self.logpas_mem[self.current_ep_idxs, worker_steps] = logpas\n",
    "\n",
    "            worker_exploratory[np.arange(self.n_workers), worker_steps] = are_exploratory\n",
    "            worker_rewards[np.arange(self.n_workers), worker_steps] = rewards\n",
    "\n",
    "            for w_idx in range(self.n_workers):\n",
    "                if worker_steps[w_idx] + 1 == self.max_episode_steps:\n",
    "                    terminals[w_idx] = 1\n",
    "                    infos[w_idx]['TimeLimit.truncated'] = True\n",
    "\n",
    "            if terminals.sum():\n",
    "                idx_terminals = np.flatnonzero(terminals)\n",
    "                next_values = np.zeros(shape=(n_workers))\n",
    "                truncated = self._truncated_fn(infos)\n",
    "                if truncated.sum():\n",
    "                    idx_truncated = np.flatnonzero(truncated)\n",
    "                    with torch.no_grad():\n",
    "                        next_values[idx_truncated] = value_model(\n",
    "                            next_states[idx_truncated]).cpu().numpy()\n",
    "\n",
    "            states = next_states\n",
    "            worker_steps += 1\n",
    "\n",
    "            if terminals.sum():\n",
    "                new_states = envs.reset(ranks=idx_terminals)\n",
    "                states[idx_terminals] = new_states\n",
    "\n",
    "                for w_idx in range(self.n_workers):\n",
    "                    if w_idx not in idx_terminals:\n",
    "                        continue\n",
    "\n",
    "                    e_idx = self.current_ep_idxs[w_idx]\n",
    "                    T = worker_steps[w_idx]\n",
    "                    self.episode_steps[e_idx] = T\n",
    "                    self.episode_reward[e_idx] = worker_rewards[w_idx, :T].sum()\n",
    "                    self.episode_exploration[e_idx] = worker_exploratory[w_idx, :T].mean()\n",
    "                    self.episode_seconds[e_idx] = time.time() - worker_seconds[w_idx]\n",
    "\n",
    "                    ep_rewards = np.concatenate(\n",
    "                        (worker_rewards[w_idx, :T], [next_values[w_idx]]))\n",
    "                    ep_discounts = self.discounts[:T+1]\n",
    "                    ep_returns = np.array(\n",
    "                        [np.sum(ep_discounts[:T+1-t] * ep_rewards[t:]) for t in range(T)])\n",
    "                    self.returns_mem[e_idx, :T] = ep_returns\n",
    "\n",
    "                    ep_states = self.states_mem[e_idx, :T]\n",
    "                    with torch.no_grad():\n",
    "                        ep_values = torch.cat((value_model(ep_states),\n",
    "                                               torch.tensor([next_values[w_idx]],\n",
    "                                                            device=value_model.device,\n",
    "                                                            dtype=torch.float32)))\n",
    "                    np_ep_values = ep_values.view(-1).cpu().numpy()\n",
    "                    ep_tau_discounts = self.tau_discounts[:T]\n",
    "                    deltas = ep_rewards[:-1] + self.gamma * np_ep_values[1:] - np_ep_values[:-1]\n",
    "                    gaes = np.array(\n",
    "                        [np.sum(self.tau_discounts[:T-t] * deltas[t:]) for t in range(T)])\n",
    "                    self.gaes_mem[e_idx, :T] = gaes\n",
    "\n",
    "                    worker_exploratory[w_idx, :] = 0\n",
    "                    worker_rewards[w_idx, :] = 0\n",
    "                    worker_steps[w_idx] = 0\n",
    "                    worker_seconds[w_idx] = time.time()\n",
    "\n",
    "                    new_ep_id = max(self.current_ep_idxs) + 1\n",
    "                    if new_ep_id >= self.max_episodes:\n",
    "                        buffer_full = True\n",
    "                        break\n",
    "\n",
    "                    self.current_ep_idxs[w_idx] = new_ep_id\n",
    "\n",
    "        ep_idxs = self.episode_steps > 0\n",
    "        ep_t = self.episode_steps[ep_idxs]\n",
    "\n",
    "        self.states_mem = [row[:ep_t[i]] for i, row in enumerate(self.states_mem[ep_idxs])]\n",
    "        self.states_mem = np.concatenate(self.states_mem)\n",
    "        self.actions_mem = [row[:ep_t[i]] for i, row in enumerate(self.actions_mem[ep_idxs])]\n",
    "        self.actions_mem = np.concatenate(self.actions_mem)\n",
    "        self.returns_mem = [row[:ep_t[i]] for i, row in enumerate(self.returns_mem[ep_idxs])]\n",
    "        self.returns_mem = torch.tensor(np.concatenate(self.returns_mem), \n",
    "                                        device=value_model.device)\n",
    "        self.gaes_mem = [row[:ep_t[i]] for i, row in enumerate(self.gaes_mem[ep_idxs])]\n",
    "        self.gaes_mem = torch.tensor(np.concatenate(self.gaes_mem), \n",
    "                                     device=value_model.device)\n",
    "        self.logpas_mem = [row[:ep_t[i]] for i, row in enumerate(self.logpas_mem[ep_idxs])]\n",
    "        self.logpas_mem = torch.tensor(np.concatenate(self.logpas_mem), \n",
    "                                       device=value_model.device)\n",
    "\n",
    "        ep_r = self.episode_reward[ep_idxs]\n",
    "        ep_x = self.episode_exploration[ep_idxs]\n",
    "        ep_s = self.episode_seconds[ep_idxs]\n",
    "        return ep_t, ep_r, ep_x, ep_s\n",
    "\n",
    "    def get_stacks(self):\n",
    "        return (self.states_mem, self.actions_mem, \n",
    "                self.returns_mem, self.gaes_mem, self.logpas_mem)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.episode_steps[self.episode_steps > 0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCCA(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim, \n",
    "                 output_dim,\n",
    "                 hidden_dims=(32,32), \n",
    "                 activation_fc=F.relu):\n",
    "        super(FCCA, self).__init__()\n",
    "        self.activation_fc = activation_fc\n",
    "\n",
    "        self.input_layer = nn.Linear(input_dim[0], hidden_dims[0])\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for i in range(len(hidden_dims)-1):\n",
    "            hidden_layer = nn.Linear(hidden_dims[i], hidden_dims[i+1])\n",
    "            self.hidden_layers.append(hidden_layer)\n",
    "        self.output_layer = nn.Linear(hidden_dims[-1], output_dim)\n",
    "\n",
    "        device = \"cpu\"\n",
    "        if torch.cuda.is_available():\n",
    "            device = \"cuda:0\"\n",
    "        self.device = torch.device(device)\n",
    "        self.to(self.device)\n",
    "        \n",
    "    def _format(self, states):\n",
    "        x = states\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.tensor(x, device=self.device, dtype=torch.float32)\n",
    "            if len(x.size()) == 1:\n",
    "                x = x.unsqueeze(0)\n",
    "        return x\n",
    "\n",
    "    def forward(self, states):\n",
    "        x = self._format(states)\n",
    "        x = self.activation_fc(self.input_layer(x))\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            x = self.activation_fc(hidden_layer(x))\n",
    "        return self.output_layer(x)\n",
    "\n",
    "    def np_pass(self, states):\n",
    "        logits = self.forward(states)\n",
    "        np_logits = logits.detach().cpu().numpy()\n",
    "        dist = torch.distributions.Categorical(logits=logits)\n",
    "        actions = dist.sample()\n",
    "        np_actions = actions.detach().cpu().numpy()\n",
    "        logpas = dist.log_prob(actions)\n",
    "        np_logpas = logpas.detach().cpu().numpy()\n",
    "        is_exploratory = np_actions != np.argmax(np_logits, axis=1)\n",
    "        return np_actions, np_logpas, is_exploratory\n",
    "    \n",
    "    def select_action(self, states):\n",
    "        logits = self.forward(states)\n",
    "        dist = torch.distributions.Categorical(logits=logits)\n",
    "        action = dist.sample()\n",
    "        return action.detach().cpu().item()\n",
    "    \n",
    "    def get_predictions(self, states, actions):\n",
    "        states, actions = self._format(states), self._format(actions)\n",
    "        logits = self.forward(states)\n",
    "        dist = torch.distributions.Categorical(logits=logits)\n",
    "        logpas = dist.log_prob(actions)\n",
    "        entropies = dist.entropy()\n",
    "        return logpas, entropies\n",
    "    \n",
    "    def select_greedy_action(self, states):\n",
    "        logits = self.forward(states)\n",
    "        return np.argmax(logits.detach().squeeze().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCV(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim,\n",
    "                 hidden_dims=(32,32), \n",
    "                 activation_fc=F.relu):\n",
    "        super(FCV, self).__init__()\n",
    "        self.activation_fc = activation_fc\n",
    "\n",
    "        self.input_layer = nn.Linear(input_dim[0], hidden_dims[0])\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for i in range(len(hidden_dims)-1):\n",
    "            hidden_layer = nn.Linear(hidden_dims[i], hidden_dims[i+1])\n",
    "            self.hidden_layers.append(hidden_layer)\n",
    "        self.output_layer = nn.Linear(hidden_dims[-1], 1)\n",
    "\n",
    "        device = \"cpu\"\n",
    "        if torch.cuda.is_available():\n",
    "            device = \"cuda:0\"\n",
    "        self.device = torch.device(device)\n",
    "        self.to(self.device)\n",
    "        \n",
    "    def _format(self, states):\n",
    "        x = states\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.tensor(x, device=self.device, dtype=torch.float32)\n",
    "            if len(x.size()) == 1:\n",
    "                x = x.unsqueeze(0)\n",
    "        return x\n",
    "\n",
    "    def forward(self, states):\n",
    "        x = self._format(states)\n",
    "        x = self.activation_fc(self.input_layer(x))\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            x = self.activation_fc(hidden_layer(x))\n",
    "        return self.output_layer(x).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO():\n",
    "    def __init__(self, \n",
    "                 policy_model_fn, \n",
    "                 policy_model_max_grad_norm,\n",
    "                 policy_optimizer_fn,\n",
    "                 policy_optimizer_lr,\n",
    "                 policy_optimization_epochs,\n",
    "                 policy_sample_ratio,\n",
    "                 policy_clip_range,\n",
    "                 policy_stopping_kl,\n",
    "                 value_model_fn, \n",
    "                 value_model_max_grad_norm,\n",
    "                 value_optimizer_fn,\n",
    "                 value_optimizer_lr,\n",
    "                 value_optimization_epochs,\n",
    "                 value_sample_ratio,\n",
    "                 value_clip_range,\n",
    "                 value_stopping_mse,\n",
    "                 episode_buffer_fn,\n",
    "                 max_buffer_episodes,\n",
    "                 max_buffer_episode_steps,\n",
    "                 entropy_loss_weight,\n",
    "                 tau,\n",
    "                 n_workers):\n",
    "        assert n_workers > 1\n",
    "        assert max_buffer_episodes >= n_workers\n",
    "\n",
    "        self.policy_model_fn = policy_model_fn\n",
    "        self.policy_model_max_grad_norm = policy_model_max_grad_norm\n",
    "        self.policy_optimizer_fn = policy_optimizer_fn\n",
    "        self.policy_optimizer_lr = policy_optimizer_lr\n",
    "        self.policy_optimization_epochs = policy_optimization_epochs\n",
    "        self.policy_sample_ratio = policy_sample_ratio\n",
    "        self.policy_clip_range = policy_clip_range\n",
    "        self.policy_stopping_kl = policy_stopping_kl\n",
    "\n",
    "        self.value_model_fn = value_model_fn\n",
    "        self.value_model_max_grad_norm = value_model_max_grad_norm\n",
    "        self.value_optimizer_fn = value_optimizer_fn\n",
    "        self.value_optimizer_lr = value_optimizer_lr\n",
    "        self.value_optimization_epochs = value_optimization_epochs\n",
    "        self.value_sample_ratio = value_sample_ratio\n",
    "        self.value_clip_range = value_clip_range\n",
    "        self.value_stopping_mse = value_stopping_mse\n",
    "\n",
    "        self.episode_buffer_fn = episode_buffer_fn\n",
    "        self.max_buffer_episodes = max_buffer_episodes\n",
    "        self.max_buffer_episode_steps = max_buffer_episode_steps\n",
    "\n",
    "        self.entropy_loss_weight = entropy_loss_weight\n",
    "        self.tau = tau\n",
    "        self.n_workers = n_workers\n",
    "\n",
    "    def optimize_model(self):\n",
    "        states, actions, returns, gaes, logpas = self.episode_buffer.get_stacks()\n",
    "        values = self.value_model(states).detach()\n",
    "        gaes = (gaes - gaes.mean()) / (gaes.std() + EPS)\n",
    "        n_samples = len(actions)\n",
    "        \n",
    "        for _ in range(self.policy_optimization_epochs):\n",
    "            batch_size = int(self.policy_sample_ratio * n_samples)\n",
    "            batch_idxs = np.random.choice(n_samples, batch_size, replace=False)\n",
    "            states_batch = states[batch_idxs]\n",
    "            actions_batch = actions[batch_idxs]\n",
    "            gaes_batch = gaes[batch_idxs]\n",
    "            logpas_batch = logpas[batch_idxs]\n",
    "\n",
    "            logpas_pred, entropies_pred = self.policy_model.get_predictions(states_batch,\n",
    "                                                                            actions_batch)\n",
    "\n",
    "            ratios = (logpas_pred - logpas_batch).exp()\n",
    "            pi_obj = gaes_batch * ratios\n",
    "            pi_obj_clipped = gaes_batch * ratios.clamp(1.0 - self.policy_clip_range,\n",
    "                                                       1.0 + self.policy_clip_range)\n",
    "            policy_loss = -torch.min(pi_obj, pi_obj_clipped).mean()\n",
    "            entropy_loss = -entropies_pred.mean() * self.entropy_loss_weight\n",
    "\n",
    "            self.policy_optimizer.zero_grad()\n",
    "            (policy_loss + entropy_loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.policy_model.parameters(), \n",
    "                                           self.policy_model_max_grad_norm)\n",
    "            self.policy_optimizer.step()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                logpas_pred_all, _ = self.policy_model.get_predictions(states, actions)\n",
    "                kl = (logpas - logpas_pred_all).mean()\n",
    "                if kl.item() > self.policy_stopping_kl:\n",
    "                    break\n",
    "\n",
    "        for _ in range(self.value_optimization_epochs):\n",
    "            batch_size = int(self.value_sample_ratio * n_samples)\n",
    "            batch_idxs = np.random.choice(n_samples, batch_size, replace=False)\n",
    "            states_batch = states[batch_idxs]\n",
    "            returns_batch = returns[batch_idxs]\n",
    "            values_batch = values[batch_idxs]\n",
    "\n",
    "            values_pred = self.value_model(states_batch)\n",
    "            values_pred_clipped = values_batch + (values_pred - values_batch).clamp(-self.value_clip_range, \n",
    "                                                                                    self.value_clip_range)\n",
    "            v_loss = (returns_batch - values_pred).pow(2)\n",
    "            v_loss_clipped = (returns_batch - values_pred_clipped).pow(2)\n",
    "            value_loss = torch.max(v_loss, v_loss_clipped).mul(0.5).mean()\n",
    "\n",
    "            self.value_optimizer.zero_grad()\n",
    "            value_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.value_model.parameters(), \n",
    "                                           self.value_model_max_grad_norm)\n",
    "            self.value_optimizer.step()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                values_pred_all = self.value_model(states)\n",
    "                mse = (values - values_pred_all).pow(2).mul(0.5).mean()\n",
    "                if mse.item() > self.value_stopping_mse:\n",
    "                    break\n",
    "\n",
    "    def train(self, make_envs_fn, make_env_fn, make_env_kargs, seed, gamma, \n",
    "              max_minutes, max_episodes, goal_mean_100_reward):\n",
    "        training_start, last_debug_time = time.time(), float('-inf')\n",
    "\n",
    "        self.checkpoint_dir = tempfile.mkdtemp()\n",
    "        self.make_envs_fn = make_envs_fn\n",
    "        self.make_env_fn = make_env_fn\n",
    "        self.make_env_kargs = make_env_kargs\n",
    "        self.seed = seed\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        env = self.make_env_fn(**self.make_env_kargs, seed=self.seed)\n",
    "        envs = self.make_envs_fn(make_env_fn, make_env_kargs, self.seed, self.n_workers)\n",
    "        torch.manual_seed(self.seed) ; np.random.seed(self.seed) ; random.seed(self.seed)\n",
    "    \n",
    "        nS, nA = env.observation_space.shape, env.action_space.n\n",
    "        self.episode_timestep, self.episode_reward = [], []\n",
    "        self.episode_seconds, self.episode_exploration = [], []\n",
    "        self.evaluation_scores = []\n",
    "\n",
    "        self.policy_model = self.policy_model_fn(nS, nA)\n",
    "        self.policy_optimizer = self.policy_optimizer_fn(self.policy_model, self.policy_optimizer_lr)\n",
    "\n",
    "        self.value_model = self.value_model_fn(nS)\n",
    "        self.value_optimizer = self.value_optimizer_fn(self.value_model, self.value_optimizer_lr)\n",
    "\n",
    "        self.episode_buffer = self.episode_buffer_fn(nS, self.gamma, self.tau,\n",
    "                                                     self.n_workers, \n",
    "                                                     self.max_buffer_episodes,\n",
    "                                                     self.max_buffer_episode_steps)\n",
    "\n",
    "        result = np.empty((max_episodes, 5))\n",
    "        result[:] = np.nan\n",
    "        training_time = 0\n",
    "        episode = 0\n",
    "\n",
    "        # collect n_steps rollout\n",
    "        while True:\n",
    "            episode_timestep, episode_reward, episode_exploration, \\\n",
    "            episode_seconds = self.episode_buffer.fill(envs, self.policy_model, self.value_model)\n",
    "            \n",
    "            n_ep_batch = len(episode_timestep)\n",
    "            self.episode_timestep.extend(episode_timestep)\n",
    "            self.episode_reward.extend(episode_reward)\n",
    "            self.episode_exploration.extend(episode_exploration)\n",
    "            self.episode_seconds.extend(episode_seconds)\n",
    "            self.optimize_model()\n",
    "            self.episode_buffer.clear()\n",
    "\n",
    "            # stats\n",
    "            evaluation_score, _ = self.evaluate(self.policy_model, env)\n",
    "            self.evaluation_scores.extend([evaluation_score,] * n_ep_batch)\n",
    "            for e in range(episode, episode + n_ep_batch):\n",
    "                self.save_checkpoint(e, self.policy_model)\n",
    "            training_time += episode_seconds.sum()\n",
    "\n",
    "            mean_10_reward = np.mean(self.episode_reward[-10:])\n",
    "            std_10_reward = np.std(self.episode_reward[-10:])\n",
    "            mean_100_reward = np.mean(self.episode_reward[-100:])\n",
    "            std_100_reward = np.std(self.episode_reward[-100:])\n",
    "            mean_100_eval_score = np.mean(self.evaluation_scores[-100:])\n",
    "            std_100_eval_score = np.std(self.evaluation_scores[-100:])\n",
    "            mean_100_exp_rat = np.mean(self.episode_exploration[-100:])\n",
    "            std_100_exp_rat = np.std(self.episode_exploration[-100:])\n",
    "            \n",
    "            total_step = int(np.sum(self.episode_timestep))\n",
    "            wallclock_elapsed = time.time() - training_start\n",
    "            result[episode:episode+n_ep_batch] = total_step, mean_100_reward, \\\n",
    "                mean_100_eval_score, training_time, wallclock_elapsed\n",
    "\n",
    "            episode += n_ep_batch\n",
    "\n",
    "            # debug stuff\n",
    "            reached_debug_time = time.time() - last_debug_time >= LEAVE_PRINT_EVERY_N_SECS\n",
    "            reached_max_minutes = wallclock_elapsed >= max_minutes * 60            \n",
    "            reached_max_episodes = episode + self.max_buffer_episodes >= max_episodes\n",
    "            reached_goal_mean_reward = mean_100_eval_score >= goal_mean_100_reward\n",
    "            training_is_over = reached_max_minutes or \\\n",
    "                               reached_max_episodes or \\\n",
    "                               reached_goal_mean_reward\n",
    "            elapsed_str = time.strftime(\"%H:%M:%S\", time.gmtime(time.time() - training_start))\n",
    "            debug_message = 'el {}, ep {:04}, ts {:07}, '\n",
    "            debug_message += 'ar 10 {:05.1f}\\u00B1{:05.1f}, '\n",
    "            debug_message += '100 {:05.1f}\\u00B1{:05.1f}, '\n",
    "            debug_message += 'ex 100 {:02.1f}\\u00B1{:02.1f}, '\n",
    "            debug_message += 'ev {:05.1f}\\u00B1{:05.1f}'\n",
    "            debug_message = debug_message.format(\n",
    "                elapsed_str, episode-1, total_step, mean_10_reward, std_10_reward, \n",
    "                mean_100_reward, std_100_reward, mean_100_exp_rat, std_100_exp_rat,\n",
    "                mean_100_eval_score, std_100_eval_score)\n",
    "            print(debug_message, end='\\r', flush=True)\n",
    "            if reached_debug_time or training_is_over:\n",
    "                print(ERASE_LINE + debug_message, flush=True)\n",
    "                last_debug_time = time.time()\n",
    "            if training_is_over:\n",
    "                if reached_max_minutes: print(u'--> reached_max_minutes \\u2715')\n",
    "                if reached_max_episodes: print(u'--> reached_max_episodes \\u2715')\n",
    "                if reached_goal_mean_reward: print(u'--> reached_goal_mean_reward \\u2713')\n",
    "                break\n",
    "\n",
    "        final_eval_score, score_std = self.evaluate(self.policy_model, env, n_episodes=100)\n",
    "        wallclock_time = time.time() - training_start\n",
    "        print('Training complete.')\n",
    "        print('Final evaluation score {:.2f}\\u00B1{:.2f} in {:.2f}s training time,'\n",
    "              ' {:.2f}s wall-clock time.\\n'.format(\n",
    "                  final_eval_score, score_std, training_time, wallclock_time))\n",
    "        env.close() ; del env\n",
    "        envs.close() ; del envs\n",
    "        self.get_cleaned_checkpoints()\n",
    "        return result, final_eval_score, training_time, wallclock_time\n",
    "\n",
    "    def evaluate(self, eval_model, eval_env, n_episodes=1, greedy=True):\n",
    "        rs = []\n",
    "        for _ in range(n_episodes):\n",
    "            s, d = eval_env.reset(), False\n",
    "            rs.append(0)\n",
    "            for _ in count():\n",
    "                if greedy:\n",
    "                    a = eval_model.select_greedy_action(s)\n",
    "                else: \n",
    "                    a = eval_model.select_action(s)\n",
    "                s, r, d, _ = eval_env.step(a)\n",
    "                rs[-1] += r\n",
    "                if d: break\n",
    "        return np.mean(rs), np.std(rs)\n",
    "\n",
    "    def get_cleaned_checkpoints(self, n_checkpoints=4):\n",
    "        try: \n",
    "            return self.checkpoint_paths\n",
    "        except AttributeError:\n",
    "            self.checkpoint_paths = {}\n",
    "\n",
    "        paths = glob.glob(os.path.join(self.checkpoint_dir, '*.tar'))\n",
    "        paths_dic = {int(path.split('.')[-2]):path for path in paths}\n",
    "        last_ep = max(paths_dic.keys())\n",
    "        # checkpoint_idxs = np.geomspace(1, last_ep+1, n_checkpoints, endpoint=True, dtype=np.int)-1\n",
    "        checkpoint_idxs = np.linspace(1, last_ep+1, n_checkpoints, endpoint=True, dtype=np.int)-1\n",
    "\n",
    "        for idx, path in paths_dic.items():\n",
    "            if idx in checkpoint_idxs:\n",
    "                self.checkpoint_paths[idx] = path\n",
    "            else:\n",
    "                os.unlink(path)\n",
    "\n",
    "        return self.checkpoint_paths\n",
    "\n",
    "    def demo_last(self, title='Fully-trained {} Agent', n_episodes=2, max_n_videos=2):\n",
    "        env = self.make_env_fn(**self.make_env_kargs, monitor_mode='evaluation', render=True, record=True)\n",
    "\n",
    "        checkpoint_paths = self.get_cleaned_checkpoints()\n",
    "        last_ep = max(checkpoint_paths.keys())\n",
    "        self.policy_model.load_state_dict(torch.load(checkpoint_paths[last_ep]))\n",
    "\n",
    "        self.evaluate(self.policy_model, env, n_episodes=n_episodes)\n",
    "        env.close()\n",
    "        data = get_gif_html(env_videos=env.videos, \n",
    "                            title=title.format(self.__class__.__name__),\n",
    "                            max_n_videos=max_n_videos)\n",
    "        del env\n",
    "        return HTML(data=data)\n",
    "\n",
    "    def demo_progression(self, title='{} Agent progression', max_n_videos=4):\n",
    "        env = self.make_env_fn(**self.make_env_kargs, monitor_mode='evaluation', render=True, record=True)\n",
    "\n",
    "        checkpoint_paths = self.get_cleaned_checkpoints()\n",
    "        for i in sorted(checkpoint_paths.keys()):\n",
    "            self.policy_model.load_state_dict(torch.load(checkpoint_paths[i]))\n",
    "            self.evaluate(self.policy_model, env, n_episodes=1)\n",
    "\n",
    "        env.close()\n",
    "        data = get_gif_html(env_videos=env.videos, \n",
    "                            title=title.format(self.__class__.__name__),\n",
    "                            subtitle_eps=sorted(checkpoint_paths.keys()),\n",
    "                            max_n_videos=max_n_videos)\n",
    "        del env\n",
    "        return HTML(data=data)\n",
    "\n",
    "    def save_checkpoint(self, episode_idx, model):\n",
    "        torch.save(model.state_dict(),\n",
    "                   os.path.join(self.checkpoint_dir, 'model.{}.tar'.format(episode_idx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ppo_results = []\n",
    "best_agent, best_eval_score = None, float('-inf')\n",
    "for seed in SEEDS:\n",
    "    environment_settings = {\n",
    "        'env_name': 'LunarLander-v2',\n",
    "        'gamma': 0.99,\n",
    "        'max_minutes': 20,\n",
    "        'max_episodes': 2000,\n",
    "        'goal_mean_100_reward': 250\n",
    "    }\n",
    "\n",
    "    policy_model_fn = lambda nS, nA: FCCA(nS, nA, hidden_dims=(256,256))\n",
    "    policy_model_max_grad_norm = float('inf')\n",
    "    policy_optimizer_fn = lambda net, lr: optim.Adam(net.parameters(), lr=lr)\n",
    "    policy_optimizer_lr = 0.0003\n",
    "    policy_optimization_epochs = 80\n",
    "    policy_sample_ratio = 0.8\n",
    "    policy_clip_range = 0.1\n",
    "    policy_stopping_kl = 0.02\n",
    "\n",
    "    value_model_fn = lambda nS: FCV(nS, hidden_dims=(256,256))\n",
    "    value_model_max_grad_norm = float('inf')\n",
    "    value_optimizer_fn = lambda net, lr: optim.Adam(net.parameters(), lr=lr)\n",
    "    value_optimizer_lr = 0.0005\n",
    "    value_optimization_epochs = 80\n",
    "    value_sample_ratio = 0.8\n",
    "    value_clip_range = float('inf')\n",
    "    value_stopping_mse = 25\n",
    "\n",
    "    episode_buffer_fn = lambda sd, g, t, nw, me, mes: EpisodeBuffer(sd, g, t, nw, me, mes)\n",
    "    max_buffer_episodes = 16\n",
    "    max_buffer_episode_steps = 1000\n",
    "    \n",
    "    entropy_loss_weight = 0.01\n",
    "    tau = 0.97\n",
    "    n_workers = 8\n",
    "\n",
    "    env_name, gamma, max_minutes, \\\n",
    "    max_episodes, goal_mean_100_reward = environment_settings.values()\n",
    "    agent = PPO(policy_model_fn, \n",
    "                policy_model_max_grad_norm,\n",
    "                policy_optimizer_fn,\n",
    "                policy_optimizer_lr,\n",
    "                policy_optimization_epochs,\n",
    "                policy_sample_ratio,\n",
    "                policy_clip_range,\n",
    "                policy_stopping_kl,\n",
    "                value_model_fn, \n",
    "                value_model_max_grad_norm,\n",
    "                value_optimizer_fn,\n",
    "                value_optimizer_lr,\n",
    "                value_optimization_epochs,\n",
    "                value_sample_ratio,\n",
    "                value_clip_range,\n",
    "                value_stopping_mse,\n",
    "                episode_buffer_fn,\n",
    "                max_buffer_episodes,\n",
    "                max_buffer_episode_steps,\n",
    "                entropy_loss_weight,\n",
    "                tau,\n",
    "                n_workers)\n",
    "\n",
    "    make_envs_fn = lambda mef, mea, s, n: MultiprocessEnv(mef, mea, s, n)\n",
    "    make_env_fn, make_env_kargs = get_make_env_fn(env_name=env_name)\n",
    "    result, final_eval_score, training_time, wallclock_time = agent.train(make_envs_fn,\n",
    "                                                                          make_env_fn,\n",
    "                                                                          make_env_kargs,\n",
    "                                                                          seed,\n",
    "                                                                          gamma,\n",
    "                                                                          max_minutes,\n",
    "                                                                          max_episodes,\n",
    "                                                                          goal_mean_100_reward)\n",
    "    ppo_results.append(result)\n",
    "    if final_eval_score > best_eval_score:\n",
    "        best_eval_score = final_eval_score\n",
    "        best_agent = agent\n",
    "ppo_results = np.array(ppo_results)\n",
    "_ = BEEP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "best_agent.demo_progression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_agent.demo_last()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_max_t, ppo_max_r, ppo_max_s, \\\n",
    "ppo_max_sec, ppo_max_rt = np.max(ppo_results, axis=0).T\n",
    "ppo_min_t, ppo_min_r, ppo_min_s, \\\n",
    "ppo_min_sec, ppo_min_rt = np.min(ppo_results, axis=0).T\n",
    "ppo_mean_t, ppo_mean_r, ppo_mean_s, \\\n",
    "ppo_mean_sec, ppo_mean_rt = np.mean(ppo_results, axis=0).T\n",
    "ppo_x = np.arange(len(ppo_mean_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 1, figsize=(15,10), sharey=False, sharex=True)\n",
    "\n",
    "# PPO\n",
    "axs[0].plot(ppo_max_r, 'k', linewidth=1)\n",
    "axs[0].plot(ppo_min_r, 'k', linewidth=1)\n",
    "axs[0].plot(ppo_mean_r, 'k:', label='PPO', linewidth=2)\n",
    "axs[0].fill_between(\n",
    "    ppo_x, ppo_min_r, ppo_max_r, facecolor='k', alpha=0.3)\n",
    "\n",
    "axs[1].plot(ppo_max_s, 'k', linewidth=1)\n",
    "axs[1].plot(ppo_min_s, 'k', linewidth=1)\n",
    "axs[1].plot(ppo_mean_s, 'k:', label='PPO', linewidth=2)\n",
    "axs[1].fill_between(\n",
    "    ppo_x, ppo_min_s, ppo_max_s, facecolor='k', alpha=0.3)\n",
    "\n",
    "# ALL\n",
    "axs[0].set_title('Moving Avg Reward (Training)')\n",
    "axs[1].set_title('Moving Avg Reward (Evaluation)')\n",
    "plt.xlabel('Episodes')\n",
    "axs[0].legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 1, figsize=(15,15), sharey=False, sharex=True)\n",
    "\n",
    "# PPO\n",
    "axs[0].plot(ppo_max_t, 'k', linewidth=1)\n",
    "axs[0].plot(ppo_min_t, 'k', linewidth=1)\n",
    "axs[0].plot(ppo_mean_t, 'k:', label='PPO', linewidth=2)\n",
    "axs[0].fill_between(\n",
    "    ppo_x, ppo_min_t, ppo_max_t, facecolor='k', alpha=0.3)\n",
    "\n",
    "axs[1].plot(ppo_max_sec, 'k', linewidth=1)\n",
    "axs[1].plot(ppo_min_sec, 'k', linewidth=1)\n",
    "axs[1].plot(ppo_mean_sec, 'k:', label='PPO', linewidth=2)\n",
    "axs[1].fill_between(\n",
    "    ppo_x, ppo_min_sec, ppo_max_sec, facecolor='k', alpha=0.3)\n",
    "\n",
    "axs[2].plot(ppo_max_rt, 'k', linewidth=1)\n",
    "axs[2].plot(ppo_min_rt, 'k', linewidth=1)\n",
    "axs[2].plot(ppo_mean_rt, 'k:', label='PPO', linewidth=2)\n",
    "axs[2].fill_between(\n",
    "    ppo_x, ppo_min_rt, ppo_max_rt, facecolor='k', alpha=0.3)\n",
    "\n",
    "# ALL\n",
    "axs[0].set_title('Total Steps')\n",
    "axs[1].set_title('Training Time')\n",
    "axs[2].set_title('Wall-clock Time')\n",
    "plt.xlabel('Episodes')\n",
    "axs[0].legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_root_dir = os.path.join(RESULTS_DIR, 'ppo')\n",
    "not os.path.exists(ppo_root_dir) and os.makedirs(ppo_root_dir)\n",
    "\n",
    "np.save(os.path.join(ppo_root_dir, 'x'), ppo_x)\n",
    "\n",
    "np.save(os.path.join(ppo_root_dir, 'max_r'), ppo_max_r)\n",
    "np.save(os.path.join(ppo_root_dir, 'min_r'), ppo_min_r)\n",
    "np.save(os.path.join(ppo_root_dir, 'mean_r'), ppo_mean_r)\n",
    "\n",
    "np.save(os.path.join(ppo_root_dir, 'max_s'), ppo_max_s)\n",
    "np.save(os.path.join(ppo_root_dir, 'min_s'), ppo_min_s )\n",
    "np.save(os.path.join(ppo_root_dir, 'mean_s'), ppo_mean_s)\n",
    "\n",
    "np.save(os.path.join(ppo_root_dir, 'max_t'), ppo_max_t)\n",
    "np.save(os.path.join(ppo_root_dir, 'min_t'), ppo_min_t)\n",
    "np.save(os.path.join(ppo_root_dir, 'mean_t'), ppo_mean_t)\n",
    "\n",
    "np.save(os.path.join(ppo_root_dir, 'max_sec'), ppo_max_sec)\n",
    "np.save(os.path.join(ppo_root_dir, 'min_sec'), ppo_min_sec)\n",
    "np.save(os.path.join(ppo_root_dir, 'mean_sec'), ppo_mean_sec)\n",
    "\n",
    "np.save(os.path.join(ppo_root_dir, 'max_rt'), ppo_max_rt)\n",
    "np.save(os.path.join(ppo_root_dir, 'min_rt'), ppo_min_rt)\n",
    "np.save(os.path.join(ppo_root_dir, 'mean_rt'), ppo_mean_rt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
