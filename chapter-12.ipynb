{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DDPG, TD3, SAC, PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec 10 22:28:24 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1080 Ti     Off | 00000000:01:00.0 Off |                  N/A |\n",
      "|  0%   23C    P8               9W / 250W |     15MiB / 11264MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      1205      G   /usr/lib/xorg/Xorg                            9MiB |\n",
      "|    0   N/A  N/A      1562      G   /usr/bin/gnome-shell                          2MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from beads_gym.environment.beads_cart_pole_environment import BeadsCartPoleEnvironment\n",
    "from beads_gym.environment.beads_quad_copter_environment import BeadsQuadCopterEnvironment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings ; warnings.filterwarnings('ignore')\n",
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.multiprocessing as mp\n",
    "from torch.distributions import Normal\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "from itertools import count\n",
    "import moviepy.editor as mpy\n",
    "\n",
    "import os.path\n",
    "import tempfile\n",
    "import random\n",
    "import base64\n",
    "import glob\n",
    "import time\n",
    "import json\n",
    "import gym\n",
    "import io\n",
    "import os\n",
    "import gc\n",
    "\n",
    "from gym import wrappers\n",
    "from IPython.display import HTML\n",
    "\n",
    "LEAVE_PRINT_EVERY_N_SECS = 300\n",
    "ERASE_LINE = '\\x1b[2K'\n",
    "EPS = 1e-6\n",
    "BEEP = lambda: os.system(\"printf '\\a'\")\n",
    "RESULTS_DIR = os.path.join('..', 'results')\n",
    "SEEDS = (12, 34, 56) #, 78, 90)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f6b83c15e50>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = BeadsQuadCopterEnvironment()\n",
    "env.reset()\n",
    "plt.imshow(env.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')\n",
    "params = {\n",
    "    'figure.figsize': (15, 8),\n",
    "    'font.size': 24,\n",
    "    'legend.fontsize': 20,\n",
    "    'axes.titlesize': 28,\n",
    "    'axes.labelsize': 24,\n",
    "    'xtick.labelsize': 20,\n",
    "    'ytick.labelsize': 20\n",
    "}\n",
    "pylab.rcParams.update(params)\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_make_env_fn(**kargs):\n",
    "    def make_env_fn(env_name, seed=None, render=None, record=False,\n",
    "                    unwrapped=False, monitor_mode=None, \n",
    "                    inner_wrappers=None, outer_wrappers=None):\n",
    "        mdir = tempfile.mkdtemp()\n",
    "        env = None\n",
    "        if render:\n",
    "            try:\n",
    "                env = gym.make(env_name, render=render)\n",
    "            except:\n",
    "                pass\n",
    "        if env_name == \"BeadsCartPoleEnvironment\":\n",
    "            env = BeadsCartPoleEnvironment()\n",
    "            env.do_render = render\n",
    "            env.do_record = record\n",
    "            env.monitor_mode = monitor_mode\n",
    "            return env\n",
    "        if env_name == \"BeadsQuadCopterEnvironment\":\n",
    "            env = BeadsQuadCopterEnvironment()\n",
    "            env.do_render = render\n",
    "            env.do_record = record\n",
    "            env.monitor_mode = monitor_mode\n",
    "            return env\n",
    "        if env is None:\n",
    "            env = gym.make(env_name)\n",
    "        if seed is not None: env.seed(seed)\n",
    "        env = env.unwrapped if unwrapped else env\n",
    "        if inner_wrappers:\n",
    "            for wrapper in inner_wrappers:\n",
    "                env = wrapper(env)\n",
    "        env = wrappers.Monitor(\n",
    "            env, mdir, force=True, \n",
    "            mode=monitor_mode, \n",
    "            video_callable=lambda e_idx: record) if monitor_mode else env\n",
    "        if outer_wrappers:\n",
    "            for wrapper in outer_wrappers:\n",
    "                env = wrapper(env)\n",
    "        return env\n",
    "    return make_env_fn, kargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_videos_html(env_videos, title, max_n_videos=4):\n",
    "    videos = np.array(env_videos)\n",
    "    if len(videos) == 0:\n",
    "        return\n",
    "    \n",
    "    n_videos = max(1, min(max_n_videos, len(videos)))\n",
    "    idxs = np.linspace(0, len(videos) - 1, n_videos).astype(int) if n_videos > 1 else [-1,]\n",
    "    videos = videos[idxs,...]\n",
    "\n",
    "    strm = '<h2>{}<h2>'.format(title)\n",
    "    for video_path, meta_path in videos:\n",
    "        video = io.open(video_path, 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "\n",
    "        with open(meta_path) as data_file:    \n",
    "            meta = json.load(data_file)\n",
    "\n",
    "        html_tag = \"\"\"\n",
    "        <h3>{0}<h3/>\n",
    "        <video width=\"960\" height=\"540\" controls>\n",
    "            <source src=\"data:video/mp4;base64,{1}\" type=\"video/mp4\" />\n",
    "        </video>\"\"\"\n",
    "        strm += html_tag.format('Episode ' + str(meta['episode_id']), encoded.decode('ascii'))\n",
    "    return strm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gif_html(video, title, video_id):\n",
    "    video = np.array(video)\n",
    "    num_frames = len(video)\n",
    "    fps = 10\n",
    "\n",
    "    # Create a VideoClip\n",
    "    clip = mpy.VideoClip(\n",
    "        # fun,\n",
    "        lambda t: video[int(t * fps)],\n",
    "        duration=num_frames / fps,\n",
    "    )\n",
    "\n",
    "    # Write the VideoClip to a file\n",
    "    clip.write_videofile(f\"{title}_{video_id}.mp4\", fps=fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RenderUint8(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "    def reset(self, **kwargs):\n",
    "        return self.env.reset(**kwargs)\n",
    "    def render(self, mode='rgb_array'):\n",
    "        frame = self.env.render(mode=mode)\n",
    "        return frame.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCQV(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim, \n",
    "                 output_dim, \n",
    "                 hidden_dims=(32,32), \n",
    "                 activation_fc=F.relu):\n",
    "        super(FCQV, self).__init__()\n",
    "        self.activation_fc = activation_fc\n",
    "\n",
    "        self.input_layer = nn.Linear(input_dim, hidden_dims[0])\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for i in range(len(hidden_dims)-1):\n",
    "            in_dim = hidden_dims[i]\n",
    "            if i == 0: \n",
    "                in_dim += output_dim\n",
    "            hidden_layer = nn.Linear(in_dim, hidden_dims[i+1])\n",
    "            self.hidden_layers.append(hidden_layer)\n",
    "        self.output_layer = nn.Linear(hidden_dims[-1], 1)\n",
    "\n",
    "        device = \"cpu\"\n",
    "        if torch.cuda.is_available():\n",
    "            device = \"cuda:0\"\n",
    "        self.device = torch.device(device)\n",
    "        self.to(self.device)\n",
    "    \n",
    "    def _format(self, state, action):\n",
    "        x, u = state, action\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.tensor(x, \n",
    "                             device=self.device, \n",
    "                             dtype=torch.float32)\n",
    "            x = x.unsqueeze(0)\n",
    "        if not isinstance(u, torch.Tensor):\n",
    "            u = torch.tensor(u, \n",
    "                             device=self.device, \n",
    "                             dtype=torch.float32)\n",
    "            u = u.unsqueeze(0)\n",
    "        return x, u\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x, u = self._format(state, action)\n",
    "        x = self.activation_fc(self.input_layer(x))\n",
    "        for i, hidden_layer in enumerate(self.hidden_layers):\n",
    "            if i == 0:\n",
    "                x = torch.cat((x, u), dim=1)\n",
    "            x = self.activation_fc(hidden_layer(x))\n",
    "        return self.output_layer(x)\n",
    "    \n",
    "    def load(self, experiences):\n",
    "        states, actions, new_states, rewards, is_terminals = experiences\n",
    "        states = torch.from_numpy(states).float().to(self.device)\n",
    "        actions = torch.from_numpy(actions).float().to(self.device)\n",
    "        new_states = torch.from_numpy(new_states).float().to(self.device)\n",
    "        rewards = torch.from_numpy(rewards).float().to(self.device)\n",
    "        is_terminals = torch.from_numpy(is_terminals).float().to(self.device)\n",
    "        return states, actions, new_states, rewards, is_terminals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCDP(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim,\n",
    "                 action_bounds,\n",
    "                 hidden_dims=(32,32), \n",
    "                 activation_fc=F.selu,\n",
    "                 out_activation_fc=F.tanh):\n",
    "        super(FCDP, self).__init__()\n",
    "        self.activation_fc = activation_fc\n",
    "        self.out_activation_fc = out_activation_fc\n",
    "        self.act_min, self.act_max = action_bounds\n",
    "\n",
    "        self.input_layer = nn.Linear(input_dim, hidden_dims[0])\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for i in range(len(hidden_dims)-1):\n",
    "            hidden_layer = nn.Linear(hidden_dims[i], hidden_dims[i+1])\n",
    "            self.hidden_layers.append(hidden_layer)\n",
    "        self.output_layer = nn.Linear(hidden_dims[-1], len(self.act_max))\n",
    "\n",
    "        device = \"cpu\"\n",
    "        if torch.cuda.is_available():\n",
    "            device = \"cuda:0\"\n",
    "        self.device = torch.device(device)\n",
    "        self.to(self.device)\n",
    "        \n",
    "        self.act_min = torch.tensor(self.act_min,\n",
    "                                    device=self.device, \n",
    "                                    dtype=torch.float32)\n",
    "\n",
    "        self.act_max = torch.tensor(self.act_max,\n",
    "                                    device=self.device, \n",
    "                                    dtype=torch.float32)\n",
    "        \n",
    "        self.nn_min = self.out_activation_fc(\n",
    "            torch.Tensor([float('-inf')])).to(self.device)\n",
    "        self.nn_max = self.out_activation_fc(\n",
    "            torch.Tensor([float('inf')])).to(self.device)\n",
    "        self.rescale_fn = lambda x: (x - self.nn_min) * (self.act_max - self.act_min) / \\\n",
    "                                    (self.nn_max - self.nn_min) + self.act_min\n",
    "\n",
    "    def _format(self, state):\n",
    "        x = state\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.tensor(x, \n",
    "                             device=self.device, \n",
    "                             dtype=torch.float32)\n",
    "            x = x.unsqueeze(0)\n",
    "        return x\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self._format(state)\n",
    "        x = self.activation_fc(self.input_layer(x))\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            x = self.activation_fc(hidden_layer(x))\n",
    "        x = self.output_layer(x)\n",
    "        x = self.out_activation_fc(x)\n",
    "        return self.rescale_fn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self, \n",
    "                 max_size=10000, \n",
    "                 batch_size=64):\n",
    "        self.ss_mem = np.empty(shape=(max_size), dtype=np.ndarray)\n",
    "        self.as_mem = np.empty(shape=(max_size), dtype=np.ndarray)\n",
    "        self.rs_mem = np.empty(shape=(max_size), dtype=np.ndarray)\n",
    "        self.ps_mem = np.empty(shape=(max_size), dtype=np.ndarray)\n",
    "        self.ds_mem = np.empty(shape=(max_size), dtype=np.ndarray)\n",
    "\n",
    "        self.max_size = max_size\n",
    "        self.batch_size = batch_size\n",
    "        self._idx = 0\n",
    "        self.size = 0\n",
    "    \n",
    "    def store(self, sample):\n",
    "        s, a, r, p, d = sample\n",
    "        self.ss_mem[self._idx] = s\n",
    "        self.as_mem[self._idx] = a\n",
    "        self.rs_mem[self._idx] = r\n",
    "        self.ps_mem[self._idx] = p\n",
    "        self.ds_mem[self._idx] = d\n",
    "        \n",
    "        self._idx += 1\n",
    "        self._idx = self._idx % self.max_size\n",
    "\n",
    "        self.size += 1\n",
    "        self.size = min(self.size, self.max_size)\n",
    "\n",
    "    def sample(self, batch_size=None):\n",
    "        if batch_size == None:\n",
    "            batch_size = self.batch_size\n",
    "\n",
    "        idxs = np.random.choice(\n",
    "            self.size, batch_size, replace=False)\n",
    "        experiences = np.vstack(self.ss_mem[idxs]), \\\n",
    "                      np.vstack(self.as_mem[idxs]), \\\n",
    "                      np.vstack(self.rs_mem[idxs]), \\\n",
    "                      np.vstack(self.ps_mem[idxs]), \\\n",
    "                      np.vstack(self.ds_mem[idxs])\n",
    "        return experiences\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedyStrategy():\n",
    "    def __init__(self, bounds):\n",
    "        self.low, self.high = bounds\n",
    "        self.ratio_noise_injected = 0\n",
    "\n",
    "    def select_action(self, model, state):\n",
    "        with torch.no_grad():\n",
    "            greedy_action = model(state).cpu().detach().data.numpy().squeeze()\n",
    "\n",
    "        action = np.clip(greedy_action, self.low, self.high)\n",
    "        return np.reshape(action, self.high.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalNoiseStrategy():\n",
    "    def __init__(self, bounds, exploration_noise_ratio=0.1, exploration_noise_amplitude=None, ou_process=False):\n",
    "        self.low, self.high = bounds\n",
    "        \n",
    "        if exploration_noise_ratio is None: assert exploration_noise_amplitude is not None\n",
    "        if exploration_noise_amplitude is None: assert exploration_noise_ratio is not None\n",
    "        self.exploration_noise_ratio = exploration_noise_ratio\n",
    "        self.exploration_noise_amplitude = exploration_noise_amplitude\n",
    "        self.ou_process = ou_process\n",
    "        self.prev_noise = np.zeros(len(self.high))\n",
    "        self.ratio_noise_injected = 0\n",
    "        \n",
    "    def select_action(self, model, state, max_exploration=False):\n",
    "        with torch.no_grad():\n",
    "            greedy_action = model(state).cpu().detach().data.numpy().squeeze()\n",
    "            \n",
    "        noise = self.prev_noise if self.ou_process else np.zeros(len(self.high))\n",
    "        if max_exploration:\n",
    "            noise += np.random.normal(loc=0, scale=self.high, size=len(self.high))\n",
    "        else:\n",
    "            if self.exploration_noise_ratio is not None:\n",
    "                noise += np.random.normal(loc=0, scale=1, size=len(self.high)) * np.abs(greedy_action) * self.exploration_noise_ratio\n",
    "            elif self.exploration_noise_amplitude is not None:\n",
    "                noise += np.random.normal(loc=0, scale=self.exploration_noise_amplitude, size=len(self.high))\n",
    "            else:\n",
    "                raise ValueError(\"No exploration noise specified\")\n",
    "\n",
    "        noisy_action = greedy_action + noise\n",
    "        self.prev_noise = noise\n",
    "        action = np.clip(noisy_action, self.low, self.high)\n",
    "        \n",
    "        self.ratio_noise_injected = np.mean(abs((greedy_action - action)/(self.high - self.low)))\n",
    "        return action\n",
    "    \n",
    "    \n",
    "class NormalNoiseDecayStrategy():\n",
    "    def __init__(\n",
    "        self,\n",
    "        bounds,\n",
    "        init_noise_ratio_mult=0.5, min_noise_ratio_mult=0.1,\n",
    "        init_noise_ratio_add=0.5, min_noise_ratio_add=0.1,\n",
    "        decay_steps=10000,\n",
    "    ):\n",
    "        self.t = 0\n",
    "        self.low, self.high = bounds\n",
    "        self.noise_ratio_mult = init_noise_ratio_mult\n",
    "        self.init_noise_ratio_mult = init_noise_ratio_mult\n",
    "        self.min_noise_ratio_mult = min_noise_ratio_mult\n",
    "        self.noise_ratio_add = init_noise_ratio_add\n",
    "        self.init_noise_ratio_add = init_noise_ratio_add\n",
    "        self.min_noise_ratio_add = min_noise_ratio_add\n",
    "        self.decay_steps = decay_steps\n",
    "        self.ratio_noise_injected = 0\n",
    "\n",
    "    def _noise_ratio_update(self):\n",
    "        noise_ratio = 1 - self.t / self.decay_steps\n",
    "        noise_ratio_mult = (self.init_noise_ratio_mult - self.min_noise_ratio_mult) * noise_ratio + self.min_noise_ratio_mult\n",
    "        self.noise_ratio_mult = np.clip(noise_ratio_mult, self.min_noise_ratio_mult, self.init_noise_ratio_mult)\n",
    "        noise_ratio_add = (self.init_noise_ratio_add - self.min_noise_ratio_add) * noise_ratio + self.min_noise_ratio_add\n",
    "        self.noise_ratio_add = np.clip(noise_ratio_add, self.min_noise_ratio_add, self.init_noise_ratio_add)\n",
    "        self.t += 1\n",
    "\n",
    "    def select_action(self, model, state, max_exploration=False):\n",
    "        with torch.no_grad():\n",
    "            greedy_action = model(state).cpu().detach().data.numpy().squeeze()\n",
    "            \n",
    "        noise = np.zeros(len(self.high))\n",
    "        if max_exploration:\n",
    "            noise += np.random.normal(loc=0, scale=self.high, size=len(self.high))\n",
    "        else:\n",
    "            mult_noise_scale = np.abs(greedy_action) * self.noise_ratio_mult\n",
    "            noise += np.random.normal(loc=0, scale=mult_noise_scale, size=len(self.high))\n",
    "            noise += np.random.normal(loc=0, scale=self.noise_ratio_add, size=len(self.high))\n",
    "\n",
    "        noisy_action = greedy_action + noise\n",
    "        action = np.clip(noisy_action, self.low, self.high)\n",
    "        \n",
    "        self._noise_ratio_update()\n",
    "        \n",
    "        self.ratio_noise_injected = np.mean(abs((greedy_action - action)/(self.high - self.low)))\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPG():\n",
    "    def __init__(self, \n",
    "                 replay_buffer_fn,\n",
    "                 policy_model_fn, \n",
    "                 policy_max_grad_norm, \n",
    "                 policy_optimizer_fn, \n",
    "                 policy_optimizer_lr,\n",
    "                 value_model_fn, \n",
    "                 value_max_grad_norm, \n",
    "                 value_optimizer_fn, \n",
    "                 value_optimizer_lr, \n",
    "                 training_strategy_fn,\n",
    "                 evaluation_strategy_fn,\n",
    "                 n_warmup_batches,\n",
    "                 update_target_every_steps,\n",
    "                 tau):\n",
    "        self.replay_buffer_fn = replay_buffer_fn\n",
    "\n",
    "        self.policy_model_fn = policy_model_fn\n",
    "        self.policy_max_grad_norm = policy_max_grad_norm\n",
    "        self.policy_optimizer_fn = policy_optimizer_fn\n",
    "        self.policy_optimizer_lr = policy_optimizer_lr\n",
    "        \n",
    "        self.value_model_fn = value_model_fn\n",
    "        self.value_max_grad_norm = value_max_grad_norm\n",
    "        self.value_optimizer_fn = value_optimizer_fn\n",
    "        self.value_optimizer_lr = value_optimizer_lr\n",
    "\n",
    "        self.training_strategy_fn = training_strategy_fn\n",
    "        self.evaluation_strategy_fn = evaluation_strategy_fn\n",
    "\n",
    "        self.n_warmup_batches = n_warmup_batches\n",
    "        self.update_target_every_steps = update_target_every_steps\n",
    "        self.tau = tau\n",
    "\n",
    "    def optimize_model(self, experiences):\n",
    "        states, actions, rewards, next_states, is_terminals = experiences\n",
    "\n",
    "        argmax_a_q_sp = self.target_policy_model(next_states)\n",
    "        max_a_q_sp = self.target_value_model(next_states, argmax_a_q_sp)\n",
    "        target_q_sa = rewards + self.gamma * max_a_q_sp * (1 - is_terminals)\n",
    "        q_sa = self.online_value_model(states, actions)\n",
    "        td_error = q_sa - target_q_sa.detach()\n",
    "        value_loss = td_error.pow(2).mul(0.5).mean()\n",
    "        self.value_optimizer.zero_grad()\n",
    "        value_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.online_value_model.parameters(), \n",
    "                                       self.value_max_grad_norm)\n",
    "        self.value_optimizer.step()\n",
    "\n",
    "        argmax_a_q_s = self.online_policy_model(states)\n",
    "        max_a_q_s = self.online_value_model(states, argmax_a_q_s)\n",
    "        policy_loss = -max_a_q_s.mean()\n",
    "        self.policy_optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.online_policy_model.parameters(), \n",
    "                                       self.policy_max_grad_norm)        \n",
    "        self.policy_optimizer.step()\n",
    "\n",
    "    def interaction_step(self, state, env, state_noise=None):\n",
    "        min_samples = self.replay_buffer.batch_size * self.n_warmup_batches\n",
    "        noisy_state = state if state_noise is None else state + state_noise\n",
    "        action = self.training_strategy.select_action(self.online_policy_model, \n",
    "                                                      noisy_state, \n",
    "                                                      len(self.replay_buffer) < min_samples)\n",
    "        new_state, reward, is_terminal, info = env.step(action)\n",
    "        is_truncated = 'TimeLimit.truncated' in info and info['TimeLimit.truncated']\n",
    "        is_failure = is_terminal and not is_truncated\n",
    "        experience = (state, action, reward, new_state, float(is_failure))\n",
    "        self.replay_buffer.store(experience)\n",
    "        self.episode_reward[-1] += reward\n",
    "        self.episode_timestep[-1] += 1\n",
    "        self.episode_exploration[-1] += self.training_strategy.ratio_noise_injected\n",
    "        return new_state, is_terminal\n",
    "    \n",
    "    def update_networks(self, tau=None):\n",
    "        tau = self.tau if tau is None else tau\n",
    "        for target, online in zip(self.target_value_model.parameters(), \n",
    "                                  self.online_value_model.parameters()):\n",
    "            target_ratio = (1.0 - tau) * target.data\n",
    "            online_ratio = tau * online.data\n",
    "            mixed_weights = target_ratio + online_ratio\n",
    "            target.data.copy_(mixed_weights)\n",
    "\n",
    "        for target, online in zip(self.target_policy_model.parameters(), \n",
    "                                  self.online_policy_model.parameters()):\n",
    "            target_ratio = (1.0 - tau) * target.data\n",
    "            online_ratio = tau * online.data\n",
    "            mixed_weights = target_ratio + online_ratio\n",
    "            target.data.copy_(mixed_weights)\n",
    "\n",
    "    def train(self, make_env_fn, make_env_kargs, seed, gamma, \n",
    "              max_minutes, max_episodes, goal_mean_100_reward):\n",
    "        training_start, last_debug_time = time.time(), float('-inf')\n",
    "\n",
    "        self.checkpoint_dir = tempfile.mkdtemp()\n",
    "        self.make_env_fn = make_env_fn\n",
    "        self.make_env_kargs = make_env_kargs\n",
    "        self.seed = seed\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        env = self.make_env_fn(**self.make_env_kargs, seed=self.seed)\n",
    "        torch.manual_seed(self.seed) ; np.random.seed(self.seed) ; random.seed(self.seed)\n",
    "    \n",
    "        nS, nA = env.observation_space.shape[0], env.action_space.shape[0]\n",
    "        action_bounds = env.action_space.low, env.action_space.high\n",
    "        self.episode_timestep = []\n",
    "        self.episode_reward = []\n",
    "        self.episode_seconds = []\n",
    "        self.evaluation_scores = []        \n",
    "        self.episode_exploration = []\n",
    "        \n",
    "        self.target_value_model = self.value_model_fn(nS, nA)\n",
    "        self.online_value_model = self.value_model_fn(nS, nA)\n",
    "        self.target_policy_model = self.policy_model_fn(nS, action_bounds)\n",
    "        self.online_policy_model = self.policy_model_fn(nS, action_bounds)\n",
    "        self.update_networks(tau=1.0)\n",
    "        self.value_optimizer = self.value_optimizer_fn(self.online_value_model, \n",
    "                                                       self.value_optimizer_lr)        \n",
    "        self.policy_optimizer = self.policy_optimizer_fn(self.online_policy_model, \n",
    "                                                         self.policy_optimizer_lr)\n",
    "\n",
    "        self.replay_buffer = self.replay_buffer_fn()\n",
    "        self.training_strategy = training_strategy_fn(action_bounds)\n",
    "        self.evaluation_strategy = evaluation_strategy_fn(action_bounds)\n",
    "                    \n",
    "        result = np.empty((max_episodes, 6))\n",
    "        result[:] = np.nan\n",
    "        training_time = 0\n",
    "        state_noise_scale = 0.05\n",
    "        reasonable_bound = 0.1\n",
    "        how_often_within_reasonable_bounds = (np.linalg.norm(np.random.normal(loc=0, scale=state_noise_scale, size=(10000, 3)), axis=1) < reasonable_bound).mean()\n",
    "        print(\n",
    "            f\"With the scale of {state_noise_scale:.2f}, the noise vector will be \"\n",
    "            f\"{100 * how_often_within_reasonable_bounds:.2f}% of the time within a ball \"\n",
    "            f\"of radius: {reasonable_bound}\"\n",
    "        )\n",
    "        for episode in range(1, max_episodes + 1):\n",
    "            episode_start = time.time()\n",
    "            \n",
    "            state, is_terminal = env.reset(), False\n",
    "            self.episode_reward.append(0.0)\n",
    "            self.episode_timestep.append(0.0)\n",
    "            self.episode_exploration.append(0.0)\n",
    "\n",
    "            for step in count():\n",
    "                if np.random.uniform() > 1.0:\n",
    "                    state_noise = np.random.normal(loc=0, scale=state_noise_scale, size=len(state))\n",
    "                else:\n",
    "                    state_noise = None\n",
    "                state, is_terminal = self.interaction_step(state, env, state_noise)\n",
    "\n",
    "                min_samples = self.replay_buffer.batch_size * self.n_warmup_batches\n",
    "                if len(self.replay_buffer) > min_samples:\n",
    "                    experiences = self.replay_buffer.sample()\n",
    "                    experiences = self.online_value_model.load(experiences)\n",
    "                    self.optimize_model(experiences)\n",
    "\n",
    "                if np.sum(self.episode_timestep) % self.update_target_every_steps == 0:\n",
    "                    self.update_networks()\n",
    "\n",
    "                if is_terminal:\n",
    "                    gc.collect()\n",
    "                    break\n",
    "            \n",
    "            # stats\n",
    "            episode_elapsed = time.time() - episode_start\n",
    "            self.episode_seconds.append(episode_elapsed)\n",
    "            training_time += episode_elapsed\n",
    "            evaluation_score, _ = self.evaluate(self.online_policy_model, env)\n",
    "            self.save_checkpoint(episode-1, self.online_policy_model)\n",
    "\n",
    "            total_step = int(np.sum(self.episode_timestep))\n",
    "            self.evaluation_scores.append(evaluation_score)\n",
    "            \n",
    "            mean_10_reward = np.mean(self.episode_reward[-10:])\n",
    "            std_10_reward = np.std(self.episode_reward[-10:])\n",
    "            mean_100_reward = np.mean(self.episode_reward[-100:])\n",
    "            std_100_reward = np.std(self.episode_reward[-100:])\n",
    "            mean_100_eval_score = np.mean(self.evaluation_scores[-100:])\n",
    "            std_100_eval_score = np.std(self.evaluation_scores[-100:])\n",
    "            lst_100_exp_rat = np.array(\n",
    "                self.episode_exploration[-100:])/np.array(self.episode_timestep[-100:])\n",
    "            mean_100_exp_rat = np.mean(lst_100_exp_rat)\n",
    "            std_100_exp_rat = np.std(lst_100_exp_rat)\n",
    "            \n",
    "            wallclock_elapsed = time.time() - training_start\n",
    "            result[episode-1] = total_step, mean_100_reward, \\\n",
    "                mean_100_eval_score, mean_100_exp_rat, training_time, wallclock_elapsed\n",
    "            \n",
    "            reached_debug_time = time.time() - last_debug_time >= LEAVE_PRINT_EVERY_N_SECS\n",
    "            reached_max_minutes = wallclock_elapsed >= max_minutes * 60\n",
    "            reached_max_episodes = episode >= max_episodes\n",
    "            reached_goal_mean_reward = mean_100_eval_score >= goal_mean_100_reward\n",
    "            training_is_over = reached_max_minutes or \\\n",
    "                               reached_max_episodes or \\\n",
    "                               reached_goal_mean_reward\n",
    "            elapsed_str = time.strftime(\"%H:%M:%S\", time.gmtime(time.time() - training_start))\n",
    "            debug_message = (\n",
    "                f'el {elapsed_str}, ep {episode-1:04}, ts {total_step:07}, '\n",
    "                f'mean_10_reward: {mean_10_reward:.2f}\\u00B1{std_10_reward:.2f}, '\n",
    "                f'mean_100_reward: {mean_100_reward:.2f}\\u00B1{std_100_reward:.2f}, '\n",
    "                f'mean_100_exp_rat: {mean_100_exp_rat:.2f}\\u00B1{std_100_exp_rat:.2f}, '\n",
    "                f'mean_100_eval_score: {mean_100_eval_score:.2f}\\u00B1{std_100_eval_score:.2f}'\n",
    "            )\n",
    "            print(debug_message, end='\\r', flush=True)\n",
    "            if reached_debug_time or training_is_over:\n",
    "                print(ERASE_LINE + debug_message, flush=True)\n",
    "                last_debug_time = time.time()\n",
    "            if training_is_over:\n",
    "                if reached_max_minutes: print(u'--> reached_max_minutes \\u2715')\n",
    "                if reached_max_episodes: print(u'--> reached_max_episodes \\u2715')\n",
    "                if reached_goal_mean_reward: print(u'--> reached_goal_mean_reward \\u2713')\n",
    "                break\n",
    "                \n",
    "        final_eval_score, score_std = self.evaluate(self.online_policy_model, env, n_episodes=100)\n",
    "        wallclock_time = time.time() - training_start\n",
    "        print('Training complete.')\n",
    "        print('Final evaluation score {:.2f}\\u00B1{:.2f} in {:.2f}min training time,'\n",
    "              ' {:.2f}min wall-clock time.\\n'.format(\n",
    "                  final_eval_score, score_std, training_time / 60, wallclock_time / 60))\n",
    "        env.close() ; del env\n",
    "        self.get_cleaned_checkpoints()\n",
    "        return result, final_eval_score, training_time, wallclock_time\n",
    "    \n",
    "    def evaluate(self, eval_policy_model, eval_env, n_episodes=1):\n",
    "        rs = []\n",
    "        for _ in range(n_episodes):\n",
    "            s, d = eval_env.reset(), False\n",
    "            rs.append(0)\n",
    "            for _ in count():\n",
    "                a = self.evaluation_strategy.select_action(eval_policy_model, s)\n",
    "                s, r, d, _ = eval_env.step(a)\n",
    "                rs[-1] += r\n",
    "                if eval_env.do_render: eval_env.render()\n",
    "                if d: break\n",
    "        return np.mean(rs), np.std(rs)\n",
    "\n",
    "    def get_cleaned_checkpoints(self, n_checkpoints=4):\n",
    "        try: \n",
    "            return self.checkpoint_paths\n",
    "        except AttributeError:\n",
    "            self.checkpoint_paths = {}\n",
    "\n",
    "        paths = glob.glob(os.path.join(self.checkpoint_dir, '*.tar'))\n",
    "        paths_dic = {int(path.split('.')[-2]):path for path in paths}\n",
    "        last_ep = max(paths_dic.keys())\n",
    "        # checkpoint_idxs = np.geomspace(1, last_ep+1, n_checkpoints, endpoint=True, dtype=np.int)-1\n",
    "        checkpoint_idxs = np.linspace(1, last_ep+1, n_checkpoints, endpoint=True, dtype=int)-1\n",
    "\n",
    "        for idx, path in paths_dic.items():\n",
    "            if idx in checkpoint_idxs:\n",
    "                self.checkpoint_paths[idx] = path\n",
    "            else:\n",
    "                os.unlink(path)\n",
    "\n",
    "        return self.checkpoint_paths\n",
    "\n",
    "    def demo_last(self, title='Fully-trained {} Agent', n_episodes=2, max_n_videos=2):\n",
    "\n",
    "        checkpoint_paths = self.get_cleaned_checkpoints()\n",
    "        last_ep = max(checkpoint_paths.keys())\n",
    "        self.online_policy_model.load_state_dict(torch.load(checkpoint_paths[last_ep]))\n",
    "\n",
    "        for i in range(n_episodes):\n",
    "            env = self.make_env_fn(**self.make_env_kargs, monitor_mode='evaluation', render=True, record=True)\n",
    "            self.evaluate(self.online_policy_model, env, n_episodes=1)\n",
    "            env.close()\n",
    "            data = get_gif_html(env.videos[0], \n",
    "                                title.format(self.__class__.__name__),\n",
    "                                i)\n",
    "        del env\n",
    "        return HTML(data=data)\n",
    "\n",
    "    def demo_progression(self, title='{} Agent progression', max_n_videos=4):\n",
    "        env = self.make_env_fn(**self.make_env_kargs, monitor_mode='evaluation', render=True, record=True)\n",
    "\n",
    "        checkpoint_paths = self.get_cleaned_checkpoints()\n",
    "        for i in sorted(checkpoint_paths.keys()):\n",
    "            self.online_policy_model.load_state_dict(torch.load(checkpoint_paths[i]))\n",
    "            self.evaluate(self.online_policy_model, env, n_episodes=1)\n",
    "\n",
    "        env.close()\n",
    "        data = get_gif_html(env_videos=env.videos, \n",
    "                            title=title.format(self.__class__.__name__),\n",
    "                            subtitle_eps=sorted(checkpoint_paths.keys()),\n",
    "                            max_n_videos=max_n_videos)\n",
    "        del env\n",
    "        return HTML(data=data)\n",
    "\n",
    "    def save_checkpoint(self, episode_idx, model):\n",
    "        torch.save(model.state_dict(), \n",
    "                   os.path.join(self.checkpoint_dir, 'model.{}.tar'.format(episode_idx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ddpg_results = []\n",
    "best_agent, best_eval_score = None, float('-inf')\n",
    "for seed in SEEDS:\n",
    "    environment_settings = {\n",
    "        # 'env_name': 'BeadsCartPole',\n",
    "        'env_name': 'BeadsQuadCopterEnvironment',\n",
    "        'gamma': 0.99, # 0.99,\n",
    "        'max_minutes': 180,\n",
    "        'max_episodes': 1000,\n",
    "        'goal_mean_100_reward': 140000.0,\n",
    "    }\n",
    "\n",
    "    policy_model_fn = lambda nS, bounds: FCDP(nS, bounds, hidden_dims=(256,256))\n",
    "    policy_max_grad_norm = float('inf')\n",
    "    policy_optimizer_fn = lambda net, lr: optim.Adam(net.parameters(), lr=lr)\n",
    "    policy_optimizer_lr = 0.0003\n",
    "\n",
    "    value_model_fn = lambda nS, nA: FCQV(nS, nA, hidden_dims=(256,256))\n",
    "    value_max_grad_norm = float('inf')\n",
    "    value_optimizer_fn = lambda net, lr: optim.Adam(net.parameters(), lr=lr)\n",
    "    value_optimizer_lr = 0.0003\n",
    "\n",
    "    # training_strategy_fn = lambda bounds: NormalNoiseStrategy(bounds, exploration_noise_ratio=0.1, exploration_noise_amplitude=1.0)\n",
    "    training_strategy_fn = lambda bounds: NormalNoiseDecayStrategy(bounds,\n",
    "                                                                   init_noise_ratio_mult=0.1,\n",
    "                                                                   min_noise_ratio_mult=0.01,\n",
    "                                                                   init_noise_ratio_add=1.5,\n",
    "                                                                   min_noise_ratio_add=0.01,\n",
    "                                                                   decay_steps=800_000)\n",
    "    # training_strategy_fn = lambda bounds: NormalNoiseStrategy(\n",
    "    #     bounds,\n",
    "    #     exploration_noise_ratio=0.1,\n",
    "    #     exploration_noise_amplitude=0.2,\n",
    "    #     ou_process=True,    \n",
    "    # )\n",
    "    evaluation_strategy_fn = lambda bounds: GreedyStrategy(bounds)\n",
    "\n",
    "    replay_buffer_fn = lambda: ReplayBuffer(max_size=1_000_000, batch_size=256)\n",
    "    n_warmup_batches = 5\n",
    "    update_target_every_steps = 2\n",
    "    tau = 0.005\n",
    "    \n",
    "    env_name, gamma, max_minutes, \\\n",
    "    max_episodes, goal_mean_100_reward = environment_settings.values()\n",
    "\n",
    "    agent = DDPG(replay_buffer_fn,\n",
    "                 policy_model_fn, \n",
    "                 policy_max_grad_norm, \n",
    "                 policy_optimizer_fn, \n",
    "                 policy_optimizer_lr,\n",
    "                 value_model_fn, \n",
    "                 value_max_grad_norm, \n",
    "                 value_optimizer_fn, \n",
    "                 value_optimizer_lr, \n",
    "                 training_strategy_fn,\n",
    "                 evaluation_strategy_fn,\n",
    "                 n_warmup_batches,\n",
    "                 update_target_every_steps,\n",
    "                 tau)\n",
    "\n",
    "    make_env_fn, make_env_kargs = get_make_env_fn(env_name=env_name)\n",
    "    result, final_eval_score, training_time, wallclock_time = agent.train(\n",
    "        make_env_fn, make_env_kargs, seed, gamma, max_minutes, max_episodes, goal_mean_100_reward)\n",
    "    ddpg_results.append(result)\n",
    "    if final_eval_score > best_eval_score:\n",
    "        best_eval_score = final_eval_score\n",
    "        print('best_agent assigned!\\n')\n",
    "        best_agent = agent\n",
    "        \n",
    "ddpg_results = np.array(ddpg_results)\n",
    "_ = BEEP()\n",
    "\n",
    "del best_agent.replay_buffer\n",
    "print(gc.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_max_t, ddpg_max_r, ddpg_max_s, ddpg_max_exp, \\\n",
    "ddpg_max_sec, ddpg_max_rt = np.max(ddpg_results, axis=0).T\n",
    "ddpg_min_t, ddpg_min_r, ddpg_min_s, ddpg_min_exp, \\\n",
    "ddpg_min_sec, ddpg_min_rt = np.min(ddpg_results, axis=0).T\n",
    "ddpg_mean_t, ddpg_mean_r, ddpg_mean_s, ddpg_mean_exp, \\\n",
    "ddpg_mean_sec, ddpg_mean_rt = np.mean(ddpg_results, axis=0).T\n",
    "ddpg_x = np.arange(len(ddpg_mean_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 1, figsize=(15,10), sharey=False, sharex=True)\n",
    "\n",
    "# DDPG\n",
    "axs[0].plot(ddpg_max_r, 'r', linewidth=1)\n",
    "axs[0].plot(ddpg_min_r, 'r', linewidth=1)\n",
    "axs[0].plot(ddpg_mean_r, 'r:', label='DDPG', linewidth=2)\n",
    "axs[0].fill_between(\n",
    "    ddpg_x, ddpg_min_r, ddpg_max_r, facecolor='r', alpha=0.3)\n",
    "\n",
    "axs[1].plot(ddpg_max_s, 'r', linewidth=1)\n",
    "axs[1].plot(ddpg_min_s, 'r', linewidth=1)\n",
    "axs[1].plot(ddpg_mean_s, 'r:', label='DDPG', linewidth=2)\n",
    "axs[1].fill_between(\n",
    "    ddpg_x, ddpg_min_s, ddpg_max_s, facecolor='r', alpha=0.3)\n",
    "\n",
    "axs[2].plot(ddpg_max_exp, 'r', linewidth=1)\n",
    "axs[2].plot(ddpg_min_exp, 'r', linewidth=1)\n",
    "axs[2].plot(ddpg_mean_exp, 'r:', label='DDPG', linewidth=2)\n",
    "axs[2].fill_between(\n",
    "    ddpg_x, ddpg_min_exp, ddpg_max_exp, facecolor='r', alpha=0.3)\n",
    "\n",
    "# ALL\n",
    "axs[0].set_title('Moving Avg Reward (Training)')\n",
    "axs[1].set_title('Moving Avg Reward (Evaluation)')\n",
    "axs[2].set_title('Moving Noise')\n",
    "plt.xlabel('Episodes')\n",
    "axs[0].legend(loc='upper left')\n",
    "plt.savefig(\"progress.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del ddpg_results\n",
    "print(gc.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_agent.demo_last(title=\"last\")\n",
    "print('done')\n",
    "# best_agent.demo_progression(title=\"progress\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 1, figsize=(15,15), sharey=False, sharex=True)\n",
    "\n",
    "# DDPG\n",
    "axs[0].plot(ddpg_max_t, 'r', linewidth=1)\n",
    "axs[0].plot(ddpg_min_t, 'r', linewidth=1)\n",
    "axs[0].plot(ddpg_mean_t, 'r:', label='DDPG', linewidth=2)\n",
    "axs[0].fill_between(\n",
    "    ddpg_x, ddpg_min_t, ddpg_max_t, facecolor='r', alpha=0.3)\n",
    "\n",
    "axs[1].plot(ddpg_max_sec, 'r', linewidth=1)\n",
    "axs[1].plot(ddpg_min_sec, 'r', linewidth=1)\n",
    "axs[1].plot(ddpg_mean_sec, 'r:', label='DDPG', linewidth=2)\n",
    "axs[1].fill_between(\n",
    "    ddpg_x, ddpg_min_sec, ddpg_max_sec, facecolor='r', alpha=0.3)\n",
    "\n",
    "axs[2].plot(ddpg_max_rt, 'r', linewidth=1)\n",
    "axs[2].plot(ddpg_min_rt, 'r', linewidth=1)\n",
    "axs[2].plot(ddpg_mean_rt, 'r:', label='DDPG', linewidth=2)\n",
    "axs[2].fill_between(\n",
    "    ddpg_x, ddpg_min_rt, ddpg_max_rt, facecolor='r', alpha=0.3)\n",
    "\n",
    "# ALL\n",
    "axs[0].set_title('Total Steps')\n",
    "axs[1].set_title('Training Time')\n",
    "axs[2].set_title('Wall-clock Time')\n",
    "plt.xlabel('Episodes')\n",
    "axs[0].legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg_root_dir = os.path.join(RESULTS_DIR, 'ddpg')\n",
    "not os.path.exists(ddpg_root_dir) and os.makedirs(ddpg_root_dir)\n",
    "\n",
    "np.save(os.path.join(ddpg_root_dir, 'x'), ddpg_x)\n",
    "\n",
    "np.save(os.path.join(ddpg_root_dir, 'max_r'), ddpg_max_r)\n",
    "np.save(os.path.join(ddpg_root_dir, 'min_r'), ddpg_min_r)\n",
    "np.save(os.path.join(ddpg_root_dir, 'mean_r'), ddpg_mean_r)\n",
    "\n",
    "np.save(os.path.join(ddpg_root_dir, 'max_s'), ddpg_max_s)\n",
    "np.save(os.path.join(ddpg_root_dir, 'min_s'), ddpg_min_s )\n",
    "np.save(os.path.join(ddpg_root_dir, 'mean_s'), ddpg_mean_s)\n",
    "\n",
    "np.save(os.path.join(ddpg_root_dir, 'max_t'), ddpg_max_t)\n",
    "np.save(os.path.join(ddpg_root_dir, 'min_t'), ddpg_min_t)\n",
    "np.save(os.path.join(ddpg_root_dir, 'mean_t'), ddpg_mean_t)\n",
    "\n",
    "np.save(os.path.join(ddpg_root_dir, 'max_sec'), ddpg_max_sec)\n",
    "np.save(os.path.join(ddpg_root_dir, 'min_sec'), ddpg_min_sec)\n",
    "np.save(os.path.join(ddpg_root_dir, 'mean_sec'), ddpg_mean_sec)\n",
    "\n",
    "np.save(os.path.join(ddpg_root_dir, 'max_rt'), ddpg_max_rt)\n",
    "np.save(os.path.join(ddpg_root_dir, 'min_rt'), ddpg_min_rt)\n",
    "np.save(os.path.join(ddpg_root_dir, 'mean_rt'), ddpg_mean_rt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TD3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalNoiseDecayStrategy():\n",
    "    def __init__(self, bounds, init_noise_ratio=0.5, min_noise_ratio=0.1, decay_steps=10000):\n",
    "        self.t = 0\n",
    "        self.low, self.high = bounds\n",
    "        self.noise_ratio = init_noise_ratio\n",
    "        self.init_noise_ratio = init_noise_ratio\n",
    "        self.min_noise_ratio = min_noise_ratio\n",
    "        self.decay_steps = decay_steps\n",
    "        self.ratio_noise_injected = 0\n",
    "\n",
    "    def _noise_ratio_update(self):\n",
    "        noise_ratio = 1 - self.t / self.decay_steps\n",
    "        noise_ratio = (self.init_noise_ratio - self.min_noise_ratio) * noise_ratio + self.min_noise_ratio\n",
    "        noise_ratio = np.clip(noise_ratio, self.min_noise_ratio, self.init_noise_ratio)\n",
    "        self.t += 1\n",
    "        return noise_ratio\n",
    "\n",
    "    def select_action(self, model, state, max_exploration=False):\n",
    "        if max_exploration:\n",
    "            noise_scale = self.high\n",
    "        else:\n",
    "            noise_scale = self.noise_ratio * self.high\n",
    "\n",
    "        with torch.no_grad():\n",
    "            greedy_action = model(state).cpu().detach().data.numpy().squeeze()\n",
    "\n",
    "        noise = np.random.normal(loc=0, scale=noise_scale, size=len(self.high))\n",
    "        noisy_action = greedy_action + noise\n",
    "        action = np.clip(noisy_action, self.low, self.high)\n",
    "\n",
    "        self.noise_ratio = self._noise_ratio_update()\n",
    "        self.ratio_noise_injected = np.mean(abs((greedy_action - action)/(self.high - self.low)))\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = NormalNoiseDecayStrategy(([-2],[2]))\n",
    "plt.plot([s._noise_ratio_update() for _ in range(50000)])\n",
    "plt.title('Normal Noise Linear ratio')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCTQV(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim, \n",
    "                 output_dim, \n",
    "                 hidden_dims=(32,32), \n",
    "                 activation_fc=F.relu):\n",
    "        super(FCTQV, self).__init__()\n",
    "        self.activation_fc = activation_fc\n",
    "\n",
    "        self.input_layer_a = nn.Linear(input_dim + output_dim, hidden_dims[0])\n",
    "        self.input_layer_b = nn.Linear(input_dim + output_dim, hidden_dims[0])\n",
    "\n",
    "        self.hidden_layers_a = nn.ModuleList()\n",
    "        self.hidden_layers_b = nn.ModuleList()\n",
    "        for i in range(len(hidden_dims)-1):\n",
    "            hidden_layer_a = nn.Linear(hidden_dims[i], hidden_dims[i+1])\n",
    "            self.hidden_layers_a.append(hidden_layer_a)\n",
    "\n",
    "            hidden_layer_b = nn.Linear(hidden_dims[i], hidden_dims[i+1])\n",
    "            self.hidden_layers_b.append(hidden_layer_b)\n",
    "\n",
    "        self.output_layer_a = nn.Linear(hidden_dims[-1], 1)\n",
    "        self.output_layer_b = nn.Linear(hidden_dims[-1], 1)\n",
    "\n",
    "        device = \"cpu\"\n",
    "        if torch.cuda.is_available():\n",
    "            device = \"cuda:0\"\n",
    "        self.device = torch.device(device)\n",
    "        self.to(self.device)\n",
    "\n",
    "    def _format(self, state, action):\n",
    "        x, u = state, action\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.tensor(x, \n",
    "                             device=self.device, \n",
    "                             dtype=torch.float32)\n",
    "            x = x.unsqueeze(0)\n",
    "        if not isinstance(u, torch.Tensor):\n",
    "            u = torch.tensor(u, \n",
    "                             device=self.device, \n",
    "                             dtype=torch.float32)\n",
    "            u = u.unsqueeze(0)\n",
    "        return x, u\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x, u = self._format(state, action)\n",
    "        x = torch.cat((x, u), dim=1)\n",
    "        xa = self.activation_fc(self.input_layer_a(x))\n",
    "        xb = self.activation_fc(self.input_layer_b(x))\n",
    "        for hidden_layer_a, hidden_layer_b in zip(self.hidden_layers_a, self.hidden_layers_b):\n",
    "            xa = self.activation_fc(hidden_layer_a(xa))\n",
    "            xb = self.activation_fc(hidden_layer_b(xb))\n",
    "        xa = self.output_layer_a(xa)\n",
    "        xb = self.output_layer_b(xb)\n",
    "        return xa, xb\n",
    "    \n",
    "    def Qa(self, state, action):\n",
    "        x, u = self._format(state, action)\n",
    "        x = torch.cat((x, u), dim=1)\n",
    "        xa = self.activation_fc(self.input_layer_a(x))\n",
    "        for hidden_layer_a in self.hidden_layers_a:\n",
    "            xa = self.activation_fc(hidden_layer_a(xa))\n",
    "        return self.output_layer_a(xa)\n",
    "    \n",
    "    def load(self, experiences):\n",
    "        states, actions, new_states, rewards, is_terminals = experiences\n",
    "        states = torch.from_numpy(states).float().to(self.device)\n",
    "        actions = torch.from_numpy(actions).float().to(self.device)\n",
    "        new_states = torch.from_numpy(new_states).float().to(self.device)\n",
    "        rewards = torch.from_numpy(rewards).float().to(self.device)\n",
    "        is_terminals = torch.from_numpy(is_terminals).float().to(self.device)\n",
    "        return states, actions, new_states, rewards, is_terminals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TD3():\n",
    "    def __init__(self, \n",
    "                 replay_buffer_fn,\n",
    "                 policy_model_fn, \n",
    "                 policy_max_grad_norm, \n",
    "                 policy_optimizer_fn, \n",
    "                 policy_optimizer_lr,\n",
    "                 value_model_fn, \n",
    "                 value_max_grad_norm, \n",
    "                 value_optimizer_fn, \n",
    "                 value_optimizer_lr, \n",
    "                 training_strategy_fn,\n",
    "                 evaluation_strategy_fn,\n",
    "                 n_warmup_batches,\n",
    "                 update_value_target_every_steps,\n",
    "                 update_policy_target_every_steps,\n",
    "                 train_policy_every_steps,\n",
    "                 tau,\n",
    "                 policy_noise_ratio,\n",
    "                 policy_noise_clip_ratio):\n",
    "        self.replay_buffer_fn = replay_buffer_fn\n",
    "\n",
    "        self.policy_model_fn = policy_model_fn\n",
    "        self.policy_max_grad_norm = policy_max_grad_norm\n",
    "        self.policy_optimizer_fn = policy_optimizer_fn\n",
    "        self.policy_optimizer_lr = policy_optimizer_lr\n",
    "        \n",
    "        self.value_model_fn = value_model_fn\n",
    "        self.value_max_grad_norm = value_max_grad_norm\n",
    "        self.value_optimizer_fn = value_optimizer_fn\n",
    "        self.value_optimizer_lr = value_optimizer_lr\n",
    "\n",
    "        self.training_strategy_fn = training_strategy_fn\n",
    "        self.evaluation_strategy_fn = evaluation_strategy_fn\n",
    "\n",
    "        self.n_warmup_batches = n_warmup_batches\n",
    "        self.update_value_target_every_steps = update_value_target_every_steps\n",
    "        self.update_policy_target_every_steps = update_policy_target_every_steps\n",
    "        self.train_policy_every_steps = train_policy_every_steps\n",
    "        \n",
    "        self.tau = tau\n",
    "        self.policy_noise_ratio = policy_noise_ratio\n",
    "        self.policy_noise_clip_ratio = policy_noise_clip_ratio\n",
    "\n",
    "    def optimize_model(self, experiences):\n",
    "        states, actions, rewards, next_states, is_terminals = experiences\n",
    "        batch_size = len(is_terminals)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            a_ran = self.target_policy_model.env_max - self.target_policy_model.env_min\n",
    "            a_noise = torch.randn_like(actions) * self.policy_noise_ratio * a_ran\n",
    "            n_min = self.target_policy_model.env_min * self.policy_noise_clip_ratio\n",
    "            n_max = self.target_policy_model.env_max * self.policy_noise_clip_ratio            \n",
    "            a_noise = torch.max(torch.min(a_noise, n_max), n_min)\n",
    "\n",
    "            argmax_a_q_sp = self.target_policy_model(next_states)\n",
    "            noisy_argmax_a_q_sp = argmax_a_q_sp + a_noise\n",
    "            noisy_argmax_a_q_sp = torch.max(torch.min(noisy_argmax_a_q_sp, \n",
    "                                                      self.target_policy_model.env_max),\n",
    "                                            self.target_policy_model.env_min)\n",
    "\n",
    "            max_a_q_sp_a, max_a_q_sp_b = self.target_value_model(next_states, noisy_argmax_a_q_sp)\n",
    "            max_a_q_sp = torch.min(max_a_q_sp_a, max_a_q_sp_b)\n",
    "\n",
    "            target_q_sa = rewards + self.gamma * max_a_q_sp * (1 - is_terminals)\n",
    "\n",
    "        q_sa_a, q_sa_b = self.online_value_model(states, actions)\n",
    "        td_error_a = q_sa_a - target_q_sa\n",
    "        td_error_b = q_sa_b - target_q_sa\n",
    "\n",
    "        value_loss = td_error_a.pow(2).mul(0.5).mean() + td_error_b.pow(2).mul(0.5).mean()\n",
    "        self.value_optimizer.zero_grad()\n",
    "        value_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.online_value_model.parameters(), \n",
    "                                       self.value_max_grad_norm)\n",
    "        self.value_optimizer.step()\n",
    "\n",
    "        if np.sum(self.episode_timestep) % self.train_policy_every_steps == 0:\n",
    "            argmax_a_q_s = self.online_policy_model(states)\n",
    "            max_a_q_s = self.online_value_model.Qa(states, argmax_a_q_s)\n",
    "\n",
    "            policy_loss = -max_a_q_s.mean()\n",
    "            self.policy_optimizer.zero_grad()\n",
    "            policy_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.online_policy_model.parameters(), \n",
    "                                           self.policy_max_grad_norm)        \n",
    "            self.policy_optimizer.step()\n",
    "\n",
    "    def interaction_step(self, state, env):\n",
    "        min_samples = self.replay_buffer.batch_size * self.n_warmup_batches\n",
    "        action = self.training_strategy.select_action(self.online_policy_model, \n",
    "                                                      state, \n",
    "                                                      len(self.replay_buffer) < min_samples)\n",
    "        new_state, reward, is_terminal, info = env.step(action)\n",
    "        is_truncated = 'TimeLimit.truncated' in info and info['TimeLimit.truncated']\n",
    "        is_failure = is_terminal and not is_truncated\n",
    "        experience = (state, action, reward, new_state, float(is_failure))\n",
    "\n",
    "        self.replay_buffer.store(experience)\n",
    "        self.episode_reward[-1] += reward\n",
    "        self.episode_timestep[-1] += 1\n",
    "        self.episode_exploration[-1] += self.training_strategy.ratio_noise_injected\n",
    "        return new_state, is_terminal\n",
    "\n",
    "    def update_value_network(self, tau=None):\n",
    "        tau = self.tau if tau is None else tau\n",
    "        for target, online in zip(self.target_value_model.parameters(), \n",
    "                                  self.online_value_model.parameters()):\n",
    "            target_ratio = (1.0 - tau) * target.data\n",
    "            online_ratio = tau * online.data\n",
    "            mixed_weights = target_ratio + online_ratio\n",
    "            target.data.copy_(mixed_weights)\n",
    "\n",
    "    def update_policy_network(self, tau=None):\n",
    "        tau = self.tau if tau is None else tau\n",
    "        for target, online in zip(self.target_policy_model.parameters(), \n",
    "                                  self.online_policy_model.parameters()):\n",
    "            target_ratio = (1.0 - tau) * target.data\n",
    "            online_ratio = tau * online.data\n",
    "            mixed_weights = target_ratio + online_ratio\n",
    "            target.data.copy_(mixed_weights)\n",
    "\n",
    "    def train(self, make_env_fn, make_env_kargs, seed, gamma, \n",
    "              max_minutes, max_episodes, goal_mean_100_reward):\n",
    "        training_start, last_debug_time = time.time(), float('-inf')\n",
    "\n",
    "        self.checkpoint_dir = tempfile.mkdtemp()\n",
    "        self.make_env_fn = make_env_fn\n",
    "        self.make_env_kargs = make_env_kargs\n",
    "        self.seed = seed\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        env = self.make_env_fn(**self.make_env_kargs, seed=self.seed)\n",
    "        torch.manual_seed(self.seed) ; np.random.seed(self.seed) ; random.seed(self.seed)\n",
    "    \n",
    "        nS, nA = env.observation_space.shape[0], env.action_space.shape[0]\n",
    "        action_bounds = env.action_space.low, env.action_space.high\n",
    "        self.episode_timestep = []\n",
    "        self.episode_reward = []\n",
    "        self.episode_seconds = []\n",
    "        self.evaluation_scores = []        \n",
    "        self.episode_exploration = []\n",
    "        \n",
    "        self.target_value_model = self.value_model_fn(nS, nA)\n",
    "        self.online_value_model = self.value_model_fn(nS, nA)\n",
    "        self.update_value_network(tau=1.0)\n",
    "\n",
    "        self.target_policy_model = self.policy_model_fn(nS, action_bounds)\n",
    "        self.online_policy_model = self.policy_model_fn(nS, action_bounds)\n",
    "        self.update_policy_network(tau=1.0)\n",
    "\n",
    "        self.value_optimizer = self.value_optimizer_fn(self.online_value_model, \n",
    "                                                       self.value_optimizer_lr)        \n",
    "        self.policy_optimizer = self.policy_optimizer_fn(self.online_policy_model, \n",
    "                                                         self.policy_optimizer_lr)\n",
    "\n",
    "        self.replay_buffer = self.replay_buffer_fn()\n",
    "        self.training_strategy = training_strategy_fn(action_bounds)\n",
    "        self.evaluation_strategy = evaluation_strategy_fn(action_bounds)\n",
    "                    \n",
    "        result = np.empty((max_episodes, 5))\n",
    "        result[:] = np.nan\n",
    "        training_time = 0\n",
    "        for episode in range(1, max_episodes + 1):\n",
    "            episode_start = time.time()\n",
    "            \n",
    "            state, is_terminal = env.reset(), False\n",
    "            self.episode_reward.append(0.0)\n",
    "            self.episode_timestep.append(0.0)\n",
    "            self.episode_exploration.append(0.0)\n",
    "\n",
    "            for step in count():\n",
    "                state, is_terminal = self.interaction_step(state, env)\n",
    "\n",
    "                min_samples = self.replay_buffer.batch_size * self.n_warmup_batches\n",
    "                if len(self.replay_buffer) > min_samples:\n",
    "                    experiences = self.replay_buffer.sample()\n",
    "                    experiences = self.online_value_model.load(experiences)\n",
    "                    self.optimize_model(experiences)\n",
    "\n",
    "                if np.sum(self.episode_timestep) % self.update_value_target_every_steps == 0:\n",
    "                    self.update_value_network()\n",
    "\n",
    "                if np.sum(self.episode_timestep) % self.update_policy_target_every_steps == 0:\n",
    "                    self.update_policy_network()\n",
    "\n",
    "                if is_terminal:\n",
    "                    gc.collect()\n",
    "                    break\n",
    "            \n",
    "            # stats\n",
    "            episode_elapsed = time.time() - episode_start\n",
    "            self.episode_seconds.append(episode_elapsed)\n",
    "            training_time += episode_elapsed\n",
    "            evaluation_score, _ = self.evaluate(self.online_policy_model, env)\n",
    "            self.save_checkpoint(episode-1, self.online_policy_model)\n",
    "\n",
    "            total_step = int(np.sum(self.episode_timestep))\n",
    "            self.evaluation_scores.append(evaluation_score)\n",
    "            \n",
    "            mean_10_reward = np.mean(self.episode_reward[-10:])\n",
    "            std_10_reward = np.std(self.episode_reward[-10:])\n",
    "            mean_100_reward = np.mean(self.episode_reward[-100:])\n",
    "            std_100_reward = np.std(self.episode_reward[-100:])\n",
    "            mean_100_eval_score = np.mean(self.evaluation_scores[-100:])\n",
    "            std_100_eval_score = np.std(self.evaluation_scores[-100:])\n",
    "            lst_100_exp_rat = np.array(\n",
    "                self.episode_exploration[-100:])/np.array(self.episode_timestep[-100:])\n",
    "            mean_100_exp_rat = np.mean(lst_100_exp_rat)\n",
    "            std_100_exp_rat = np.std(lst_100_exp_rat)\n",
    "            \n",
    "            wallclock_elapsed = time.time() - training_start\n",
    "            result[episode-1] = total_step, mean_100_reward, \\\n",
    "                mean_100_eval_score, training_time, wallclock_elapsed\n",
    "            \n",
    "            reached_debug_time = time.time() - last_debug_time >= LEAVE_PRINT_EVERY_N_SECS\n",
    "            reached_max_minutes = wallclock_elapsed >= max_minutes * 60\n",
    "            reached_max_episodes = episode >= max_episodes\n",
    "            reached_goal_mean_reward = mean_100_eval_score >= goal_mean_100_reward\n",
    "            training_is_over = reached_max_minutes or \\\n",
    "                               reached_max_episodes or \\\n",
    "                               reached_goal_mean_reward\n",
    "            elapsed_str = time.strftime(\"%H:%M:%S\", time.gmtime(time.time() - training_start))\n",
    "            debug_message = 'el {}, ep {:04}, ts {:07}, '\n",
    "            debug_message += 'ar 10 {:05.1f}\\u00B1{:05.1f}, '\n",
    "            debug_message += '100 {:05.1f}\\u00B1{:05.1f}, '\n",
    "            debug_message += 'ex 100 {:02.1f}\\u00B1{:02.1f}, '\n",
    "            debug_message += 'ev {:05.1f}\\u00B1{:05.1f}'\n",
    "            debug_message = debug_message.format(\n",
    "                elapsed_str, episode-1, total_step, mean_10_reward, std_10_reward, \n",
    "                mean_100_reward, std_100_reward, mean_100_exp_rat, std_100_exp_rat,\n",
    "                mean_100_eval_score, std_100_eval_score)\n",
    "            print(debug_message, end='\\r', flush=True)\n",
    "            if reached_debug_time or training_is_over:\n",
    "                print(ERASE_LINE + debug_message, flush=True)\n",
    "                last_debug_time = time.time()\n",
    "            if training_is_over:\n",
    "                if reached_max_minutes: print(u'--> reached_max_minutes \\u2715')\n",
    "                if reached_max_episodes: print(u'--> reached_max_episodes \\u2715')\n",
    "                if reached_goal_mean_reward: print(u'--> reached_goal_mean_reward \\u2713')\n",
    "                break\n",
    "                \n",
    "        final_eval_score, score_std = self.evaluate(self.online_policy_model, env, n_episodes=100)\n",
    "        wallclock_time = time.time() - training_start\n",
    "        print('Training complete.')\n",
    "        print('Final evaluation score {:.2f}\\u00B1{:.2f} in {:.2f}s training time,'\n",
    "              ' {:.2f}s wall-clock time.\\n'.format(\n",
    "                  final_eval_score, score_std, training_time, wallclock_time))\n",
    "        env.close() ; del env\n",
    "        self.get_cleaned_checkpoints()\n",
    "        return result, final_eval_score, training_time, wallclock_time\n",
    "    \n",
    "    def evaluate(self, eval_policy_model, eval_env, n_episodes=1):\n",
    "        rs = []\n",
    "        for _ in range(n_episodes):\n",
    "            s, d = eval_env.reset(), False\n",
    "            rs.append(0)\n",
    "            for _ in count():\n",
    "                a = self.evaluation_strategy.select_action(eval_policy_model, s)\n",
    "                s, r, d, _ = eval_env.step(a)\n",
    "                rs[-1] += r\n",
    "                if d: break\n",
    "        return np.mean(rs), np.std(rs)\n",
    "\n",
    "    def get_cleaned_checkpoints(self, n_checkpoints=4):\n",
    "        try: \n",
    "            return self.checkpoint_paths\n",
    "        except AttributeError:\n",
    "            self.checkpoint_paths = {}\n",
    "\n",
    "        paths = glob.glob(os.path.join(self.checkpoint_dir, '*.tar'))\n",
    "        paths_dic = {int(path.split('.')[-2]):path for path in paths}\n",
    "        last_ep = max(paths_dic.keys())\n",
    "        # checkpoint_idxs = np.geomspace(1, last_ep+1, n_checkpoints, endpoint=True, dtype=np.int)-1\n",
    "        checkpoint_idxs = np.linspace(1, last_ep+1, n_checkpoints, endpoint=True, dtype=np.int)-1\n",
    "\n",
    "        for idx, path in paths_dic.items():\n",
    "            if idx in checkpoint_idxs:\n",
    "                self.checkpoint_paths[idx] = path\n",
    "            else:\n",
    "                os.unlink(path)\n",
    "\n",
    "        return self.checkpoint_paths\n",
    "\n",
    "    def demo_last(self, title='Fully-trained {} Agent', n_episodes=2, max_n_videos=2):\n",
    "        env = self.make_env_fn(**self.make_env_kargs, monitor_mode='evaluation', render=True, record=True)\n",
    "\n",
    "        checkpoint_paths = self.get_cleaned_checkpoints()\n",
    "        last_ep = max(checkpoint_paths.keys())\n",
    "        self.online_policy_model.load_state_dict(torch.load(checkpoint_paths[last_ep]))\n",
    "\n",
    "        self.evaluate(self.online_policy_model, env, n_episodes=n_episodes)\n",
    "        env.close()\n",
    "        data = get_gif_html(env_videos=env.videos, \n",
    "                            title=title.format(self.__class__.__name__),\n",
    "                            max_n_videos=max_n_videos)\n",
    "        del env\n",
    "        return HTML(data=data)\n",
    "\n",
    "    def demo_progression(self, title='{} Agent progression', max_n_videos=4):\n",
    "        env = self.make_env_fn(**self.make_env_kargs, monitor_mode='evaluation', render=True, record=True)\n",
    "\n",
    "        checkpoint_paths = self.get_cleaned_checkpoints()\n",
    "        for i in sorted(checkpoint_paths.keys()):\n",
    "            self.online_policy_model.load_state_dict(torch.load(checkpoint_paths[i]))\n",
    "            self.evaluate(self.online_policy_model, env, n_episodes=1)\n",
    "\n",
    "        env.close()\n",
    "        data = get_gif_html(env_videos=env.videos, \n",
    "                            title=title.format(self.__class__.__name__),\n",
    "                            subtitle_eps=sorted(checkpoint_paths.keys()),\n",
    "                            max_n_videos=max_n_videos)\n",
    "        del env\n",
    "        return HTML(data=data)\n",
    "\n",
    "    def save_checkpoint(self, episode_idx, model):\n",
    "        torch.save(model.state_dict(), \n",
    "                   os.path.join(self.checkpoint_dir, 'model.{}.tar'.format(episode_idx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "td3_results = []\n",
    "best_agent, best_eval_score = None, float('-inf')\n",
    "for seed in SEEDS:\n",
    "    environment_settings = {\n",
    "        'env_name': 'HopperBulletEnv-v0',\n",
    "        'gamma': 0.99,\n",
    "        'max_minutes': 300,\n",
    "        'max_episodes': 10000,\n",
    "        'goal_mean_100_reward': 1500\n",
    "    }\n",
    "    \n",
    "    policy_model_fn = lambda nS, bounds: FCDP(nS, bounds, hidden_dims=(256,256))\n",
    "    policy_max_grad_norm = float('inf')\n",
    "    policy_optimizer_fn = lambda net, lr: optim.Adam(net.parameters(), lr=lr)\n",
    "    policy_optimizer_lr = 0.0003\n",
    "\n",
    "    value_model_fn = lambda nS, nA: FCTQV(nS, nA, hidden_dims=(256,256))\n",
    "    value_max_grad_norm = float('inf')\n",
    "    value_optimizer_fn = lambda net, lr: optim.Adam(net.parameters(), lr=lr)\n",
    "    value_optimizer_lr = 0.0003\n",
    "\n",
    "    training_strategy_fn = lambda bounds: NormalNoiseDecayStrategy(bounds,\n",
    "                                                                   init_noise_ratio=0.5,\n",
    "                                                                   min_noise_ratio=0.1,\n",
    "                                                                   decay_steps=200000)\n",
    "    evaluation_strategy_fn = lambda bounds: GreedyStrategy(bounds)\n",
    "\n",
    "    replay_buffer_fn = lambda: ReplayBuffer(max_size=1000000, batch_size=256)\n",
    "    n_warmup_batches = 5\n",
    "    update_value_target_every_steps = 2\n",
    "    update_policy_target_every_steps = 2\n",
    "    train_policy_every_steps = 2\n",
    "    policy_noise_ratio = 0.1\n",
    "    policy_noise_clip_ratio = 0.5\n",
    "    tau = 0.01\n",
    "\n",
    "    env_name, gamma, max_minutes, \\\n",
    "    max_episodes, goal_mean_100_reward = environment_settings.values()\n",
    "\n",
    "    agent = TD3(replay_buffer_fn,\n",
    "                policy_model_fn, \n",
    "                policy_max_grad_norm, \n",
    "                policy_optimizer_fn, \n",
    "                policy_optimizer_lr,\n",
    "                value_model_fn,\n",
    "                value_max_grad_norm, \n",
    "                value_optimizer_fn, \n",
    "                value_optimizer_lr, \n",
    "                training_strategy_fn,\n",
    "                evaluation_strategy_fn,\n",
    "                n_warmup_batches,\n",
    "                update_value_target_every_steps,\n",
    "                update_policy_target_every_steps,\n",
    "                train_policy_every_steps,\n",
    "                tau,\n",
    "                policy_noise_ratio,\n",
    "                policy_noise_clip_ratio)\n",
    "\n",
    "    make_env_fn, make_env_kargs = get_make_env_fn(env_name=env_name, inner_wrappers=[RenderUint8])\n",
    "    result, final_eval_score, training_time, wallclock_time = agent.train(\n",
    "        make_env_fn, make_env_kargs, seed, gamma, max_minutes, max_episodes, goal_mean_100_reward)\n",
    "    td3_results.append(result)\n",
    "    if final_eval_score > best_eval_score:\n",
    "        best_eval_score = final_eval_score\n",
    "        best_agent = agent\n",
    "td3_results = np.array(td3_results)\n",
    "_ = BEEP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "best_agent.demo_progression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_agent.demo_last()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td3_max_t, td3_max_r, td3_max_s, \\\n",
    "td3_max_sec, td3_max_rt = np.max(td3_results, axis=0).T\n",
    "td3_min_t, td3_min_r, td3_min_s, \\\n",
    "td3_min_sec, td3_min_rt = np.min(td3_results, axis=0).T\n",
    "td3_mean_t, td3_mean_r, td3_mean_s, \\\n",
    "td3_mean_sec, td3_mean_rt = np.mean(td3_results, axis=0).T\n",
    "td3_x = np.arange(len(td3_mean_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 1, figsize=(15,10), sharey=False, sharex=True)\n",
    "\n",
    "# TD3\n",
    "axs[0].plot(td3_max_r, 'b', linewidth=1)\n",
    "axs[0].plot(td3_min_r, 'b', linewidth=1)\n",
    "axs[0].plot(td3_mean_r, 'b:', label='TD3', linewidth=2)\n",
    "axs[0].fill_between(\n",
    "    td3_x, td3_min_r, td3_max_r, facecolor='b', alpha=0.3)\n",
    "\n",
    "axs[1].plot(td3_max_s, 'b', linewidth=1)\n",
    "axs[1].plot(td3_min_s, 'b', linewidth=1)\n",
    "axs[1].plot(td3_mean_s, 'b:', label='TD3', linewidth=2)\n",
    "axs[1].fill_between(\n",
    "    td3_x, td3_min_s, td3_max_s, facecolor='b', alpha=0.3)\n",
    "\n",
    "# ALL\n",
    "axs[0].set_title('Moving Avg Reward (Training)')\n",
    "axs[1].set_title('Moving Avg Reward (Evaluation)')\n",
    "plt.xlabel('Episodes')\n",
    "axs[0].legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 1, figsize=(15,15), sharey=False, sharex=True)\n",
    "\n",
    "# TD3\n",
    "axs[0].plot(td3_max_t, 'b', linewidth=1)\n",
    "axs[0].plot(td3_min_t, 'b', linewidth=1)\n",
    "axs[0].plot(td3_mean_t, 'b:', label='TD3', linewidth=2)\n",
    "axs[0].fill_between(\n",
    "    td3_x, td3_min_t, td3_max_t, facecolor='b', alpha=0.3)\n",
    "\n",
    "axs[1].plot(td3_max_sec, 'b', linewidth=1)\n",
    "axs[1].plot(td3_min_sec, 'b', linewidth=1)\n",
    "axs[1].plot(td3_mean_sec, 'b:', label='TD3', linewidth=2)\n",
    "axs[1].fill_between(\n",
    "    td3_x, td3_min_sec, td3_max_sec, facecolor='b', alpha=0.3)\n",
    "\n",
    "axs[2].plot(td3_max_rt, 'b', linewidth=1)\n",
    "axs[2].plot(td3_min_rt, 'b', linewidth=1)\n",
    "axs[2].plot(td3_mean_rt, 'b:', label='TD3', linewidth=2)\n",
    "axs[2].fill_between(\n",
    "    td3_x, td3_min_rt, td3_max_rt, facecolor='b', alpha=0.3)\n",
    "\n",
    "# ALL\n",
    "axs[0].set_title('Total Steps')\n",
    "axs[1].set_title('Training Time')\n",
    "axs[2].set_title('Wall-clock Time')\n",
    "plt.xlabel('Episodes')\n",
    "axs[0].legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td3_root_dir = os.path.join(RESULTS_DIR, 'td3')\n",
    "not os.path.exists(td3_root_dir) and os.makedirs(td3_root_dir)\n",
    "\n",
    "np.save(os.path.join(td3_root_dir, 'x'), td3_x)\n",
    "\n",
    "np.save(os.path.join(td3_root_dir, 'max_r'), td3_max_r)\n",
    "np.save(os.path.join(td3_root_dir, 'min_r'), td3_min_r)\n",
    "np.save(os.path.join(td3_root_dir, 'mean_r'), td3_mean_r)\n",
    "\n",
    "np.save(os.path.join(td3_root_dir, 'max_s'), td3_max_s)\n",
    "np.save(os.path.join(td3_root_dir, 'min_s'), td3_min_s )\n",
    "np.save(os.path.join(td3_root_dir, 'mean_s'), td3_mean_s)\n",
    "\n",
    "np.save(os.path.join(td3_root_dir, 'max_t'), td3_max_t)\n",
    "np.save(os.path.join(td3_root_dir, 'min_t'), td3_min_t)\n",
    "np.save(os.path.join(td3_root_dir, 'mean_t'), td3_mean_t)\n",
    "\n",
    "np.save(os.path.join(td3_root_dir, 'max_sec'), td3_max_sec)\n",
    "np.save(os.path.join(td3_root_dir, 'min_sec'), td3_min_sec)\n",
    "np.save(os.path.join(td3_root_dir, 'mean_sec'), td3_mean_sec)\n",
    "\n",
    "np.save(os.path.join(td3_root_dir, 'max_rt'), td3_max_rt)\n",
    "np.save(os.path.join(td3_root_dir, 'min_rt'), td3_min_rt)\n",
    "np.save(os.path.join(td3_root_dir, 'mean_rt'), td3_mean_rt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCQSA(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim, \n",
    "                 output_dim, \n",
    "                 hidden_dims=(32,32), \n",
    "                 activation_fc=F.relu):\n",
    "        super(FCQSA, self).__init__()\n",
    "        self.activation_fc = activation_fc\n",
    "\n",
    "        self.input_layer = nn.Linear(input_dim + output_dim, hidden_dims[0])\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for i in range(len(hidden_dims)-1):\n",
    "            hidden_layer = nn.Linear(hidden_dims[i], hidden_dims[i+1])\n",
    "            self.hidden_layers.append(hidden_layer)\n",
    "        self.output_layer = nn.Linear(hidden_dims[-1], 1)\n",
    "\n",
    "        device = \"cpu\"\n",
    "        if torch.cuda.is_available():\n",
    "            device = \"cuda:0\"\n",
    "        self.device = torch.device(device)\n",
    "        self.to(self.device)\n",
    "\n",
    "    def _format(self, state, action):\n",
    "        x, u = state, action\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.tensor(x, \n",
    "                             device=self.device, \n",
    "                             dtype=torch.float32)\n",
    "            x = x.unsqueeze(0)\n",
    "        if not isinstance(u, torch.Tensor):\n",
    "            u = torch.tensor(u, \n",
    "                             device=self.device, \n",
    "                             dtype=torch.float32)\n",
    "            u = u.unsqueeze(0)\n",
    "        return x, u\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x, u = self._format(state, action)\n",
    "        x = self.activation_fc(self.input_layer(torch.cat((x, u), dim=1)))\n",
    "        for i, hidden_layer in enumerate(self.hidden_layers):\n",
    "            x = self.activation_fc(hidden_layer(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "    \n",
    "    def load(self, experiences):\n",
    "        states, actions, new_states, rewards, is_terminals = experiences\n",
    "        states = torch.from_numpy(states).float().to(self.device)\n",
    "        actions = torch.from_numpy(actions).float().to(self.device)\n",
    "        new_states = torch.from_numpy(new_states).float().to(self.device)\n",
    "        rewards = torch.from_numpy(rewards).float().to(self.device)\n",
    "        is_terminals = torch.from_numpy(is_terminals).float().to(self.device)\n",
    "        return states, actions, new_states, rewards, is_terminals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCGP(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim, \n",
    "                 action_bounds,\n",
    "                 log_std_min=-20, \n",
    "                 log_std_max=2,\n",
    "                 hidden_dims=(32,32), \n",
    "                 activation_fc=F.relu,\n",
    "                 entropy_lr=0.001):\n",
    "        super(FCGP, self).__init__()\n",
    "        self.activation_fc = activation_fc\n",
    "        self.env_min, self.env_max = action_bounds\n",
    "        \n",
    "        self.log_std_min = log_std_min\n",
    "        self.log_std_max = log_std_max\n",
    "        \n",
    "        self.input_layer = nn.Linear(input_dim, \n",
    "                                     hidden_dims[0])\n",
    "        \n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for i in range(len(hidden_dims)-1):\n",
    "            hidden_layer = nn.Linear(\n",
    "                hidden_dims[i], hidden_dims[i+1])\n",
    "            self.hidden_layers.append(hidden_layer)\n",
    "\n",
    "        self.output_layer_mean = nn.Linear(hidden_dims[-1], len(self.env_max))\n",
    "        self.output_layer_log_std = nn.Linear(hidden_dims[-1], len(self.env_max))\n",
    "\n",
    "        device = \"cpu\"\n",
    "        if torch.cuda.is_available():\n",
    "            device = \"cuda:0\"\n",
    "        self.device = torch.device(device)\n",
    "        self.to(self.device)\n",
    "\n",
    "        self.env_min = torch.tensor(self.env_min,\n",
    "                                    device=self.device, \n",
    "                                    dtype=torch.float32)\n",
    "\n",
    "        self.env_max = torch.tensor(self.env_max,\n",
    "                                    device=self.device, \n",
    "                                    dtype=torch.float32)\n",
    "        \n",
    "        self.nn_min = F.tanh(torch.Tensor([float('-inf')])).to(self.device)\n",
    "        self.nn_max = F.tanh(torch.Tensor([float('inf')])).to(self.device)\n",
    "        self.rescale_fn = lambda x: (x - self.nn_min) * (self.env_max - self.env_min) / \\\n",
    "                                    (self.nn_max - self.nn_min) + self.env_min\n",
    "\n",
    "        self.target_entropy = -np.prod(self.env_max.shape)\n",
    "        self.logalpha = torch.zeros(1, requires_grad=True, device=self.device)\n",
    "        self.alpha_optimizer = optim.Adam([self.logalpha], lr=entropy_lr)\n",
    "\n",
    "    def _format(self, state):\n",
    "        x = state\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.tensor(x, \n",
    "                             device=self.device, \n",
    "                             dtype=torch.float32)\n",
    "            x = x.unsqueeze(0)\n",
    "        return x\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self._format(state)\n",
    "        x = self.activation_fc(self.input_layer(x))\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            x = self.activation_fc(hidden_layer(x))\n",
    "        x_mean = self.output_layer_mean(x)\n",
    "        x_log_std = self.output_layer_log_std(x)\n",
    "        x_log_std = torch.clamp(x_log_std, \n",
    "                                self.log_std_min, \n",
    "                                self.log_std_max)\n",
    "        return x_mean, x_log_std\n",
    "\n",
    "    def full_pass(self, state, epsilon=1e-6):\n",
    "        mean, log_std = self.forward(state)\n",
    "\n",
    "        pi_s = Normal(mean, log_std.exp())\n",
    "        pre_tanh_action = pi_s.rsample()\n",
    "        tanh_action = torch.tanh(pre_tanh_action)\n",
    "        action = self.rescale_fn(tanh_action)\n",
    "\n",
    "        log_prob = pi_s.log_prob(pre_tanh_action) - torch.log(\n",
    "            (1 - tanh_action.pow(2)).clamp(0, 1) + epsilon)\n",
    "        log_prob = log_prob.sum(dim=1, keepdim=True)\n",
    "\n",
    "        return action, log_prob, self.rescale_fn(torch.tanh(mean))\n",
    "\n",
    "    def _update_exploration_ratio(self, greedy_action, action_taken):\n",
    "        env_min, env_max = self.env_min.cpu().numpy(), self.env_max.cpu().numpy()\n",
    "        self.exploration_ratio = np.mean(abs((greedy_action - action_taken)/(env_max - env_min)))\n",
    "\n",
    "    def _get_actions(self, state):\n",
    "        mean, log_std = self.forward(state)\n",
    "\n",
    "        action = self.rescale_fn(torch.tanh(Normal(mean, log_std.exp()).sample()))\n",
    "        greedy_action = self.rescale_fn(torch.tanh(mean))\n",
    "        random_action = np.random.uniform(low=self.env_min.cpu().numpy(),\n",
    "                                          high=self.env_max.cpu().numpy())\n",
    "\n",
    "        action_shape = self.env_max.cpu().numpy().shape\n",
    "        action = action.detach().cpu().numpy().reshape(action_shape)\n",
    "        greedy_action = greedy_action.detach().cpu().numpy().reshape(action_shape)\n",
    "        random_action = random_action.reshape(action_shape)\n",
    "\n",
    "        return action, greedy_action, random_action\n",
    "\n",
    "    def select_random_action(self, state):\n",
    "        action, greedy_action, random_action = self._get_actions(state)\n",
    "        self._update_exploration_ratio(greedy_action, random_action)\n",
    "        return random_action\n",
    "\n",
    "    def select_greedy_action(self, state):\n",
    "        action, greedy_action, random_action = self._get_actions(state)\n",
    "        self._update_exploration_ratio(greedy_action, greedy_action)\n",
    "        return greedy_action\n",
    "\n",
    "    def select_action(self, state):\n",
    "        action, greedy_action, random_action = self._get_actions(state)\n",
    "        self._update_exploration_ratio(greedy_action, action)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAC():\n",
    "    def __init__(self, \n",
    "                 replay_buffer_fn,\n",
    "                 policy_model_fn, \n",
    "                 policy_max_grad_norm, \n",
    "                 policy_optimizer_fn, \n",
    "                 policy_optimizer_lr,\n",
    "                 value_model_fn,\n",
    "                 value_max_grad_norm, \n",
    "                 value_optimizer_fn, \n",
    "                 value_optimizer_lr,\n",
    "                 n_warmup_batches,\n",
    "                 update_target_every_steps,\n",
    "                 tau):\n",
    "        self.replay_buffer_fn = replay_buffer_fn\n",
    "\n",
    "        self.policy_model_fn = policy_model_fn\n",
    "        self.policy_max_grad_norm = policy_max_grad_norm\n",
    "        self.policy_optimizer_fn = policy_optimizer_fn\n",
    "        self.policy_optimizer_lr = policy_optimizer_lr\n",
    "\n",
    "        self.value_model_fn = value_model_fn\n",
    "        self.value_max_grad_norm = value_max_grad_norm\n",
    "        self.value_optimizer_fn = value_optimizer_fn\n",
    "        self.value_optimizer_lr = value_optimizer_lr\n",
    "\n",
    "        self.n_warmup_batches = n_warmup_batches\n",
    "        self.update_target_every_steps = update_target_every_steps\n",
    "\n",
    "        self.tau = tau\n",
    "\n",
    "    def optimize_model(self, experiences):\n",
    "        states, actions, rewards, next_states, is_terminals = experiences\n",
    "        batch_size = len(is_terminals)\n",
    "\n",
    "        # policy loss\n",
    "        current_actions, logpi_s, _ = self.policy_model.full_pass(states)\n",
    "\n",
    "        target_alpha = (logpi_s + self.policy_model.target_entropy).detach()\n",
    "        alpha_loss = -(self.policy_model.logalpha * target_alpha).mean()\n",
    "\n",
    "        self.policy_model.alpha_optimizer.zero_grad()\n",
    "        alpha_loss.backward()\n",
    "        self.policy_model.alpha_optimizer.step()\n",
    "        alpha = self.policy_model.logalpha.exp()\n",
    "\n",
    "        current_q_sa_a = self.online_value_model_a(states, current_actions)\n",
    "        current_q_sa_b = self.online_value_model_b(states, current_actions)\n",
    "        current_q_sa = torch.min(current_q_sa_a, current_q_sa_b)\n",
    "        policy_loss = (alpha * logpi_s - current_q_sa).mean()\n",
    "\n",
    "        # Q loss\n",
    "        ap, logpi_sp, _ = self.policy_model.full_pass(next_states)\n",
    "        q_spap_a = self.target_value_model_a(next_states, ap)\n",
    "        q_spap_b = self.target_value_model_b(next_states, ap)\n",
    "        q_spap = torch.min(q_spap_a, q_spap_b) - alpha * logpi_sp\n",
    "        target_q_sa = (rewards + self.gamma * q_spap * (1 - is_terminals)).detach()\n",
    "\n",
    "        q_sa_a = self.online_value_model_a(states, actions)\n",
    "        q_sa_b = self.online_value_model_b(states, actions)\n",
    "        qa_loss = (q_sa_a - target_q_sa).pow(2).mul(0.5).mean()\n",
    "        qb_loss = (q_sa_b - target_q_sa).pow(2).mul(0.5).mean()\n",
    "\n",
    "        self.value_optimizer_a.zero_grad()\n",
    "        qa_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.online_value_model_a.parameters(), \n",
    "                                       self.value_max_grad_norm)\n",
    "        self.value_optimizer_a.step()\n",
    "\n",
    "        self.value_optimizer_b.zero_grad()\n",
    "        qb_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.online_value_model_b.parameters(),\n",
    "                                       self.value_max_grad_norm)\n",
    "        self.value_optimizer_b.step()\n",
    "\n",
    "        self.policy_optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy_model.parameters(), \n",
    "                                       self.policy_max_grad_norm)        \n",
    "        self.policy_optimizer.step()\n",
    "\n",
    "    def interaction_step(self, state, env):\n",
    "        min_samples = self.replay_buffer.batch_size * self.n_warmup_batches\n",
    "        if len(self.replay_buffer) < min_samples:\n",
    "            action = self.policy_model.select_random_action(state)\n",
    "        else:\n",
    "            action = self.policy_model.select_action(state)\n",
    "\n",
    "        new_state, reward, is_terminal, info = env.step(action)\n",
    "        is_truncated = 'TimeLimit.truncated' in info and info['TimeLimit.truncated']\n",
    "        is_failure = is_terminal and not is_truncated\n",
    "        experience = (state, action, reward, new_state, float(is_failure))\n",
    "\n",
    "        self.replay_buffer.store(experience)\n",
    "        self.episode_reward[-1] += reward\n",
    "        self.episode_timestep[-1] += 1\n",
    "        self.episode_exploration[-1] += self.policy_model.exploration_ratio\n",
    "        return new_state, is_terminal\n",
    "\n",
    "    def update_value_networks(self, tau=None):\n",
    "        tau = self.tau if tau is None else tau\n",
    "        for target, online in zip(self.target_value_model_a.parameters(), \n",
    "                                  self.online_value_model_a.parameters()):\n",
    "            target_ratio = (1.0 - tau) * target.data\n",
    "            online_ratio = tau * online.data\n",
    "            mixed_weights = target_ratio + online_ratio\n",
    "            target.data.copy_(mixed_weights)\n",
    "\n",
    "        for target, online in zip(self.target_value_model_b.parameters(), \n",
    "                                  self.online_value_model_b.parameters()):\n",
    "            target_ratio = (1.0 - tau) * target.data\n",
    "            online_ratio = tau * online.data\n",
    "            mixed_weights = target_ratio + online_ratio\n",
    "            target.data.copy_(mixed_weights)\n",
    "\n",
    "    def train(self, make_env_fn, make_env_kargs, seed, gamma, \n",
    "              max_minutes, max_episodes, goal_mean_100_reward):\n",
    "        training_start, last_debug_time = time.time(), float('-inf')\n",
    "\n",
    "        self.checkpoint_dir = tempfile.mkdtemp()\n",
    "        self.make_env_fn = make_env_fn\n",
    "        self.make_env_kargs = make_env_kargs\n",
    "        self.seed = seed\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        env = self.make_env_fn(**self.make_env_kargs, seed=self.seed)\n",
    "        torch.manual_seed(self.seed) ; np.random.seed(self.seed) ; random.seed(self.seed)\n",
    "    \n",
    "        nS, nA = env.observation_space.shape[0], env.action_space.shape[0]\n",
    "        action_bounds = env.action_space.low, env.action_space.high\n",
    "        self.episode_timestep = []\n",
    "        self.episode_reward = []\n",
    "        self.episode_seconds = []\n",
    "        self.evaluation_scores = []        \n",
    "        self.episode_exploration = []\n",
    "\n",
    "        self.target_value_model_a = self.value_model_fn(nS, nA)\n",
    "        self.online_value_model_a = self.value_model_fn(nS, nA)\n",
    "        self.target_value_model_b = self.value_model_fn(nS, nA)\n",
    "        self.online_value_model_b = self.value_model_fn(nS, nA)\n",
    "        self.update_value_networks(tau=1.0)\n",
    "\n",
    "        self.policy_model = self.policy_model_fn(nS, action_bounds)\n",
    "\n",
    "        self.value_optimizer_a = self.value_optimizer_fn(self.online_value_model_a,\n",
    "                                                         self.value_optimizer_lr)\n",
    "        self.value_optimizer_b = self.value_optimizer_fn(self.online_value_model_b,\n",
    "                                                         self.value_optimizer_lr)\n",
    "        self.policy_optimizer = self.policy_optimizer_fn(self.policy_model,\n",
    "                                                         self.policy_optimizer_lr)\n",
    "\n",
    "        self.replay_buffer = self.replay_buffer_fn()\n",
    "                    \n",
    "        result = np.empty((max_episodes, 5))\n",
    "        result[:] = np.nan\n",
    "        training_time = 0\n",
    "        for episode in range(1, max_episodes + 1):\n",
    "            episode_start = time.time()\n",
    "            \n",
    "            state, is_terminal = env.reset(), False\n",
    "            self.episode_reward.append(0.0)\n",
    "            self.episode_timestep.append(0.0)\n",
    "            self.episode_exploration.append(0.0)\n",
    "\n",
    "            for step in count():\n",
    "                state, is_terminal = self.interaction_step(state, env)\n",
    "\n",
    "                min_samples = self.replay_buffer.batch_size * self.n_warmup_batches\n",
    "                if len(self.replay_buffer) > min_samples:\n",
    "                    experiences = self.replay_buffer.sample()\n",
    "                    experiences = self.online_value_model_a.load(experiences)\n",
    "                    self.optimize_model(experiences)\n",
    "\n",
    "                if np.sum(self.episode_timestep) % self.update_target_every_steps == 0:\n",
    "                    self.update_value_networks()\n",
    "\n",
    "                if is_terminal:\n",
    "                    gc.collect()\n",
    "                    break\n",
    "\n",
    "            # stats\n",
    "            episode_elapsed = time.time() - episode_start\n",
    "            self.episode_seconds.append(episode_elapsed)\n",
    "            training_time += episode_elapsed\n",
    "            evaluation_score, _ = self.evaluate(self.policy_model, env)\n",
    "            self.save_checkpoint(episode-1, self.policy_model)\n",
    "\n",
    "            total_step = int(np.sum(self.episode_timestep))\n",
    "            self.evaluation_scores.append(evaluation_score)\n",
    "            \n",
    "            mean_10_reward = np.mean(self.episode_reward[-10:])\n",
    "            std_10_reward = np.std(self.episode_reward[-10:])\n",
    "            mean_100_reward = np.mean(self.episode_reward[-100:])\n",
    "            std_100_reward = np.std(self.episode_reward[-100:])\n",
    "            mean_100_eval_score = np.mean(self.evaluation_scores[-100:])\n",
    "            std_100_eval_score = np.std(self.evaluation_scores[-100:])\n",
    "            lst_100_exp_rat = np.array(\n",
    "                self.episode_exploration[-100:])/np.array(self.episode_timestep[-100:])\n",
    "            mean_100_exp_rat = np.mean(lst_100_exp_rat)\n",
    "            std_100_exp_rat = np.std(lst_100_exp_rat)\n",
    "            \n",
    "            wallclock_elapsed = time.time() - training_start\n",
    "            result[episode-1] = total_step, mean_100_reward, \\\n",
    "                mean_100_eval_score, training_time, wallclock_elapsed\n",
    "            \n",
    "            reached_debug_time = time.time() - last_debug_time >= LEAVE_PRINT_EVERY_N_SECS\n",
    "            reached_max_minutes = wallclock_elapsed >= max_minutes * 60\n",
    "            reached_max_episodes = episode >= max_episodes\n",
    "            reached_goal_mean_reward = mean_100_eval_score >= goal_mean_100_reward\n",
    "            training_is_over = reached_max_minutes or \\\n",
    "                               reached_max_episodes or \\\n",
    "                               reached_goal_mean_reward\n",
    "            elapsed_str = time.strftime(\"%H:%M:%S\", time.gmtime(time.time() - training_start))\n",
    "            debug_message = 'el {}, ep {:04}, ts {:07}, '\n",
    "            debug_message += 'ar 10 {:05.1f}\\u00B1{:05.1f}, '\n",
    "            debug_message += '100 {:05.1f}\\u00B1{:05.1f}, '\n",
    "            debug_message += 'ex 100 {:02.1f}\\u00B1{:02.1f}, '\n",
    "            debug_message += 'ev {:05.1f}\\u00B1{:05.1f}'\n",
    "            debug_message = debug_message.format(\n",
    "                elapsed_str, episode-1, total_step, mean_10_reward, std_10_reward, \n",
    "                mean_100_reward, std_100_reward, mean_100_exp_rat, std_100_exp_rat,\n",
    "                mean_100_eval_score, std_100_eval_score)\n",
    "            print(debug_message, end='\\r', flush=True)\n",
    "            if reached_debug_time or training_is_over:\n",
    "                print(ERASE_LINE + debug_message, flush=True)\n",
    "                last_debug_time = time.time()\n",
    "            if training_is_over:\n",
    "                if reached_max_minutes: print(u'--> reached_max_minutes \\u2715')\n",
    "                if reached_max_episodes: print(u'--> reached_max_episodes \\u2715')\n",
    "                if reached_goal_mean_reward: print(u'--> reached_goal_mean_reward \\u2713')\n",
    "                break\n",
    "                \n",
    "        final_eval_score, score_std = self.evaluate(self.policy_model, env, n_episodes=100)\n",
    "        wallclock_time = time.time() - training_start\n",
    "        print('Training complete.')\n",
    "        print('Final evaluation score {:.2f}\\u00B1{:.2f} in {:.2f}s training time,'\n",
    "              ' {:.2f}s wall-clock time.\\n'.format(\n",
    "                  final_eval_score, score_std, training_time, wallclock_time))\n",
    "        env.close() ; del env\n",
    "        self.get_cleaned_checkpoints()\n",
    "        return result, final_eval_score, training_time, wallclock_time\n",
    "    \n",
    "    def evaluate(self, eval_policy_model, eval_env, n_episodes=1):\n",
    "        rs = []\n",
    "        for _ in range(n_episodes):\n",
    "            s, d = eval_env.reset(), False\n",
    "            rs.append(0)\n",
    "            for _ in count():\n",
    "                a = eval_policy_model.select_greedy_action(s)\n",
    "                s, r, d, _ = eval_env.step(a)\n",
    "                rs[-1] += r\n",
    "                if d: break\n",
    "        return np.mean(rs), np.std(rs)\n",
    "\n",
    "    def get_cleaned_checkpoints(self, n_checkpoints=4):\n",
    "        try: \n",
    "            return self.checkpoint_paths\n",
    "        except AttributeError:\n",
    "            self.checkpoint_paths = {}\n",
    "\n",
    "        paths = glob.glob(os.path.join(self.checkpoint_dir, '*.tar'))\n",
    "        paths_dic = {int(path.split('.')[-2]):path for path in paths}\n",
    "        last_ep = max(paths_dic.keys())\n",
    "        # checkpoint_idxs = np.geomspace(1, last_ep+1, n_checkpoints, endpoint=True, dtype=np.int)-1\n",
    "        checkpoint_idxs = np.linspace(1, last_ep+1, n_checkpoints, endpoint=True, dtype=np.int)-1\n",
    "\n",
    "        for idx, path in paths_dic.items():\n",
    "            if idx in checkpoint_idxs:\n",
    "                self.checkpoint_paths[idx] = path\n",
    "            else:\n",
    "                os.unlink(path)\n",
    "\n",
    "        return self.checkpoint_paths\n",
    "\n",
    "    def demo_last(self, title='Fully-trained {} Agent', n_episodes=2, max_n_videos=2):\n",
    "        env = self.make_env_fn(**self.make_env_kargs, monitor_mode='evaluation', render=True, record=True)\n",
    "\n",
    "        checkpoint_paths = self.get_cleaned_checkpoints()\n",
    "        last_ep = max(checkpoint_paths.keys())\n",
    "        self.policy_model.load_state_dict(torch.load(checkpoint_paths[last_ep]))\n",
    "\n",
    "        self.evaluate(self.policy_model, env, n_episodes=n_episodes)\n",
    "        env.close()\n",
    "        data = get_gif_html(env_videos=env.videos, \n",
    "                            title=title.format(self.__class__.__name__),\n",
    "                            max_n_videos=max_n_videos)\n",
    "        del env\n",
    "        return HTML(data=data)\n",
    "\n",
    "    def demo_progression(self, title='{} Agent progression', max_n_videos=4):\n",
    "        env = self.make_env_fn(**self.make_env_kargs, monitor_mode='evaluation', render=True, record=True)\n",
    "\n",
    "        checkpoint_paths = self.get_cleaned_checkpoints()\n",
    "        for i in sorted(checkpoint_paths.keys()):\n",
    "            self.policy_model.load_state_dict(torch.load(checkpoint_paths[i]))\n",
    "            self.evaluate(self.policy_model, env, n_episodes=1)\n",
    "\n",
    "        env.close()\n",
    "        data = get_gif_html(env_videos=env.videos, \n",
    "                            title=title.format(self.__class__.__name__),\n",
    "                            subtitle_eps=sorted(checkpoint_paths.keys()),\n",
    "                            max_n_videos=max_n_videos)\n",
    "        del env\n",
    "        return HTML(data=data)\n",
    "\n",
    "    def save_checkpoint(self, episode_idx, model):\n",
    "        torch.save(model.state_dict(), \n",
    "                   os.path.join(self.checkpoint_dir, 'model.{}.tar'.format(episode_idx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sac_results = []\n",
    "best_agent, best_eval_score = None, float('-inf')\n",
    "for seed in SEEDS:\n",
    "    environment_settings = {\n",
    "        'env_name': 'HalfCheetahBulletEnv-v0',\n",
    "        'gamma': 0.99,\n",
    "        'max_minutes': 300,\n",
    "        'max_episodes': 10000,\n",
    "        'goal_mean_100_reward': 2000\n",
    "    }\n",
    "\n",
    "    policy_model_fn = lambda nS, bounds: FCGP(nS, bounds, hidden_dims=(256,256))\n",
    "    policy_max_grad_norm = float('inf')\n",
    "    policy_optimizer_fn = lambda net, lr: optim.Adam(net.parameters(), lr=lr)\n",
    "    policy_optimizer_lr = 0.0003\n",
    "\n",
    "    value_model_fn = lambda nS, nA: FCQSA(nS, nA, hidden_dims=(256,256))\n",
    "    value_max_grad_norm = float('inf')\n",
    "    value_optimizer_fn = lambda net, lr: optim.Adam(net.parameters(), lr=lr)\n",
    "    value_optimizer_lr = 0.0005\n",
    "\n",
    "    replay_buffer_fn = lambda: ReplayBuffer(max_size=100000, batch_size=64)\n",
    "    n_warmup_batches = 10\n",
    "    update_target_every_steps = 1\n",
    "    tau = 0.001\n",
    "\n",
    "    env_name, gamma, max_minutes, \\\n",
    "    max_episodes, goal_mean_100_reward = environment_settings.values()\n",
    "                \n",
    "    agent = SAC(replay_buffer_fn,\n",
    "                policy_model_fn, \n",
    "                policy_max_grad_norm,\n",
    "                policy_optimizer_fn, \n",
    "                policy_optimizer_lr,\n",
    "                value_model_fn,\n",
    "                value_max_grad_norm, \n",
    "                value_optimizer_fn, \n",
    "                value_optimizer_lr, \n",
    "                n_warmup_batches,\n",
    "                update_target_every_steps,\n",
    "                tau)\n",
    "\n",
    "    make_env_fn, make_env_kargs = get_make_env_fn(env_name=env_name, inner_wrappers=[RenderUint8])\n",
    "    result, final_eval_score, training_time, wallclock_time = agent.train(\n",
    "        make_env_fn, make_env_kargs, seed, gamma, max_minutes, max_episodes, goal_mean_100_reward)\n",
    "    sac_results.append(result)\n",
    "    if final_eval_score > best_eval_score:\n",
    "        best_eval_score = final_eval_score\n",
    "        best_agent = agent\n",
    "sac_results = np.array(sac_results)\n",
    "_ = BEEP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "best_agent.demo_progression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_agent.demo_last()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sac_max_t, sac_max_r, sac_max_s, \\\n",
    "sac_max_sec, sac_max_rt = np.max(sac_results, axis=0).T\n",
    "sac_min_t, sac_min_r, sac_min_s, \\\n",
    "sac_min_sec, sac_min_rt = np.min(sac_results, axis=0).T\n",
    "sac_mean_t, sac_mean_r, sac_mean_s, \\\n",
    "sac_mean_sec, sac_mean_rt = np.mean(sac_results, axis=0).T\n",
    "sac_x = np.arange(len(sac_mean_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 1, figsize=(15,10), sharey=False, sharex=True)\n",
    "\n",
    "# SAC\n",
    "axs[0].plot(sac_max_r, 'g', linewidth=1)\n",
    "axs[0].plot(sac_min_r, 'g', linewidth=1)\n",
    "axs[0].plot(sac_mean_r, 'g:', label='SAC', linewidth=2)\n",
    "axs[0].fill_between(\n",
    "    sac_x, sac_min_r, sac_max_r, facecolor='g', alpha=0.3)\n",
    "\n",
    "axs[1].plot(sac_max_s, 'g', linewidth=1)\n",
    "axs[1].plot(sac_min_s, 'g', linewidth=1)\n",
    "axs[1].plot(sac_mean_s, 'g:', label='SAC', linewidth=2)\n",
    "axs[1].fill_between(\n",
    "    sac_x, sac_min_s, sac_max_s, facecolor='g', alpha=0.3)\n",
    "\n",
    "# ALL\n",
    "axs[0].set_title('Moving Avg Reward (Training)')\n",
    "axs[1].set_title('Moving Avg Reward (Evaluation)')\n",
    "plt.xlabel('Episodes')\n",
    "axs[0].legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 1, figsize=(15,15), sharey=False, sharex=True)\n",
    "\n",
    "# SAC\n",
    "axs[0].plot(sac_max_t, 'g', linewidth=1)\n",
    "axs[0].plot(sac_min_t, 'g', linewidth=1)\n",
    "axs[0].plot(sac_mean_t, 'g:', label='SAC', linewidth=2)\n",
    "axs[0].fill_between(\n",
    "    sac_x, sac_min_t, sac_max_t, facecolor='g', alpha=0.3)\n",
    "\n",
    "axs[1].plot(sac_max_sec, 'g', linewidth=1)\n",
    "axs[1].plot(sac_min_sec, 'g', linewidth=1)\n",
    "axs[1].plot(sac_mean_sec, 'g:', label='SAC', linewidth=2)\n",
    "axs[1].fill_between(\n",
    "    sac_x, sac_min_sec, sac_max_sec, facecolor='g', alpha=0.3)\n",
    "\n",
    "axs[2].plot(sac_max_rt, 'g', linewidth=1)\n",
    "axs[2].plot(sac_min_rt, 'g', linewidth=1)\n",
    "axs[2].plot(sac_mean_rt, 'g:', label='SAC', linewidth=2)\n",
    "axs[2].fill_between(\n",
    "    sac_x, sac_min_rt, sac_max_rt, facecolor='g', alpha=0.3)\n",
    "\n",
    "# ALL\n",
    "axs[0].set_title('Total Steps')\n",
    "axs[1].set_title('Training Time')\n",
    "axs[2].set_title('Wall-clock Time')\n",
    "plt.xlabel('Episodes')\n",
    "axs[0].legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sac_root_dir = os.path.join(RESULTS_DIR, 'sac')\n",
    "not os.path.exists(sac_root_dir) and os.makedirs(sac_root_dir)\n",
    "\n",
    "np.save(os.path.join(sac_root_dir, 'x'), sac_x)\n",
    "\n",
    "np.save(os.path.join(sac_root_dir, 'max_r'), sac_max_r)\n",
    "np.save(os.path.join(sac_root_dir, 'min_r'), sac_min_r)\n",
    "np.save(os.path.join(sac_root_dir, 'mean_r'), sac_mean_r)\n",
    "\n",
    "np.save(os.path.join(sac_root_dir, 'max_s'), sac_max_s)\n",
    "np.save(os.path.join(sac_root_dir, 'min_s'), sac_min_s )\n",
    "np.save(os.path.join(sac_root_dir, 'mean_s'), sac_mean_s)\n",
    "\n",
    "np.save(os.path.join(sac_root_dir, 'max_t'), sac_max_t)\n",
    "np.save(os.path.join(sac_root_dir, 'min_t'), sac_min_t)\n",
    "np.save(os.path.join(sac_root_dir, 'mean_t'), sac_mean_t)\n",
    "\n",
    "np.save(os.path.join(sac_root_dir, 'max_sec'), sac_max_sec)\n",
    "np.save(os.path.join(sac_root_dir, 'min_sec'), sac_min_sec)\n",
    "np.save(os.path.join(sac_root_dir, 'mean_sec'), sac_mean_sec)\n",
    "\n",
    "np.save(os.path.join(sac_root_dir, 'max_rt'), sac_max_rt)\n",
    "np.save(os.path.join(sac_root_dir, 'min_rt'), sac_min_rt)\n",
    "np.save(os.path.join(sac_root_dir, 'mean_rt'), sac_mean_rt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiprocessEnv(object):\n",
    "    def __init__(self, make_env_fn, make_env_kargs, seed, n_workers):\n",
    "        self.make_env_fn = make_env_fn\n",
    "        self.make_env_kargs = make_env_kargs\n",
    "        self.seed = seed\n",
    "        self.n_workers = n_workers\n",
    "        self.pipes = [mp.Pipe() for rank in range(self.n_workers)]\n",
    "        self.workers = [\n",
    "            mp.Process(\n",
    "                target=self.work, \n",
    "                args=(rank, self.pipes[rank][1])) for rank in range(self.n_workers)]\n",
    "        [w.start() for w in self.workers]\n",
    "        self.dones = {rank:False for rank in range(self.n_workers)}\n",
    "\n",
    "    def reset(self, ranks=None, **kwargs):\n",
    "        if not (ranks is None):\n",
    "            [self.send_msg(('reset', {}), rank) for rank in ranks]            \n",
    "            return np.stack([parent_end.recv() for rank, (parent_end, _) in enumerate(self.pipes) if rank in ranks])\n",
    "\n",
    "        self.broadcast_msg(('reset', kwargs))\n",
    "        return np.stack([parent_end.recv() for parent_end, _ in self.pipes])\n",
    "\n",
    "    def step(self, actions):\n",
    "        assert len(actions) == self.n_workers\n",
    "        [self.send_msg(\n",
    "            ('step', {'action':actions[rank]}), \n",
    "            rank) for rank in range(self.n_workers)]\n",
    "        results = []\n",
    "        for rank in range(self.n_workers):\n",
    "            parent_end, _ = self.pipes[rank]\n",
    "            o, r, d, i = parent_end.recv()\n",
    "            results.append((o,\n",
    "                            float(r),\n",
    "                            float(d),\n",
    "                            i))\n",
    "        return [np.stack(block).squeeze() for block in np.array(results).T]\n",
    "\n",
    "    def close(self, **kwargs):\n",
    "        self.broadcast_msg(('close', kwargs))\n",
    "        [w.join() for w in self.workers]\n",
    "    \n",
    "    def work(self, rank, worker_end):\n",
    "        env = self.make_env_fn(**self.make_env_kargs, seed=self.seed+rank)\n",
    "        while True:\n",
    "            cmd, kwargs = worker_end.recv()\n",
    "            if cmd == 'reset':\n",
    "                worker_end.send(env.reset(**kwargs))\n",
    "            elif cmd == 'step':\n",
    "                worker_end.send(env.step(**kwargs))\n",
    "            elif cmd == '_past_limit':\n",
    "                worker_end.send(env._elapsed_steps >= env._max_episode_steps)\n",
    "            else:\n",
    "                # including close command \n",
    "                env.close(**kwargs) ; del env ; worker_end.close()\n",
    "                break\n",
    "\n",
    "    def send_msg(self, msg, rank):\n",
    "        parent_end, _ = self.pipes[rank]\n",
    "        parent_end.send(msg)\n",
    "\n",
    "    def broadcast_msg(self, msg):    \n",
    "        [parent_end.send(msg) for parent_end, _ in self.pipes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpisodeBuffer():\n",
    "    def __init__(self,\n",
    "                 state_dim,\n",
    "                 gamma,\n",
    "                 tau,\n",
    "                 n_workers,\n",
    "                 max_episodes,\n",
    "                 max_episode_steps):\n",
    "        \n",
    "        assert max_episodes >= n_workers\n",
    "\n",
    "        self.state_dim = state_dim\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.n_workers = n_workers\n",
    "        self.max_episodes = max_episodes\n",
    "        self.max_episode_steps = max_episode_steps\n",
    "\n",
    "        self._truncated_fn = np.vectorize(lambda x: 'TimeLimit.truncated' in x and x['TimeLimit.truncated'])\n",
    "        self.discounts = np.logspace(\n",
    "            0, max_episode_steps+1, num=max_episode_steps+1, base=gamma, endpoint=False, dtype=np.float128)\n",
    "        self.tau_discounts = np.logspace(\n",
    "            0, max_episode_steps+1, num=max_episode_steps+1, base=gamma*tau, endpoint=False, dtype=np.float128)\n",
    "\n",
    "        device = 'cpu'\n",
    "        if torch.cuda.is_available():\n",
    "            device = 'cuda:0'\n",
    "        self.device = torch.device(device)\n",
    "\n",
    "        self.clear()\n",
    "\n",
    "    def clear(self):\n",
    "        self.states_mem = np.empty(\n",
    "            shape=np.concatenate(((self.max_episodes, self.max_episode_steps), self.state_dim)), dtype=np.float64)\n",
    "        self.states_mem[:] = np.nan\n",
    "\n",
    "        self.actions_mem = np.empty(shape=(self.max_episodes, self.max_episode_steps), dtype=np.uint8)\n",
    "        self.actions_mem[:] = np.nan\n",
    "\n",
    "        self.returns_mem = np.empty(shape=(self.max_episodes,self.max_episode_steps), dtype=np.float32)\n",
    "        self.returns_mem[:] = np.nan\n",
    "\n",
    "        self.gaes_mem = np.empty(shape=(self.max_episodes, self.max_episode_steps), dtype=np.float32)\n",
    "        self.gaes_mem[:] = np.nan\n",
    "\n",
    "        self.logpas_mem = np.empty(shape=(self.max_episodes, self.max_episode_steps), dtype=np.float32)\n",
    "        self.logpas_mem[:] = np.nan\n",
    "\n",
    "        self.episode_steps = np.zeros(shape=(self.max_episodes), dtype=np.uint16)\n",
    "        self.episode_reward = np.zeros(shape=(self.max_episodes), dtype=np.float32)\n",
    "        self.episode_exploration = np.zeros(shape=(self.max_episodes), dtype=np.float32)\n",
    "        self.episode_seconds = np.zeros(shape=(self.max_episodes), dtype=np.float64)\n",
    "\n",
    "        self.current_ep_idxs = np.arange(n_workers, dtype=np.uint16)\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "    def fill(self, envs, policy_model, value_model):\n",
    "        states = envs.reset()\n",
    "\n",
    "        worker_rewards = np.zeros(shape=(n_workers, self.max_episode_steps), dtype=np.float32)\n",
    "        worker_exploratory = np.zeros(shape=(n_workers, self.max_episode_steps), dtype=np.bool)\n",
    "        worker_steps = np.zeros(shape=(n_workers), dtype=np.uint16)\n",
    "        worker_seconds = np.array([time.time(),] * n_workers, dtype=np.float64)\n",
    "\n",
    "        buffer_full = False\n",
    "        while not buffer_full and len(self.episode_steps[self.episode_steps > 0]) < self.max_episodes/2:\n",
    "            with torch.no_grad():\n",
    "                actions, logpas, are_exploratory = policy_model.np_pass(states)\n",
    "                values = value_model(states)\n",
    "\n",
    "            next_states, rewards, terminals, infos = envs.step(actions)\n",
    "            self.states_mem[self.current_ep_idxs, worker_steps] = states\n",
    "            self.actions_mem[self.current_ep_idxs, worker_steps] = actions\n",
    "            self.logpas_mem[self.current_ep_idxs, worker_steps] = logpas\n",
    "\n",
    "            worker_exploratory[np.arange(self.n_workers), worker_steps] = are_exploratory\n",
    "            worker_rewards[np.arange(self.n_workers), worker_steps] = rewards\n",
    "\n",
    "            for w_idx in range(self.n_workers):\n",
    "                if worker_steps[w_idx] + 1 == self.max_episode_steps:\n",
    "                    terminals[w_idx] = 1\n",
    "                    infos[w_idx]['TimeLimit.truncated'] = True\n",
    "\n",
    "            if terminals.sum():\n",
    "                idx_terminals = np.flatnonzero(terminals)\n",
    "                next_values = np.zeros(shape=(n_workers))\n",
    "                truncated = self._truncated_fn(infos)\n",
    "                if truncated.sum():\n",
    "                    idx_truncated = np.flatnonzero(truncated)\n",
    "                    with torch.no_grad():\n",
    "                        next_values[idx_truncated] = value_model(\n",
    "                            next_states[idx_truncated]).cpu().numpy()\n",
    "\n",
    "            states = next_states\n",
    "            worker_steps += 1\n",
    "\n",
    "            if terminals.sum():\n",
    "                new_states = envs.reset(ranks=idx_terminals)\n",
    "                states[idx_terminals] = new_states\n",
    "\n",
    "                for w_idx in range(self.n_workers):\n",
    "                    if w_idx not in idx_terminals:\n",
    "                        continue\n",
    "\n",
    "                    e_idx = self.current_ep_idxs[w_idx]\n",
    "                    T = worker_steps[w_idx]\n",
    "                    self.episode_steps[e_idx] = T\n",
    "                    self.episode_reward[e_idx] = worker_rewards[w_idx, :T].sum()\n",
    "                    self.episode_exploration[e_idx] = worker_exploratory[w_idx, :T].mean()\n",
    "                    self.episode_seconds[e_idx] = time.time() - worker_seconds[w_idx]\n",
    "\n",
    "                    ep_rewards = np.concatenate(\n",
    "                        (worker_rewards[w_idx, :T], [next_values[w_idx]]))\n",
    "                    ep_discounts = self.discounts[:T+1]\n",
    "                    ep_returns = np.array(\n",
    "                        [np.sum(ep_discounts[:T+1-t] * ep_rewards[t:]) for t in range(T)])\n",
    "                    self.returns_mem[e_idx, :T] = ep_returns\n",
    "\n",
    "                    ep_states = self.states_mem[e_idx, :T]\n",
    "                    with torch.no_grad():\n",
    "                        ep_values = torch.cat((value_model(ep_states),\n",
    "                                               torch.tensor([next_values[w_idx]],\n",
    "                                                            device=value_model.device,\n",
    "                                                            dtype=torch.float32)))\n",
    "                    np_ep_values = ep_values.view(-1).cpu().numpy()\n",
    "                    ep_tau_discounts = self.tau_discounts[:T]\n",
    "                    deltas = ep_rewards[:-1] + self.gamma * np_ep_values[1:] - np_ep_values[:-1]\n",
    "                    gaes = np.array(\n",
    "                        [np.sum(self.tau_discounts[:T-t] * deltas[t:]) for t in range(T)])\n",
    "                    self.gaes_mem[e_idx, :T] = gaes\n",
    "\n",
    "                    worker_exploratory[w_idx, :] = 0\n",
    "                    worker_rewards[w_idx, :] = 0\n",
    "                    worker_steps[w_idx] = 0\n",
    "                    worker_seconds[w_idx] = time.time()\n",
    "\n",
    "                    new_ep_id = max(self.current_ep_idxs) + 1\n",
    "                    if new_ep_id >= self.max_episodes:\n",
    "                        buffer_full = True\n",
    "                        break\n",
    "\n",
    "                    self.current_ep_idxs[w_idx] = new_ep_id\n",
    "\n",
    "        ep_idxs = self.episode_steps > 0\n",
    "        ep_t = self.episode_steps[ep_idxs]\n",
    "\n",
    "        self.states_mem = [row[:ep_t[i]] for i, row in enumerate(self.states_mem[ep_idxs])]\n",
    "        self.states_mem = np.concatenate(self.states_mem)\n",
    "        self.actions_mem = [row[:ep_t[i]] for i, row in enumerate(self.actions_mem[ep_idxs])]\n",
    "        self.actions_mem = np.concatenate(self.actions_mem)\n",
    "        self.returns_mem = [row[:ep_t[i]] for i, row in enumerate(self.returns_mem[ep_idxs])]\n",
    "        self.returns_mem = torch.tensor(np.concatenate(self.returns_mem), \n",
    "                                        device=value_model.device)\n",
    "        self.gaes_mem = [row[:ep_t[i]] for i, row in enumerate(self.gaes_mem[ep_idxs])]\n",
    "        self.gaes_mem = torch.tensor(np.concatenate(self.gaes_mem), \n",
    "                                     device=value_model.device)\n",
    "        self.logpas_mem = [row[:ep_t[i]] for i, row in enumerate(self.logpas_mem[ep_idxs])]\n",
    "        self.logpas_mem = torch.tensor(np.concatenate(self.logpas_mem), \n",
    "                                       device=value_model.device)\n",
    "\n",
    "        ep_r = self.episode_reward[ep_idxs]\n",
    "        ep_x = self.episode_exploration[ep_idxs]\n",
    "        ep_s = self.episode_seconds[ep_idxs]\n",
    "        return ep_t, ep_r, ep_x, ep_s\n",
    "\n",
    "    def get_stacks(self):\n",
    "        return (self.states_mem, self.actions_mem, \n",
    "                self.returns_mem, self.gaes_mem, self.logpas_mem)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.episode_steps[self.episode_steps > 0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCCA(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim, \n",
    "                 output_dim,\n",
    "                 hidden_dims=(32,32), \n",
    "                 activation_fc=F.relu):\n",
    "        super(FCCA, self).__init__()\n",
    "        self.activation_fc = activation_fc\n",
    "\n",
    "        self.input_layer = nn.Linear(input_dim[0], hidden_dims[0])\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for i in range(len(hidden_dims)-1):\n",
    "            hidden_layer = nn.Linear(hidden_dims[i], hidden_dims[i+1])\n",
    "            self.hidden_layers.append(hidden_layer)\n",
    "        self.output_layer = nn.Linear(hidden_dims[-1], output_dim)\n",
    "\n",
    "        device = \"cpu\"\n",
    "        if torch.cuda.is_available():\n",
    "            device = \"cuda:0\"\n",
    "        self.device = torch.device(device)\n",
    "        self.to(self.device)\n",
    "        \n",
    "    def _format(self, states):\n",
    "        x = states\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.tensor(x, device=self.device, dtype=torch.float32)\n",
    "            if len(x.size()) == 1:\n",
    "                x = x.unsqueeze(0)\n",
    "        return x\n",
    "\n",
    "    def forward(self, states):\n",
    "        x = self._format(states)\n",
    "        x = self.activation_fc(self.input_layer(x))\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            x = self.activation_fc(hidden_layer(x))\n",
    "        return self.output_layer(x)\n",
    "\n",
    "    def np_pass(self, states):\n",
    "        logits = self.forward(states)\n",
    "        np_logits = logits.detach().cpu().numpy()\n",
    "        dist = torch.distributions.Categorical(logits=logits)\n",
    "        actions = dist.sample()\n",
    "        np_actions = actions.detach().cpu().numpy()\n",
    "        logpas = dist.log_prob(actions)\n",
    "        np_logpas = logpas.detach().cpu().numpy()\n",
    "        is_exploratory = np_actions != np.argmax(np_logits, axis=1)\n",
    "        return np_actions, np_logpas, is_exploratory\n",
    "    \n",
    "    def select_action(self, states):\n",
    "        logits = self.forward(states)\n",
    "        dist = torch.distributions.Categorical(logits=logits)\n",
    "        action = dist.sample()\n",
    "        return action.detach().cpu().item()\n",
    "    \n",
    "    def get_predictions(self, states, actions):\n",
    "        states, actions = self._format(states), self._format(actions)\n",
    "        logits = self.forward(states)\n",
    "        dist = torch.distributions.Categorical(logits=logits)\n",
    "        logpas = dist.log_prob(actions)\n",
    "        entropies = dist.entropy()\n",
    "        return logpas, entropies\n",
    "    \n",
    "    def select_greedy_action(self, states):\n",
    "        logits = self.forward(states)\n",
    "        return np.argmax(logits.detach().squeeze().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCV(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim,\n",
    "                 hidden_dims=(32,32), \n",
    "                 activation_fc=F.relu):\n",
    "        super(FCV, self).__init__()\n",
    "        self.activation_fc = activation_fc\n",
    "\n",
    "        self.input_layer = nn.Linear(input_dim[0], hidden_dims[0])\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for i in range(len(hidden_dims)-1):\n",
    "            hidden_layer = nn.Linear(hidden_dims[i], hidden_dims[i+1])\n",
    "            self.hidden_layers.append(hidden_layer)\n",
    "        self.output_layer = nn.Linear(hidden_dims[-1], 1)\n",
    "\n",
    "        device = \"cpu\"\n",
    "        if torch.cuda.is_available():\n",
    "            device = \"cuda:0\"\n",
    "        self.device = torch.device(device)\n",
    "        self.to(self.device)\n",
    "        \n",
    "    def _format(self, states):\n",
    "        x = states\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.tensor(x, device=self.device, dtype=torch.float32)\n",
    "            if len(x.size()) == 1:\n",
    "                x = x.unsqueeze(0)\n",
    "        return x\n",
    "\n",
    "    def forward(self, states):\n",
    "        x = self._format(states)\n",
    "        x = self.activation_fc(self.input_layer(x))\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            x = self.activation_fc(hidden_layer(x))\n",
    "        return self.output_layer(x).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO():\n",
    "    def __init__(self, \n",
    "                 policy_model_fn, \n",
    "                 policy_model_max_grad_norm,\n",
    "                 policy_optimizer_fn,\n",
    "                 policy_optimizer_lr,\n",
    "                 policy_optimization_epochs,\n",
    "                 policy_sample_ratio,\n",
    "                 policy_clip_range,\n",
    "                 policy_stopping_kl,\n",
    "                 value_model_fn, \n",
    "                 value_model_max_grad_norm,\n",
    "                 value_optimizer_fn,\n",
    "                 value_optimizer_lr,\n",
    "                 value_optimization_epochs,\n",
    "                 value_sample_ratio,\n",
    "                 value_clip_range,\n",
    "                 value_stopping_mse,\n",
    "                 episode_buffer_fn,\n",
    "                 max_buffer_episodes,\n",
    "                 max_buffer_episode_steps,\n",
    "                 entropy_loss_weight,\n",
    "                 tau,\n",
    "                 n_workers):\n",
    "        assert n_workers > 1\n",
    "        assert max_buffer_episodes >= n_workers\n",
    "\n",
    "        self.policy_model_fn = policy_model_fn\n",
    "        self.policy_model_max_grad_norm = policy_model_max_grad_norm\n",
    "        self.policy_optimizer_fn = policy_optimizer_fn\n",
    "        self.policy_optimizer_lr = policy_optimizer_lr\n",
    "        self.policy_optimization_epochs = policy_optimization_epochs\n",
    "        self.policy_sample_ratio = policy_sample_ratio\n",
    "        self.policy_clip_range = policy_clip_range\n",
    "        self.policy_stopping_kl = policy_stopping_kl\n",
    "\n",
    "        self.value_model_fn = value_model_fn\n",
    "        self.value_model_max_grad_norm = value_model_max_grad_norm\n",
    "        self.value_optimizer_fn = value_optimizer_fn\n",
    "        self.value_optimizer_lr = value_optimizer_lr\n",
    "        self.value_optimization_epochs = value_optimization_epochs\n",
    "        self.value_sample_ratio = value_sample_ratio\n",
    "        self.value_clip_range = value_clip_range\n",
    "        self.value_stopping_mse = value_stopping_mse\n",
    "\n",
    "        self.episode_buffer_fn = episode_buffer_fn\n",
    "        self.max_buffer_episodes = max_buffer_episodes\n",
    "        self.max_buffer_episode_steps = max_buffer_episode_steps\n",
    "\n",
    "        self.entropy_loss_weight = entropy_loss_weight\n",
    "        self.tau = tau\n",
    "        self.n_workers = n_workers\n",
    "\n",
    "    def optimize_model(self):\n",
    "        states, actions, returns, gaes, logpas = self.episode_buffer.get_stacks()\n",
    "        values = self.value_model(states).detach()\n",
    "        gaes = (gaes - gaes.mean()) / (gaes.std() + EPS)\n",
    "        n_samples = len(actions)\n",
    "        \n",
    "        for _ in range(self.policy_optimization_epochs):\n",
    "            batch_size = int(self.policy_sample_ratio * n_samples)\n",
    "            batch_idxs = np.random.choice(n_samples, batch_size, replace=False)\n",
    "            states_batch = states[batch_idxs]\n",
    "            actions_batch = actions[batch_idxs]\n",
    "            gaes_batch = gaes[batch_idxs]\n",
    "            logpas_batch = logpas[batch_idxs]\n",
    "\n",
    "            logpas_pred, entropies_pred = self.policy_model.get_predictions(states_batch,\n",
    "                                                                            actions_batch)\n",
    "\n",
    "            ratios = (logpas_pred - logpas_batch).exp()\n",
    "            pi_obj = gaes_batch * ratios\n",
    "            pi_obj_clipped = gaes_batch * ratios.clamp(1.0 - self.policy_clip_range,\n",
    "                                                       1.0 + self.policy_clip_range)\n",
    "            policy_loss = -torch.min(pi_obj, pi_obj_clipped).mean()\n",
    "            entropy_loss = -entropies_pred.mean() * self.entropy_loss_weight\n",
    "\n",
    "            self.policy_optimizer.zero_grad()\n",
    "            (policy_loss + entropy_loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.policy_model.parameters(), \n",
    "                                           self.policy_model_max_grad_norm)\n",
    "            self.policy_optimizer.step()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                logpas_pred_all, _ = self.policy_model.get_predictions(states, actions)\n",
    "                kl = (logpas - logpas_pred_all).mean()\n",
    "                if kl.item() > self.policy_stopping_kl:\n",
    "                    break\n",
    "\n",
    "        for _ in range(self.value_optimization_epochs):\n",
    "            batch_size = int(self.value_sample_ratio * n_samples)\n",
    "            batch_idxs = np.random.choice(n_samples, batch_size, replace=False)\n",
    "            states_batch = states[batch_idxs]\n",
    "            returns_batch = returns[batch_idxs]\n",
    "            values_batch = values[batch_idxs]\n",
    "\n",
    "            values_pred = self.value_model(states_batch)\n",
    "            values_pred_clipped = values_batch + (values_pred - values_batch).clamp(-self.value_clip_range, \n",
    "                                                                                    self.value_clip_range)\n",
    "            v_loss = (returns_batch - values_pred).pow(2)\n",
    "            v_loss_clipped = (returns_batch - values_pred_clipped).pow(2)\n",
    "            value_loss = torch.max(v_loss, v_loss_clipped).mul(0.5).mean()\n",
    "\n",
    "            self.value_optimizer.zero_grad()\n",
    "            value_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.value_model.parameters(), \n",
    "                                           self.value_model_max_grad_norm)\n",
    "            self.value_optimizer.step()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                values_pred_all = self.value_model(states)\n",
    "                mse = (values - values_pred_all).pow(2).mul(0.5).mean()\n",
    "                if mse.item() > self.value_stopping_mse:\n",
    "                    break\n",
    "\n",
    "    def train(self, make_envs_fn, make_env_fn, make_env_kargs, seed, gamma, \n",
    "              max_minutes, max_episodes, goal_mean_100_reward):\n",
    "        training_start, last_debug_time = time.time(), float('-inf')\n",
    "\n",
    "        self.checkpoint_dir = tempfile.mkdtemp()\n",
    "        self.make_envs_fn = make_envs_fn\n",
    "        self.make_env_fn = make_env_fn\n",
    "        self.make_env_kargs = make_env_kargs\n",
    "        self.seed = seed\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        env = self.make_env_fn(**self.make_env_kargs, seed=self.seed)\n",
    "        envs = self.make_envs_fn(make_env_fn, make_env_kargs, self.seed, self.n_workers)\n",
    "        torch.manual_seed(self.seed) ; np.random.seed(self.seed) ; random.seed(self.seed)\n",
    "    \n",
    "        nS, nA = env.observation_space.shape, env.action_space.n\n",
    "        self.episode_timestep, self.episode_reward = [], []\n",
    "        self.episode_seconds, self.episode_exploration = [], []\n",
    "        self.evaluation_scores = []\n",
    "\n",
    "        self.policy_model = self.policy_model_fn(nS, nA)\n",
    "        self.policy_optimizer = self.policy_optimizer_fn(self.policy_model, self.policy_optimizer_lr)\n",
    "\n",
    "        self.value_model = self.value_model_fn(nS)\n",
    "        self.value_optimizer = self.value_optimizer_fn(self.value_model, self.value_optimizer_lr)\n",
    "\n",
    "        self.episode_buffer = self.episode_buffer_fn(nS, self.gamma, self.tau,\n",
    "                                                     self.n_workers, \n",
    "                                                     self.max_buffer_episodes,\n",
    "                                                     self.max_buffer_episode_steps)\n",
    "\n",
    "        result = np.empty((max_episodes, 5))\n",
    "        result[:] = np.nan\n",
    "        training_time = 0\n",
    "        episode = 0\n",
    "\n",
    "        # collect n_steps rollout\n",
    "        while True:\n",
    "            episode_timestep, episode_reward, episode_exploration, \\\n",
    "            episode_seconds = self.episode_buffer.fill(envs, self.policy_model, self.value_model)\n",
    "            \n",
    "            n_ep_batch = len(episode_timestep)\n",
    "            self.episode_timestep.extend(episode_timestep)\n",
    "            self.episode_reward.extend(episode_reward)\n",
    "            self.episode_exploration.extend(episode_exploration)\n",
    "            self.episode_seconds.extend(episode_seconds)\n",
    "            self.optimize_model()\n",
    "            self.episode_buffer.clear()\n",
    "\n",
    "            # stats\n",
    "            evaluation_score, _ = self.evaluate(self.policy_model, env)\n",
    "            self.evaluation_scores.extend([evaluation_score,] * n_ep_batch)\n",
    "            for e in range(episode, episode + n_ep_batch):\n",
    "                self.save_checkpoint(e, self.policy_model)\n",
    "            training_time += episode_seconds.sum()\n",
    "\n",
    "            mean_10_reward = np.mean(self.episode_reward[-10:])\n",
    "            std_10_reward = np.std(self.episode_reward[-10:])\n",
    "            mean_100_reward = np.mean(self.episode_reward[-100:])\n",
    "            std_100_reward = np.std(self.episode_reward[-100:])\n",
    "            mean_100_eval_score = np.mean(self.evaluation_scores[-100:])\n",
    "            std_100_eval_score = np.std(self.evaluation_scores[-100:])\n",
    "            mean_100_exp_rat = np.mean(self.episode_exploration[-100:])\n",
    "            std_100_exp_rat = np.std(self.episode_exploration[-100:])\n",
    "            \n",
    "            total_step = int(np.sum(self.episode_timestep))\n",
    "            wallclock_elapsed = time.time() - training_start\n",
    "            result[episode:episode+n_ep_batch] = total_step, mean_100_reward, \\\n",
    "                mean_100_eval_score, training_time, wallclock_elapsed\n",
    "\n",
    "            episode += n_ep_batch\n",
    "\n",
    "            # debug stuff\n",
    "            reached_debug_time = time.time() - last_debug_time >= LEAVE_PRINT_EVERY_N_SECS\n",
    "            reached_max_minutes = wallclock_elapsed >= max_minutes * 60            \n",
    "            reached_max_episodes = episode + self.max_buffer_episodes >= max_episodes\n",
    "            reached_goal_mean_reward = mean_100_eval_score >= goal_mean_100_reward\n",
    "            training_is_over = reached_max_minutes or \\\n",
    "                               reached_max_episodes or \\\n",
    "                               reached_goal_mean_reward\n",
    "            elapsed_str = time.strftime(\"%H:%M:%S\", time.gmtime(time.time() - training_start))\n",
    "            debug_message = 'el {}, ep {:04}, ts {:07}, '\n",
    "            debug_message += 'ar 10 {:05.1f}\\u00B1{:05.1f}, '\n",
    "            debug_message += '100 {:05.1f}\\u00B1{:05.1f}, '\n",
    "            debug_message += 'ex 100 {:02.1f}\\u00B1{:02.1f}, '\n",
    "            debug_message += 'ev {:05.1f}\\u00B1{:05.1f}'\n",
    "            debug_message = debug_message.format(\n",
    "                elapsed_str, episode-1, total_step, mean_10_reward, std_10_reward, \n",
    "                mean_100_reward, std_100_reward, mean_100_exp_rat, std_100_exp_rat,\n",
    "                mean_100_eval_score, std_100_eval_score)\n",
    "            print(debug_message, end='\\r', flush=True)\n",
    "            if reached_debug_time or training_is_over:\n",
    "                print(ERASE_LINE + debug_message, flush=True)\n",
    "                last_debug_time = time.time()\n",
    "            if training_is_over:\n",
    "                if reached_max_minutes: print(u'--> reached_max_minutes \\u2715')\n",
    "                if reached_max_episodes: print(u'--> reached_max_episodes \\u2715')\n",
    "                if reached_goal_mean_reward: print(u'--> reached_goal_mean_reward \\u2713')\n",
    "                break\n",
    "\n",
    "        final_eval_score, score_std = self.evaluate(self.policy_model, env, n_episodes=100)\n",
    "        wallclock_time = time.time() - training_start\n",
    "        print('Training complete.')\n",
    "        print('Final evaluation score {:.2f}\\u00B1{:.2f} in {:.2f}s training time,'\n",
    "              ' {:.2f}s wall-clock time.\\n'.format(\n",
    "                  final_eval_score, score_std, training_time, wallclock_time))\n",
    "        env.close() ; del env\n",
    "        envs.close() ; del envs\n",
    "        self.get_cleaned_checkpoints()\n",
    "        return result, final_eval_score, training_time, wallclock_time\n",
    "\n",
    "    def evaluate(self, eval_model, eval_env, n_episodes=1, greedy=True):\n",
    "        rs = []\n",
    "        for _ in range(n_episodes):\n",
    "            s, d = eval_env.reset(), False\n",
    "            rs.append(0)\n",
    "            for _ in count():\n",
    "                if greedy:\n",
    "                    a = eval_model.select_greedy_action(s)\n",
    "                else: \n",
    "                    a = eval_model.select_action(s)\n",
    "                s, r, d, _ = eval_env.step(a)\n",
    "                rs[-1] += r\n",
    "                if d: break\n",
    "        return np.mean(rs), np.std(rs)\n",
    "\n",
    "    def get_cleaned_checkpoints(self, n_checkpoints=4):\n",
    "        try: \n",
    "            return self.checkpoint_paths\n",
    "        except AttributeError:\n",
    "            self.checkpoint_paths = {}\n",
    "\n",
    "        paths = glob.glob(os.path.join(self.checkpoint_dir, '*.tar'))\n",
    "        paths_dic = {int(path.split('.')[-2]):path for path in paths}\n",
    "        last_ep = max(paths_dic.keys())\n",
    "        # checkpoint_idxs = np.geomspace(1, last_ep+1, n_checkpoints, endpoint=True, dtype=np.int)-1\n",
    "        checkpoint_idxs = np.linspace(1, last_ep+1, n_checkpoints, endpoint=True, dtype=np.int)-1\n",
    "\n",
    "        for idx, path in paths_dic.items():\n",
    "            if idx in checkpoint_idxs:\n",
    "                self.checkpoint_paths[idx] = path\n",
    "            else:\n",
    "                os.unlink(path)\n",
    "\n",
    "        return self.checkpoint_paths\n",
    "\n",
    "    def demo_last(self, title='Fully-trained {} Agent', n_episodes=2, max_n_videos=2):\n",
    "        env = self.make_env_fn(**self.make_env_kargs, monitor_mode='evaluation', render=True, record=True)\n",
    "\n",
    "        checkpoint_paths = self.get_cleaned_checkpoints()\n",
    "        last_ep = max(checkpoint_paths.keys())\n",
    "        self.policy_model.load_state_dict(torch.load(checkpoint_paths[last_ep]))\n",
    "\n",
    "        self.evaluate(self.policy_model, env, n_episodes=n_episodes)\n",
    "        env.close()\n",
    "        data = get_gif_html(env_videos=env.videos, \n",
    "                            title=title.format(self.__class__.__name__),\n",
    "                            max_n_videos=max_n_videos)\n",
    "        del env\n",
    "        return HTML(data=data)\n",
    "\n",
    "    def demo_progression(self, title='{} Agent progression', max_n_videos=4):\n",
    "        env = self.make_env_fn(**self.make_env_kargs, monitor_mode='evaluation', render=True, record=True)\n",
    "\n",
    "        checkpoint_paths = self.get_cleaned_checkpoints()\n",
    "        for i in sorted(checkpoint_paths.keys()):\n",
    "            self.policy_model.load_state_dict(torch.load(checkpoint_paths[i]))\n",
    "            self.evaluate(self.policy_model, env, n_episodes=1)\n",
    "\n",
    "        env.close()\n",
    "        data = get_gif_html(env_videos=env.videos, \n",
    "                            title=title.format(self.__class__.__name__),\n",
    "                            subtitle_eps=sorted(checkpoint_paths.keys()),\n",
    "                            max_n_videos=max_n_videos)\n",
    "        del env\n",
    "        return HTML(data=data)\n",
    "\n",
    "    def save_checkpoint(self, episode_idx, model):\n",
    "        torch.save(model.state_dict(),\n",
    "                   os.path.join(self.checkpoint_dir, 'model.{}.tar'.format(episode_idx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ppo_results = []\n",
    "best_agent, best_eval_score = None, float('-inf')\n",
    "for seed in SEEDS:\n",
    "    environment_settings = {\n",
    "        'env_name': 'LunarLander-v2',\n",
    "        'gamma': 0.99,\n",
    "        'max_minutes': 20,\n",
    "        'max_episodes': 2000,\n",
    "        'goal_mean_100_reward': 250\n",
    "    }\n",
    "\n",
    "    policy_model_fn = lambda nS, nA: FCCA(nS, nA, hidden_dims=(256,256))\n",
    "    policy_model_max_grad_norm = float('inf')\n",
    "    policy_optimizer_fn = lambda net, lr: optim.Adam(net.parameters(), lr=lr)\n",
    "    policy_optimizer_lr = 0.0003\n",
    "    policy_optimization_epochs = 80\n",
    "    policy_sample_ratio = 0.8\n",
    "    policy_clip_range = 0.1\n",
    "    policy_stopping_kl = 0.02\n",
    "\n",
    "    value_model_fn = lambda nS: FCV(nS, hidden_dims=(256,256))\n",
    "    value_model_max_grad_norm = float('inf')\n",
    "    value_optimizer_fn = lambda net, lr: optim.Adam(net.parameters(), lr=lr)\n",
    "    value_optimizer_lr = 0.0005\n",
    "    value_optimization_epochs = 80\n",
    "    value_sample_ratio = 0.8\n",
    "    value_clip_range = float('inf')\n",
    "    value_stopping_mse = 25\n",
    "\n",
    "    episode_buffer_fn = lambda sd, g, t, nw, me, mes: EpisodeBuffer(sd, g, t, nw, me, mes)\n",
    "    max_buffer_episodes = 16\n",
    "    max_buffer_episode_steps = 1000\n",
    "    \n",
    "    entropy_loss_weight = 0.01\n",
    "    tau = 0.97\n",
    "    n_workers = 8\n",
    "\n",
    "    env_name, gamma, max_minutes, \\\n",
    "    max_episodes, goal_mean_100_reward = environment_settings.values()\n",
    "    agent = PPO(policy_model_fn, \n",
    "                policy_model_max_grad_norm,\n",
    "                policy_optimizer_fn,\n",
    "                policy_optimizer_lr,\n",
    "                policy_optimization_epochs,\n",
    "                policy_sample_ratio,\n",
    "                policy_clip_range,\n",
    "                policy_stopping_kl,\n",
    "                value_model_fn, \n",
    "                value_model_max_grad_norm,\n",
    "                value_optimizer_fn,\n",
    "                value_optimizer_lr,\n",
    "                value_optimization_epochs,\n",
    "                value_sample_ratio,\n",
    "                value_clip_range,\n",
    "                value_stopping_mse,\n",
    "                episode_buffer_fn,\n",
    "                max_buffer_episodes,\n",
    "                max_buffer_episode_steps,\n",
    "                entropy_loss_weight,\n",
    "                tau,\n",
    "                n_workers)\n",
    "\n",
    "    make_envs_fn = lambda mef, mea, s, n: MultiprocessEnv(mef, mea, s, n)\n",
    "    make_env_fn, make_env_kargs = get_make_env_fn(env_name=env_name)\n",
    "    result, final_eval_score, training_time, wallclock_time = agent.train(make_envs_fn,\n",
    "                                                                          make_env_fn,\n",
    "                                                                          make_env_kargs,\n",
    "                                                                          seed,\n",
    "                                                                          gamma,\n",
    "                                                                          max_minutes,\n",
    "                                                                          max_episodes,\n",
    "                                                                          goal_mean_100_reward)\n",
    "    ppo_results.append(result)\n",
    "    if final_eval_score > best_eval_score:\n",
    "        best_eval_score = final_eval_score\n",
    "        best_agent = agent\n",
    "ppo_results = np.array(ppo_results)\n",
    "_ = BEEP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "best_agent.demo_progression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_agent.demo_last()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_max_t, ppo_max_r, ppo_max_s, \\\n",
    "ppo_max_sec, ppo_max_rt = np.max(ppo_results, axis=0).T\n",
    "ppo_min_t, ppo_min_r, ppo_min_s, \\\n",
    "ppo_min_sec, ppo_min_rt = np.min(ppo_results, axis=0).T\n",
    "ppo_mean_t, ppo_mean_r, ppo_mean_s, \\\n",
    "ppo_mean_sec, ppo_mean_rt = np.mean(ppo_results, axis=0).T\n",
    "ppo_x = np.arange(len(ppo_mean_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 1, figsize=(15,10), sharey=False, sharex=True)\n",
    "\n",
    "# PPO\n",
    "axs[0].plot(ppo_max_r, 'k', linewidth=1)\n",
    "axs[0].plot(ppo_min_r, 'k', linewidth=1)\n",
    "axs[0].plot(ppo_mean_r, 'k:', label='PPO', linewidth=2)\n",
    "axs[0].fill_between(\n",
    "    ppo_x, ppo_min_r, ppo_max_r, facecolor='k', alpha=0.3)\n",
    "\n",
    "axs[1].plot(ppo_max_s, 'k', linewidth=1)\n",
    "axs[1].plot(ppo_min_s, 'k', linewidth=1)\n",
    "axs[1].plot(ppo_mean_s, 'k:', label='PPO', linewidth=2)\n",
    "axs[1].fill_between(\n",
    "    ppo_x, ppo_min_s, ppo_max_s, facecolor='k', alpha=0.3)\n",
    "\n",
    "# ALL\n",
    "axs[0].set_title('Moving Avg Reward (Training)')\n",
    "axs[1].set_title('Moving Avg Reward (Evaluation)')\n",
    "plt.xlabel('Episodes')\n",
    "axs[0].legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 1, figsize=(15,15), sharey=False, sharex=True)\n",
    "\n",
    "# PPO\n",
    "axs[0].plot(ppo_max_t, 'k', linewidth=1)\n",
    "axs[0].plot(ppo_min_t, 'k', linewidth=1)\n",
    "axs[0].plot(ppo_mean_t, 'k:', label='PPO', linewidth=2)\n",
    "axs[0].fill_between(\n",
    "    ppo_x, ppo_min_t, ppo_max_t, facecolor='k', alpha=0.3)\n",
    "\n",
    "axs[1].plot(ppo_max_sec, 'k', linewidth=1)\n",
    "axs[1].plot(ppo_min_sec, 'k', linewidth=1)\n",
    "axs[1].plot(ppo_mean_sec, 'k:', label='PPO', linewidth=2)\n",
    "axs[1].fill_between(\n",
    "    ppo_x, ppo_min_sec, ppo_max_sec, facecolor='k', alpha=0.3)\n",
    "\n",
    "axs[2].plot(ppo_max_rt, 'k', linewidth=1)\n",
    "axs[2].plot(ppo_min_rt, 'k', linewidth=1)\n",
    "axs[2].plot(ppo_mean_rt, 'k:', label='PPO', linewidth=2)\n",
    "axs[2].fill_between(\n",
    "    ppo_x, ppo_min_rt, ppo_max_rt, facecolor='k', alpha=0.3)\n",
    "\n",
    "# ALL\n",
    "axs[0].set_title('Total Steps')\n",
    "axs[1].set_title('Training Time')\n",
    "axs[2].set_title('Wall-clock Time')\n",
    "plt.xlabel('Episodes')\n",
    "axs[0].legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_root_dir = os.path.join(RESULTS_DIR, 'ppo')\n",
    "not os.path.exists(ppo_root_dir) and os.makedirs(ppo_root_dir)\n",
    "\n",
    "np.save(os.path.join(ppo_root_dir, 'x'), ppo_x)\n",
    "\n",
    "np.save(os.path.join(ppo_root_dir, 'max_r'), ppo_max_r)\n",
    "np.save(os.path.join(ppo_root_dir, 'min_r'), ppo_min_r)\n",
    "np.save(os.path.join(ppo_root_dir, 'mean_r'), ppo_mean_r)\n",
    "\n",
    "np.save(os.path.join(ppo_root_dir, 'max_s'), ppo_max_s)\n",
    "np.save(os.path.join(ppo_root_dir, 'min_s'), ppo_min_s )\n",
    "np.save(os.path.join(ppo_root_dir, 'mean_s'), ppo_mean_s)\n",
    "\n",
    "np.save(os.path.join(ppo_root_dir, 'max_t'), ppo_max_t)\n",
    "np.save(os.path.join(ppo_root_dir, 'min_t'), ppo_min_t)\n",
    "np.save(os.path.join(ppo_root_dir, 'mean_t'), ppo_mean_t)\n",
    "\n",
    "np.save(os.path.join(ppo_root_dir, 'max_sec'), ppo_max_sec)\n",
    "np.save(os.path.join(ppo_root_dir, 'min_sec'), ppo_min_sec)\n",
    "np.save(os.path.join(ppo_root_dir, 'mean_sec'), ppo_mean_sec)\n",
    "\n",
    "np.save(os.path.join(ppo_root_dir, 'max_rt'), ppo_max_rt)\n",
    "np.save(os.path.join(ppo_root_dir, 'min_rt'), ppo_min_rt)\n",
    "np.save(os.path.join(ppo_root_dir, 'mean_rt'), ppo_mean_rt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bead_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
